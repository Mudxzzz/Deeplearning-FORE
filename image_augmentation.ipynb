{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image augmentation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOiww14R5FdH31CJappCDv3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/deeplearning/blob/main/image_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jPZlZonpAlq"
      },
      "source": [
        "# Last amended: 13th Feb, 2021\r\n",
        "# Ref:\r\n",
        "#   http://cs231n.github.io/neural-networks-2/\r\n",
        "#\t  https://machinelearningmastery.com/image-augmentation-deep-learning-keras/\r\n",
        "#\r\n",
        "# Objective:\r\n",
        "#   a) How Image augmentation is performed.\r\n",
        "#   b) Image augmentation basics\r\n",
        "#\r\n",
        "#\r\n",
        "# Data augmentation 4-steps\r\n",
        "#\t\t1. First create the ImageDataGenerator object with necessary properies\r\n",
        "#\t    2. If required, learn the 'train' data. Needed only for some types of\r\n",
        "#          augmentation/processing such as zca_whitening. see Ref below:\r\n",
        "#          Ref: https://stackoverflow.com/questions/46705600/keras-fit-image-augmentations-to-training-data-using-flow-from-directory\r\n",
        "#\t    3. Create the data-generator iterator\r\n",
        "#       4. Use the iterator in modeling in a for-loop\r\n",
        "# Sample code:\r\n",
        "# (https://keras.io/preprocessing/image/)\r\n",
        "#\r\n",
        "#\t\tdatagen = ImageDataGenerator( process_parameters)     # Create learner object\r\n",
        "#\t\tdatagen.fit(train)                                    # Learn train data\r\n",
        "#\t\tbatch_gen_iterator = datagen.flow(X_train,            # Get iterator object\r\n",
        "#                                     y_train,\r\n",
        "#                                     batch_size=batch_size\r\n",
        "#                                     )\r\n",
        "#\r\n",
        "#       for X_batch, y_batch in batch_gen_iterator:         # Use iterator--one way\r\n",
        "#                       model.fit(x_batch, y_batch)\r\n",
        "#                       batches += 1\r\n",
        "#\t\t\t                  if batches >= 5:\r\n",
        "#  \t                        break\r\n",
        "#\r\n",
        "#       model.fit_generator(batch_gen_iterator,               # Use iterator--another way\r\n",
        "#                           steps_per_epoch=5,                # Call 5 batches from generator\r\n",
        "#                           epochs=epochs)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjtPoKFWpzYC"
      },
      "source": [
        "\r\n",
        "# 1. Call libraries\r\n",
        "%reset -f\r\n",
        "\r\n",
        "# 1.1\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "\r\n",
        "# Open Anaconda as Administrator\r\n",
        "#  conda activate tf\r\n",
        "#  conda install -c conda-forge matplotlib\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "# 1.2 Import keras libraries\r\n",
        "#     This will import mnist dataset\r\n",
        "from tensorflow.keras.datasets import mnist\r\n",
        "\r\n",
        "# 1.3 For generating images on the fly (no need to store)\r\n",
        "#     from given images\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "# 1.4 Backend module helps extract/set properties of keras-backend\r\n",
        "#     By default the backend configuration file is at:\r\n",
        "#        $HOME/.keras/keras.json or /home/ashok/.keras or\r\n",
        "#        C:\\Users\\ashok\\.keras\\keras.json\r\n",
        "#     Ref: https://keras.io/backend/\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "\r\n",
        "\r\n",
        "# 1.5 Set image data format to be for tensorflow\r\n",
        "#     Override, settings in the configuration file\r\n",
        "#     That is set as channels_last, if not already so\r\n",
        "K.set_image_data_format('channels_last') # Alternative 'channels_first' for theano\r\n",
        "\r\n",
        "\r\n",
        " # 1.6 So what was the result of setting image data format above?\r\n",
        "K.image_data_format()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spq1PCx7p9l4"
      },
      "source": [
        "# 2. Use mnist module to fetch data from Internet & also load data\r\n",
        "#    Dataset is cached locally at $HOME/.keras/datasets\r\n",
        "#    or at C:\\Users\\ashok\\.keras\\datasets\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP_cs5-IqBvA"
      },
      "source": [
        "# 2.1 First help\r\n",
        "help(mnist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxz-rfgAqGmF"
      },
      "source": [
        "# 2.2 Download mnist dataset.\r\n",
        "#     Data download to C:\\Users\\ashok\\.keras\\datasets\r\n",
        "#     mnist.npz. 'npz' is a file format where several\r\n",
        "#     numpy arrays can be stored compressed or uncompressed\r\n",
        "(x_tr, y_tr), (x_te, y_te) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thjtTQV8qLI6"
      },
      "source": [
        "# 2.3 How many images and image dimensions?\r\n",
        "#     Depth of image is important dimension.\r\n",
        "#     It is missing here.\r\n",
        "\r\n",
        "x_tr.shape                    # [60000,28,28]\r\n",
        "x_te.shape                    # (10000,28,28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dby5oBLAqQvg"
      },
      "source": [
        "# 2.4 Just look at first image and get its label\r\n",
        "x_tr[0, :28, :28]\r\n",
        "y_tr[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuIGMHc-qUZc"
      },
      "source": [
        "# Create a grid of 3x3 images\r\n",
        "# AA. Plot sample digit images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmgo6uPVqaf7"
      },
      "source": [
        "# 3. Plot nine images\r\n",
        "for i in range(0, 9):               # range(0,9): Starting from 0, generate 9 numbers\r\n",
        "                                    # 0,1,..8 (9 is excluded)\r\n",
        "\tplt.subplot(3,3, i +1)  \t        # Plot in a grid of 3 rows, 3 columns at position (i +1)\r\n",
        "\t                                  # Try: help(plt.subplot)\r\n",
        "\t                                  # For last image, i =8, 339 => 3 rows X 3 cols at index 9\r\n",
        "\tplt.imshow(x_tr[i],\r\n",
        "  cmap=plt.get_cmap('gray')) # Map image color to gray palette\r\n",
        "\t\t\t\t\t                   # or colormap (try without cmap)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zooR8J98qosY"
      },
      "source": [
        "fig,ax = plt.subplots(3,3)\r\n",
        "af = ax.flatten()\r\n",
        "af.shape\r\n",
        "for axx,j in zip(af,range(0, 9)):\r\n",
        "\taxx.imshow(x_tr[j],cmap= plt.cm.gray)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaxidaE2qr6i"
      },
      "source": [
        "# 3.1 Show the figure with plots\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AhsRgNnqvfI"
      },
      "source": [
        "######################## BB ##################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BPOdU0oq0Ux"
      },
      "source": [
        "# 4.\r\n",
        "## BB.Image generation and Feature Standardization\r\n",
        "#     It is possible to standardize pixel values across the entire dataset.\r\n",
        "#     One can perform feature standardization by setting the featurewise_center\r\n",
        "#     and featurewise_std_normalization arguments on the ImageDataGenerator class.\r\n",
        "#     These are in fact set to True by default and creating an instance of\r\n",
        "#     ImageDataGenerator with no arguments will have the same effect.\r\n",
        "# Standardize images across the dataset, mean=0, stdev=1\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbFSdepOq7EI"
      },
      "source": [
        "# 4.1 Add no_of_channels to shape (tensorflow format: width X ht X channel):\r\n",
        "#     Reshape to be [NoOfSamples][width][height][channel]\r\n",
        "x_tr[0,:28,:28].shape                           # It is a flat 2D image\r\n",
        "# 4.1.1\r\n",
        "X_train = x_tr.reshape(60000, 28, 28,1)         # Being gray coloured, no of channels is 1\r\n",
        "X_test = x_te.reshape(10000,  28, 28,1)\r\n",
        "# 4.1.2\r\n",
        "X_train.shape\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYh1wG4iq8gI"
      },
      "source": [
        "# 4.2 Convert from int to float as required by Tensorflow\r\n",
        "X_train.dtype                         # Unsigned integer\r\n",
        "\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "\r\n",
        "X_train.dtype                         # 'float32'\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_J3KMsorAnQ"
      },
      "source": [
        "#4.2.1 For lack of memory, we will experiment on just 400 images\r\n",
        "X_train = X_train[:400,...]\r\n",
        "y_train = y_tr[:400,...]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZQWuHNsrDW3"
      },
      "source": [
        "# 4.3 Define generated-data preparation parameters\r\n",
        "#     & create the image generator object.\r\n",
        "#       Generate images in batches on-the-fly from some given images\r\n",
        "\r\n",
        "# 4.3.1\r\n",
        "# Ref: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\r\n",
        "help(ImageDataGenerator)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql8CzX8mrI2h"
      },
      "source": [
        "# 4.3.2\r\n",
        "# See http://cs231n.github.io/neural-networks-2/\r\n",
        "# featurewise_center\r\n",
        "#     Mean subtraction is the most common form of preprocessing.\r\n",
        "#     It involves subtracting the mean across every individual\r\n",
        "#     feature in the data, and has the geometric interpretation\r\n",
        "#     of centering the cloud of data around the origin along\r\n",
        "#     every dimension. In numpy, this operation would be\r\n",
        "#     implemented as: X -= np.mean(X, axis = 0).\r\n",
        "#     With images specifically, for convenience it can be common\r\n",
        "#     to subtract a single value from all pixels (e.g. X -= np.mean(X)),\r\n",
        "#     or to do so separately across the three color channels.\r\n",
        "# featurewise_std_normalization\r\n",
        "#     Normalization refers to normalizing the data dimensions\r\n",
        "#     so that they are of approximately the same scale. There\r\n",
        "#     are two common ways of achieving this normalization.\r\n",
        "#     One is to divide each dimension by its standard deviation,\r\n",
        "#     once it has been zero-centered: (X /= np.std(X, axis = 0)).\r\n",
        "#     Another form of this preprocessing normalizes each dimension\r\n",
        "#     so that the min and max along the dimension is -1 and 1 respectively.\r\n",
        "#     It only makes sense to apply this preprocessing if you have a reason\r\n",
        "#     to believe that different input features have different scales\r\n",
        "#     (or units), but they should be of approximately equal importance\r\n",
        "#     to the learning algorithm. In case of images, the relative scales\r\n",
        "#     of pixels are already approximately equal (and in range from 0 to 255),\r\n",
        "#     so it is not strictly necessary to perform this additional preprocessing step.\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHMVZ2BRrK6Z"
      },
      "source": [
        "# 4.3.3 Learner object\r\n",
        "datagen = ImageDataGenerator(\r\n",
        "                             zca_whitening=True,  # Ref: Kaggle: https://www.kaggle.com/nicw102168/exploring-zca-and-color-image-whitening\r\n",
        "\t\t\t\t\t\t\t                      # ZCA is a whitening transformation that decorrelates\r\n",
        "\t\t\t\t\t\t\t                      # (whiten) the data (image). De-correlation implies\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # removing linear dependencies. In images lot of\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # correlation exists between adjacent pixels.\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # zca is a technique to remove this 'common information'\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # between pixels. Consider this: We have a rectangle\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # which is filled with white colour inside and outside\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # colour is totally black. That is we have a black\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # rectangle on which is superimposed white rectangle.\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # If common information is removed, result will be\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # just a 'white-boundary' of inner rectangle and\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # both outside and inside of it completely black. We\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # have, therefore, a sort of embossed picture.\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # ==>  Transforms the image to a sort of embossed image <==\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # That is effect of ZCA on 244,244,244,10,10,10,10 would be\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  #  approx: 0,0,244,0,0,0,0\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  # Less redundancy in the image is intended to better\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  #  highlight the structures and features in the image\r\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  #   to the learning algorithm. \r\n",
        "                             zca_epsilon=0.001,   # Sharpness of edges when data get decorrelated\r\n",
        "\t\t\t\t\t\t\t                      # See this link to check effect of zca_epsilon on image\r\n",
        "\t\t\t\t\t\t\t                      # https://stats.stackexchange.com/a/117459\r\n",
        "                             featurewise_center=True,\r\n",
        "                             featurewise_std_normalization=True\r\n",
        "                            )\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4YvqTSgrOeI"
      },
      "source": [
        "\r\n",
        "type(datagen)\t\t # ImageDataGenerator object\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obgdOWYprQ8y"
      },
      "source": [
        "# 4.4 Learn statistics of actual data, train\r\n",
        "#     If you JUMP this step, you get a warning while plotting:\r\n",
        "#     This ImageDataGenerator specifies `featurewise_center`, but it\r\n",
        "#       hasn't been fit on any training data. Fit it\r\n",
        "#        first by calling `.fit(numpy_data)`.\r\n",
        "\r\n",
        "# 4.4.1 Learn data now\r\n",
        "datagen.fit(X_train)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt4Dy44ErUjH"
      },
      "source": [
        "\r\n",
        "# 4.5 'datagen' itself is an iterator. One can Iterate\r\n",
        "#     over it to get images. But this iterator needs\r\n",
        "#     to be confiured using flow() method to decide\r\n",
        "#     upon batch size to output at a time.\r\n",
        "#     Create first the datagen.flow() iterator\r\n",
        "\r\n",
        "# 4.4.2 Get iterator object. Iterator returns both\r\n",
        "#       transformed data as also its label\r\n",
        "\r\n",
        "batch_gen_iterator = datagen.flow(X_train,\r\n",
        "                                  y_train,\r\n",
        "                                  batch_size=9      # How many images to return at a time\r\n",
        "                                  )\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QzxSZ16rXdd"
      },
      "source": [
        "\r\n",
        "type(batch_gen_iterator)\t\t# Iterator object that retruns numpy arrays\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TlypHdEraqO"
      },
      "source": [
        "# 4.5.1 Operate Data generator, just to verify\r\n",
        "# Get first batch of images and labels\r\n",
        "\r\n",
        "img_batch = batch_gen_iterator\r\n",
        "x,y = next(img_batch)\r\n",
        "type(x)          # numpy.ndarray\r\n",
        "type(y)\t\t\t # numpy.ndarray\r\n",
        "x.shape          # Batch of 9 images of shape (28, 28, 1)\r\n",
        "x[0].shape       # (28, 28, 1)\r\n",
        "\r\n",
        "y.shape          # (9,) : One label for each image\r\n",
        "y                # Get all nine labels\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irey_LI_rezi"
      },
      "source": [
        "\r\n",
        "# 4.5.2 Reshape first image & plot\r\n",
        "x = x[0].reshape(28,28)   # Get one image\r\n",
        "plt.imshow(x,cmap=plt.cm.gray)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmcmWKYlrkZY"
      },
      "source": [
        "# 4.5.3 Same as above again\r\n",
        "x,y = next(img_batch)\r\n",
        "x = x[0].reshape(28,28)   # Get one image\r\n",
        "plt.imshow(x, cmap=plt.get_cmap('gray'))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jyh6eiFVrlPJ"
      },
      "source": [
        "# 4.6 Configure batch size and retrieve one batch of images\r\n",
        "def plot_images():\r\n",
        "\tfor X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\r\n",
        "\t\t# create a grid of 3x3 images\r\n",
        "\t\tfor i in range(0, 9):\r\n",
        "\t\t\tplt.subplot(330 + 1 + i)\r\n",
        "\t\t\timg = X_batch[i].reshape(28, 28)\r\n",
        "\t\t\tplt.imshow(img, cmap=plt.cm.gray)\r\n",
        "\t\t# show the plot\r\n",
        "\t\tplt.show()\r\n",
        "\t\tbreak\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJoWjCxXroEJ"
      },
      "source": [
        "# 4.7 Iterate over data-generator to get images in batches\r\n",
        "#     embossing is clearer in these images\r\n",
        "#     'Emboss means image stands out in relief'\r\n",
        "\r\n",
        "plot_images()       # Plot a batch of nine images\r\n",
        "plot_images()       # Plot another batch of nine images\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wktEXYzUrqvv"
      },
      "source": [
        "# 4.8 Running above example one can see that the effect is different,\r\n",
        "#     seemingly somewhat darkening and lightening different digits.\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Iqq6_miruhs"
      },
      "source": [
        "######################## CC ##################################\r\n",
        "#### Rotation of images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ_OxUgsryhI"
      },
      "source": [
        "\r\n",
        "# 5.1 Let us retrieve again X_train, y_train\r\n",
        "#     from original data\r\n",
        "X_train = x_tr.reshape(60000, 28, 28,1)\r\n",
        "X_test = x_te.reshape(10000,  28, 28,1)\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "X_train = X_train[:400,...]\r\n",
        "y_train = y_tr[:400,...]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkeFMiQWr3Z_"
      },
      "source": [
        "\r\n",
        "# 5.2 Reshaped to be [NoOfSamples][width][height][channel]\r\n",
        "X_train.shape\r\n",
        "X_test.shape\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtsroQkMr62c"
      },
      "source": [
        "\r\n",
        "# 6\r\n",
        "## DD. Random Rotations\r\n",
        "# Sometimes images in sample data may have varying and different rotations in the scene.\r\n",
        "#  One can train one's model to better handle rotations of images by artificially and randomly\r\n",
        "#   rotating images from dataset during training.\r\n",
        "\r\n",
        "# 6.1 Define data preparation\r\n",
        "datagen = ImageDataGenerator(\r\n",
        "                            rotation_range=90\r\n",
        "                            )\r\n",
        "\r\n",
        "# 6.2 Fit parameters on our small data only (400 images)\r\n",
        "datagen.fit(X_train)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHHz1RFxr7i_"
      },
      "source": [
        "\r\n",
        "# 6.4\r\n",
        "plot_images()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6RSa05xr9-c"
      },
      "source": [
        "\r\n",
        "# 7\r\n",
        "## E. Random Shifts\r\n",
        "#     Objects in images may not be centered in the frame. They may be off-center\r\n",
        "#     in a variety of different ways. One can train deep learning network to expect\r\n",
        "#     and currently handle off-center objects by artificially creating shifted versions\r\n",
        "#     of training data. Keras supports separate horizontal and vertical random shifting\r\n",
        "#     of training data by the width_shift_range and height_shift_range arguments.\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9XJW5TBsBUs"
      },
      "source": [
        "# 7.1 Define shift values, horizontal and vertical\r\n",
        "shift1 = 0.2\r\n",
        "shift2 = 0.3\r\n",
        "datagen = ImageDataGenerator(\r\n",
        "                             width_shift_range=shift1,     # Horizontal shift\r\n",
        "                             height_shift_range=shift2     # Vertical shift\r\n",
        "                             )\r\n",
        "# 7.2 Learn data\r\n",
        "datagen.fit(X_train)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK-_gRhusE5I"
      },
      "source": [
        "\r\n",
        "# 7.3 Configure batch size and retrieve one batch of images\r\n",
        "plot_images()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eicyv1JzsKay"
      },
      "source": [
        "\r\n",
        "# 8\r\n",
        "## F. Random Flips\r\n",
        "#       Another augmentation to image data that can improve performance on\r\n",
        "#       large and complex problems is to create random flips of images in\r\n",
        "#       training data.  Keras supports random flipping along both the vertical\r\n",
        "#       and horizontal axes using the vertical_flip and horizontal_flip arguments.\r\n",
        "\r\n",
        "# 8.1 Define data preparation\r\n",
        "datagen = ImageDataGenerator(\r\n",
        "                             horizontal_flip=True,\r\n",
        "                             vertical_flip=True,\r\n",
        "                            )\r\n",
        "\r\n",
        "# 8.2 Learn data\r\n",
        "datagen.fit(X_train)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7el-vxzsLEq"
      },
      "source": [
        "\r\n",
        "# 8.3 Configure batch size and retrieve one batch of images\r\n",
        "plot_images()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIzJneVesN9x"
      },
      "source": [
        "\r\n",
        "# 9\r\n",
        "## G. Mixed. All in one.\r\n",
        "#       Another augmentation to image data that can improve performance on\r\n",
        "#       large and complex problems is to create random flips of images in\r\n",
        "#       training data.  Keras supports random flipping along both the vertical\r\n",
        "#       and horizontal axes using the vertical_flip and horizontal_flip arguments.\r\n",
        "\r\n",
        "shift1 = 0.2\r\n",
        "shift2 = 0.3\r\n",
        "# 9.1 Define data preparation. All are applied in some measure\r\n",
        "datagen = ImageDataGenerator(\r\n",
        "                             horizontal_flip=True,\r\n",
        "                             vertical_flip=True,\r\n",
        "                             width_shift_range=shift1,     # Horizontal shift\r\n",
        "                             height_shift_range=shift2,     # Vertical shift\r\n",
        "                             rotation_range=90,\r\n",
        "                             zoom_range = 2.0,\r\n",
        "                             shear_range = 30             # Shear angle in counter-clockwise direction in degrees\r\n",
        "                            )\r\n",
        "# 9.2 Learn data\r\n",
        "datagen.fit(X_train)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJTrraowsRJs"
      },
      "source": [
        "\r\n",
        "# 9.3 Configure batch size and retrieve one batch of images\r\n",
        "plot_images()\r\n",
        "plot_images()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J8nQtxjsZcQ"
      },
      "source": [
        "############ I am done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnKlorOjscHZ"
      },
      "source": [
        "###################### DD. #############################################\r\n",
        "\r\n",
        "\r\n",
        "# 9\r\n",
        "## F. Saving Augmented Images to File\r\n",
        "#      The data preparation and augmentation is performed just in time by Keras.\r\n",
        "#      This is efficient in terms of memory, but one may require the exact images\r\n",
        "#      used during training. For example, perhaps one would like to use them with\r\n",
        "#      a different software package later or only generate them once and use them\r\n",
        "#      on multiple different deep learning models or configurations.\r\n",
        "#      Keras allows you to save the images generated during training. The directory,\r\n",
        "#      filename prefix and image file type can be specified to the flow() function\r\n",
        "#      before training.\r\n",
        "\r\n",
        "# 9.1 Define data preparation. All default parameter values\r\n",
        "datagen = ImageDataGenerator()\r\n",
        "\r\n",
        "# 9.2 Learn data statistics\r\n",
        "datagen.fit(X_train)\r\n",
        "\r\n",
        "# 9.3 Configure batch size and retrieve one batch of images\r\n",
        "#dir = \"/home/ashok/useless\"\r\n",
        "dir = \"C:\\\\Users\\\\ashok\\\\useless\"\r\n",
        "\r\n",
        "# 9.4 Make directory\r\n",
        "#os.system('rm -rf  /home/ashok/useless')\r\n",
        "os.mkdir(\"C:\\\\Users\\\\ashok\\\\useless\")\r\n",
        "\r\n",
        "\r\n",
        "# 9.5 Generator iterator.\r\n",
        "#     Generate images on-the-fy\r\n",
        "data_iterator = datagen.flow(X_train,\r\n",
        "                             y_train,\r\n",
        "                             batch_size=9,\r\n",
        "                             save_to_dir=dir,\r\n",
        "                             save_prefix='aug',\r\n",
        "                             save_format='png'\r\n",
        "                             )\r\n",
        "\r\n",
        "# 9.6 Generate and save\r\n",
        "for X_batch, y_batch in data_iterator:\r\n",
        "\t# create a grid of 3x3 images\r\n",
        "\tfor i in range(0, 9):\r\n",
        "\t\tplt.subplot(330 + 1 + i)\r\n",
        "\t\tplt.imshow(X_batch[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\r\n",
        "\t# show the plot\r\n",
        "\tplt.show()\r\n",
        "    break\r\n",
        "\r\n",
        "########################\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK93XfJhsc00"
      },
      "source": [
        "\r\n",
        "# Supplementary Exercises:\r\n",
        "# 1.\r\n",
        "import numpy as np\r\n",
        "x = np.linspace(-np.pi, +np.pi, 100)\r\n",
        "for i in range(0,9):\r\n",
        "\tplt.subplot(520+i+1)\r\n",
        "\tplt.plot(x+i*10, np.sin(x+i*10))\r\n",
        "\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# 2.0\r\n",
        "x = np.random.randint(low = 0, high = 1000, size = (4,5))\r\n",
        "x\r\n",
        "x.shape\r\n",
        "y = x.reshape((4,5,1))\r\n",
        "y.shape\r\n",
        "y\r\n",
        "# 2.1\r\n",
        "y.dtype\r\n",
        "y = y.astype('float64')\r\n",
        "y.shape\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA9AnrDbsgVh"
      },
      "source": [
        "\r\n",
        "######################## About ZCA whitening ###########################\r\n",
        "\r\n",
        "##    ZCA Whitening (\"Mahalanobis transformation\")\r\n",
        "#     What is whitening? See: 1. http://cs231n.github.io/neural-networks-2/\r\n",
        "#                             2. https://en.wikipedia.org/wiki/Whitening_transformation\r\n",
        "#     Whitening is something like PCA\r\n",
        "#     A whitening transformation is a linear transformation (as PCA is) that transforms\r\n",
        "#     a dataset with a known covariance matrix into a set of new variables whose\r\n",
        "#     covariance is the identity matrix, meaning that they are uncorrelated and each\r\n",
        "#     have variance 1.\r\n",
        "#     The transformation is called \"whitening\" because it changes the input vector\r\n",
        "#     into a white noise vector.\r\n",
        "#     Less redundancy in the image is intended to better highlight the structures\r\n",
        "#     and features in the image to the learning algorithm. See some images at this link:\r\n",
        "#     https://stats.stackexchange.com/a/117459\r\n",
        "\r\n",
        "## Impt Note: Before experimenting EXIT IPYTHON. Execute AA; Jump BB and then come to CC.\r\n",
        "#########################################################################\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-1eVpMIsi9V"
      },
      "source": [
        "\r\n",
        "\"\"\"\r\n",
        "Using ImageDataGenerator without fitting\r\n",
        "========================================\r\n",
        "\r\n",
        "Ref: https://stackoverflow.com/questions/46705600/keras-fit-image-augmentations-to-training-data-using-flow-from-directory\r\n",
        "     https://github.com/keras-team/keras/issues/68\r\n",
        "\r\n",
        "Just try to use the following object with zca_whitening:\r\n",
        "\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "    rescale=1. / 255,           # Normalize colour intensities in 0-1 range\r\n",
        "    shear_range=0.2,             # Shear varies from 0-0.2\r\n",
        "    zoom_range=0.2,\r\n",
        "    horizontal_flip=True,\r\n",
        "    preprocessing_function=preprocess,\r\n",
        "    featurewise_center=True,\r\n",
        "    zca_whitening=True)\r\n",
        "\r\n",
        "When you do so set epochs to 1 in model.fit_generator.\r\n",
        "The fitting process starts with a warning:\r\n",
        "\r\n",
        "`zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)\r\n",
        "\r\n",
        "As data is huge, fit can be called on a sample, as:\r\n",
        "train_datagen .fit(X_sample)  # where X_sample is a small but random sample of your data\r\n",
        "\r\n",
        "zca is a technique to remove 'common information'\r\n",
        "between pixels. Consider this: We have a rectangle\r\n",
        "which is filled with white colour inside and outside\r\n",
        "colour is totally black. That is we have a black\r\n",
        "rectangle on which is superimposed white rectangle.\r\n",
        "If common information is removed, result will be\r\n",
        "just a 'white-boundary' of inner rectangle and\r\n",
        "both outside and inside of it completely black. We\r\n",
        "have, therefore, a sort of embossed picture.\r\n",
        "\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}