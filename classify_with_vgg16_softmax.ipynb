{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classify_with_vgg16 softmax.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GQL6YWWY9SP8",
        "OlB5imiZJK34",
        "ovAfE-NFJRo8",
        "AKdtLJhd9q_x",
        "grGEVbnj-Ixa",
        "ke2c95BJ_XnT",
        "yrZpJLFc_fJS",
        "U3Vb9eM6qvVE",
        "1luUFXHtqzsH",
        "uXgeLAkSpuXY",
        "XEg0KtOmAEPL",
        "h4w1IkBDqYm3",
        "DGss4iZIqcZY",
        "NmXF0PgNqQdd"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/deeplearning/blob/main/classify_with_vgg16_softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEXKIYEf82qQ"
      },
      "source": [
        "'''\r\n",
        "Last amended: 23rd Feb, 2021\r\n",
        "Ref: Page 143 (Section 5.3) of Book 'Deep Learning with python'\r\n",
        "     by Francois Chollet\r\n",
        "     https://www.kaggle.com/rajmehra03/a-comprehensive-guide-to-transfer-learning#data\r\n",
        "Objective:\r\n",
        "         i) Transfer Learning:     Building powerful image classification\r\n",
        "                                   models using very little data using\r\n",
        "                                   pre-trained applications\r\n",
        "         ii) Feature Enngineering: Using engineered features with\r\n",
        "                                   Random Forest Classifier\r\n",
        "         iii)Learning Rate Annealing:Moderating Learning rate\r\n",
        "                                     on/near plateau                      \r\n",
        "\r\n",
        "Steps:\r\n",
        "\t1.   Create higher level abstract features from train data\r\n",
        "       and save these to file\r\n",
        "\t2.   Use saved features as input to a FC model to make predictions\r\n",
        "  3. Save FC model\r\n",
        "  4. Use the complete model to make predictions\r\n",
        "\r\n",
        "Data from Kaggle: https://www.kaggle.com/c/dogs-vs-cats/data\r\n",
        "\r\n",
        "In our setup, we:\r\n",
        "- created a folder: Images/cats_dogs/ folder\r\n",
        "- created train/ and validation/ subfolders inside cats_dogs/\r\n",
        "- created cats/ and dogs/ subfolders inside train/ and validation/\r\n",
        "\r\n",
        "In summary, this is our directory structure:\r\n",
        "\r\n",
        "Images/\r\n",
        "\tdata/\r\n",
        "\t    train/\r\n",
        "        \tdogs/\r\n",
        "        \t    dog001.jpg\r\n",
        "        \t    dog002.jpg\r\n",
        "        \t    ...\r\n",
        "        \tcats/\r\n",
        "        \t    cat001.jpg\r\n",
        "        \t    cat002.jpg\r\n",
        "        \t    ...\r\n",
        "\t    validation/\r\n",
        "        \tdogs/\r\n",
        "        \t    dog001.jpg\r\n",
        "        \t    dog002.jpg\r\n",
        "        \t    ...\r\n",
        "        \tcats/\r\n",
        "        \t    cat001.jpg\r\n",
        "        \t    cat002.jpg\r\n",
        "        \t    ...\r\n",
        "\r\n",
        "\t$ source activate tensorflow\r\n",
        "\t$ ipython\r\n",
        "\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQL6YWWY9SP8"
      },
      "source": [
        "## PART I: Extract features from train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfgXApdG9F6v"
      },
      "source": [
        "####********************************************************************************\r\n",
        "#### *****************   PART I: Tranform train data to abstract features and save**\r\n",
        "####********************************************************************************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlB5imiZJK34"
      },
      "source": [
        "#### Call libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCjy9SoP9cBb"
      },
      "source": [
        "%reset -f\r\n",
        "## 1. Call libraries\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# 1.1 Classes for creating models\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense\r\n",
        "\r\n",
        "# 1.2 Class for accessing pre-built models\r\n",
        "from tensorflow.keras import applications\r\n",
        "\r\n",
        "# 1.3 Class for generating infinite images\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "# 1.4 Miscelleneous\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import time, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfNA1baCcmWf"
      },
      "source": [
        "# 1.5 Display multiple commands outputs from a sell\r\n",
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovAfE-NFJRo8"
      },
      "source": [
        "#### Bringing file from Google drive to VM\r\n",
        "And unzip it there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8KBaPl4EhKk"
      },
      "source": [
        "#2.0 Copy file cats_dogs.tar.gz to our HOME folder\r\n",
        "! cp /content/drive/MyDrive/Colab_data_files/cats_dogs.tar.gz $HOME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRtANTgyEqWT",
        "outputId": "5afff893-a865-4bf3-d447-af65c681fe91"
      },
      "source": [
        "#2.0.1 Check if file copied. If so, unzip it\r\n",
        "!ls $HOME"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cats_dogs.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkVma-GtGn9k"
      },
      "source": [
        "# 2.0.2 We will keep our unzipped files here\r\n",
        "!mkdir $HOME/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwNQpaRFFFGD"
      },
      "source": [
        "# 2.0.3 Untar the gz file\r\n",
        "\r\n",
        "! tar -xvzf $HOME/cats_dogs.tar.gz -C $HOME/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3mt8DfSJ1GW",
        "outputId": "a8040a13-2452-4e27-9387-427d7490cfb8"
      },
      "source": [
        "! ls /root/data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cats_dogs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1WKDYWXHHaD",
        "outputId": "55140878-22fd-4052-d598-2899ee9dcc4b"
      },
      "source": [
        "# Check the folders/files\r\n",
        "!ls -la $HOME/data/cats_dogs\r\n",
        "# And where is my HOME?\r\n",
        "! echo $HOME    # /root"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 20\n",
            "drwxrwxr-x 5 1000 1000 4096 Feb  7  2018 .\n",
            "drwxr-xr-x 3 root root 4096 Feb 23 23:47 ..\n",
            "drwxrwxr-x 3 1000 1000 4096 Feb  7  2018 test\n",
            "drwxrwxr-x 4 1000 1000 4096 Feb  7  2018 train\n",
            "drwxrwxr-x 4 1000 1000 4096 Feb  7  2018 validation\n",
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKdtLJhd9q_x"
      },
      "source": [
        "### AA. Constants & Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W02H8V9R9nGy"
      },
      "source": [
        "############################# AA. Constants & Hyperparameters ###################3\r\n",
        "## 3. Constants/hyperparameters\r\n",
        "\r\n",
        "# 3.1 Where are cats and dogs?\r\n",
        "#train_data_dir = \"C:\\\\Users\\\\ashok\\\\Desktop\\\\chmod\\\\2. data_augmentation\\\\cats_dogs\\\\train\"\r\n",
        "#train_data_dir      =  '/home/ashok/Images/cats_dogs/train'\r\n",
        "train_data_dir = \"/root/data/cats_dogs/train\"\r\n",
        "\r\n",
        "#validation_data_dir = \"C:\\\\Users\\\\ashok\\\\Desktop\\\\chmod\\\\2. data_augmentation\\\\cats_dogs\\\\validation\"\r\n",
        "#validation_data_dir =  '/home/ashok/Images/cats_dogs/validation'\r\n",
        "validation_data_dir = \"/root/data/cats_dogs/validation\"\r\n",
        "\r\n",
        "# 3.2 Constrain dimensions of our \r\n",
        "#     images during image generation:\r\n",
        "\r\n",
        "img_width, img_height = 75,75       # Large size images affect model-fitting speed\r\n",
        "\r\n",
        "\r\n",
        "# 3.3 How many sampels of each one of them?\r\n",
        "\r\n",
        "nb_train_samples, nb_validation_samples = 2000, 800\r\n",
        "\r\n",
        "\r\n",
        "# 3.4 Predict in batches that fit RAM\r\n",
        "#     and also sample-size is fully divisible by it\r\n",
        "\r\n",
        "batch_size = 50         # Maybe for 4GB machine, batch-size of 32 will be OK\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hposIiuXUIg2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1l1UQjrUJN8"
      },
      "source": [
        "#### numpy .npy format?\r\n",
        "\r\n",
        "A simple format for saving numpy arrays to disk with the full information about them.\r\n",
        "\r\n",
        "The .npy format is the standard binary file format in NumPy for persisting a single arbitrary NumPy array on disk. The format stores all of the shape and dtype information necessary to reconstruct the array correctly even on another machine with a different architecture. The format is designed to be as simple as possible while achieving its limited goals.\r\n",
        "\r\n",
        "The .npz format is the standard format for persisting multiple NumPy arrays on disk. A .npz file is a zip file containing multiple .npy files, one for each arra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjFt1VVN96zS"
      },
      "source": [
        "# 3.5 File to which transformed bottleneck features\r\n",
        "#     for train data wil be stored\r\n",
        "#bf_filename = 'C:\\\\Users\\\\ashok\\\\Desktop\\\\chmod\\\\3. veryDeepConvNets\\\\models\\\\bottleneck_features_train.npy'\r\n",
        "#bf_filename = '/home/ashok/.keras/models/bottleneck_features_train.npy'\r\n",
        "bf_filename = '/root/data/cats_dogs/bottleneck_features_train.npy'\r\n",
        "\r\n",
        "# 3.6 File to which transformed bottleneck features \r\n",
        "#     for validation data wil be stored\r\n",
        "#val_filename = 'C:\\\\Users\\\\ashok\\\\Desktop\\\\chmod\\\\3. veryDeepConvNets\\\\models\\\\bottleneck_features_validation.npy'\r\n",
        "val_filename = '/root/data/cats_dogs/bottleneck_features_validation.npy'\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grGEVbnj-Ixa"
      },
      "source": [
        "### BB. Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTFb5I3D9_Fq",
        "outputId": "a9c9fcb9-fd58-497a-ffe9-a47a1f3f8529"
      },
      "source": [
        "############################# BB. Data Generation ###################3\r\n",
        "\r\n",
        "## 4. Data augmentation\r\n",
        "\r\n",
        "# 4.1 Instanstiate an image data generator:\r\n",
        "#     Needed to feed into the model\r\n",
        "\r\n",
        "datagen_train = ImageDataGenerator(\r\n",
        "                                    rescale=1. / 255,\r\n",
        "                                    rotation_range=10,     # randomly rotate images in the range (degrees, 0 to 180)\r\n",
        "                                    zoom_range = 0.1,      # Randomly zoom image \r\n",
        "                                    width_shift_range=0.2, # randomly shift images horizontally (fraction of total width)\r\n",
        "                                    height_shift_range=0.2,# randomly shift images vertically (fraction of total height)\r\n",
        "                                    horizontal_flip=True,  # randomly flip images\r\n",
        "                                    vertical_flip=False   # randomly flip images\r\n",
        "                                   )\r\n",
        "\r\n",
        "# 4.2 Configure datagen_train further\r\n",
        "#     Datagenerator is configured twice.\r\n",
        "#     First configuration\r\n",
        "#     is about image manipulation features\r\n",
        "#     IInd configuration is regarding data source,\r\n",
        "#     data classes, batchsize  etc\r\n",
        "\r\n",
        "generator_tr = datagen_train.flow_from_directory(\r\n",
        "              directory = train_data_dir,\t\t      # Path to target train directory.\r\n",
        "              target_size=(img_width, img_height),# Dimensions to which all images will be resized.\r\n",
        "              batch_size=batch_size,              # At a time so many images will be output\r\n",
        "              class_mode=None,                    # Return NO labels along with image data\r\n",
        "              shuffle=False                       # Default shuffle = True\r\n",
        "                                                  # Now images are picked up first from\r\n",
        "                                                  #  one folder then from another; no shuffling\r\n",
        "                                                  #   We will be using images NOT for\r\n",
        "                                                  #    learning any model but only for prediction\r\n",
        "                                                  #      so shuffle = False is OK as we now know\r\n",
        "                                                  #       that Ist 1000 images are of one kind\r\n",
        "                                                  #        and next 1000 images of another kind\r\n",
        "                                                  # See: https://github.com/keras-team/keras/issues/3296\r\n",
        "              )\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTAg_Jqz-V2c"
      },
      "source": [
        "\r\n",
        "\"\"\"\r\n",
        "# 4.2 If data was not arranged in the directory,\r\n",
        "#     then iterator would be:\r\n",
        "\r\n",
        "generator_t = datagen.flow(X_train,  # Should have rank 4. In case of grayscale data,\r\n",
        "                                     #   the channels axis should have value 1, and in\r\n",
        "                                     #    case of RGB data, it should have value 3.\r\n",
        "                           y_train,  #  X_train labels\r\n",
        "                           shuffle=False,\r\n",
        "                           batch_size=batch_size\r\n",
        "                        )\r\n",
        "\r\n",
        "  There is, however, no 'target_size' parameter here\r\n",
        "\r\n",
        "\"\"\"\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74qwS67u-cCD"
      },
      "source": [
        "#### Generation for validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpYwJpZy-fv7",
        "outputId": "15a77e24-8ee9-4d92-fc36-380815b881f0"
      },
      "source": [
        "# 4.3  Generator for validation data.\r\n",
        "#      Initialize ImageDataGenerator object once more\r\n",
        "#      shuffle = False => Sort data in alphanumeric order\r\n",
        "\r\n",
        "datagen_val = ImageDataGenerator(rescale=1. / 255)\r\n",
        "generator_val = datagen_val.flow_from_directory(\r\n",
        "                                          validation_data_dir,\r\n",
        "                                          target_size=(img_width, img_height),\r\n",
        "                                          batch_size=batch_size,\r\n",
        "                                          class_mode=None,\r\n",
        "                                          shuffle=False   # Default shuffle = True\r\n",
        "                                                      # Now images are picked up first from\r\n",
        "                                                      #  one folder then from another; no shuffling\r\n",
        "                                                      #   We will be using images NOT for\r\n",
        "                                                      #    learning any model but only for prediction\r\n",
        "                                                      #      so shuffle = False is OK as we now know\r\n",
        "                                                      #       that Ist 1000 images are of one kind\r\n",
        "                                                      #        and next 1000 images of another kind\r\n",
        "                                                      # See: https://github.com/keras-team/keras/issues/3296\r\n",
        "                                          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 800 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej575ooU-vUR"
      },
      "source": [
        "### CC. Modeling & Feature creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziJdkL1n-sej"
      },
      "source": [
        "############################# CC. Modeling & Feature creation #####################\r\n",
        "############################For both train and validation data ####################\r\n",
        "####################Created features become our fresh train/validation data########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kACp609V-zn6",
        "outputId": "5f545ac8-03c5-48de-eeb2-78aed6c2bb49"
      },
      "source": [
        "# 5. Buld VGG16 network model with 'imagenet' weights\r\n",
        "#     Do not include the top FC layer of VGG16 model\r\n",
        "#      Weights will be downloaded, if absent\r\n",
        "\r\n",
        "model = applications.VGG16(\r\n",
        "\t                         include_top=False,\r\n",
        "\t                         weights='imagenet',\r\n",
        "\t                         input_shape=(img_width, img_height,3)\r\n",
        "\t                       )\r\n",
        "# 5.1\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 75, 75, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 75, 75, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 75, 75, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 37, 37, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 37, 37, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 37, 37, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 18, 18, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 18, 18, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 18, 18, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 18, 18, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 9, 9, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 9, 9, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoiO6Izk-_9z",
        "outputId": "68cdef3e-b438-4d38-8552-858f8f00f2a8"
      },
      "source": [
        "# 5.2 Feed images through VGG16 model in batches \r\n",
        "#     And make 'predictions' in 2000/50 = 40 steps.\r\n",
        "#     Following takes time 7 +3 = 10 minutes\r\n",
        "#     Note that there is no need for 'fit' method as weights are\r\n",
        "#     already learnt\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "# 4.1 By feeding the input samples from generator, \r\n",
        "#     create vgg16 output/predictions uptil the last\r\n",
        "#     layer. We call it 'bottleneck features' as it\r\n",
        "#     is not the desired end result\r\n",
        "#     steps:  How many batches of images to output or how many times to call image-generator\r\n",
        "\r\n",
        "bottleneck_features_train = model.predict_generator(\r\n",
        "                                                    generator = generator_tr,\r\n",
        "                                                    steps = nb_train_samples // batch_size,\r\n",
        "                                                    verbose = 1\r\n",
        "                                                    )\r\n",
        "end = time.time()\r\n",
        "print(\"Time taken: \",(end - start)/60, \"minutes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 111s 3s/step\n",
            "Time taken:  1.8482813278834025 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4WJI7_8_Hr9",
        "outputId": "bc2c5428-66aa-4f01-ee88-e124d9cfd0dc"
      },
      "source": [
        "# 5.3   Similarly, make predictions for \r\n",
        "#       validation data and extract features\r\n",
        "#       Takes 12 minutes\r\n",
        "\r\n",
        "start = time.time()\r\n",
        "bottleneck_features_validation = model.predict_generator(\r\n",
        "                                                         generator = generator_val,\r\n",
        "                                                         steps = nb_validation_samples // batch_size,\r\n",
        "                                                         verbose = 1\r\n",
        "                                                         )\r\n",
        "\r\n",
        "end = time.time()\r\n",
        "print(\"Time taken: \",(end - start)/60, \"minutes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 43s 3s/step\n",
            "Time taken:  0.7224789619445801 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke2c95BJ_XnT"
      },
      "source": [
        "### Saving Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNhLQ_Fb_QWL"
      },
      "source": [
        "############################# DD. Saving features ###################\r\n",
        "\r\n",
        "# 6. Save the train features\r\n",
        "# 6.1 First delete the file to whcih we will save\r\n",
        "\r\n",
        "if os.path.exists(bf_filename):\r\n",
        "    os.system('rm ' + bf_filename)\r\n",
        "\r\n",
        "# 6.2 Next save the train-features\r\n",
        "np.save(open(bf_filename, 'wb'), bottleneck_features_train)\r\n",
        "\r\n",
        "\r\n",
        "# 6.3 Save validation features from model\r\n",
        "if os.path.exists(val_filename):\r\n",
        "    os.system('rm ' + val_filename)\r\n",
        "\r\n",
        "np.save(open(val_filename, 'wb'), bottleneck_features_validation)\r\n",
        "\r\n",
        "# 6.4 Quit python so that complete memory is reset\r\n",
        "#     Maybe reboot your lubuntu (NOT WINDOWS)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akvdc2NIk_sZ"
      },
      "source": [
        "# 7.0 Else, Let us delete some variables\r\n",
        "del bottleneck_features_validation\r\n",
        "del bottleneck_features_train\r\n",
        "del model\r\n",
        "del datagen_train\r\n",
        "del datagen_val "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrZpJLFc_fJS"
      },
      "source": [
        "## PART-II Use Extracted features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGFncvYO_iY9"
      },
      "source": [
        "################### ########### ##################### #######\r\n",
        "################### PART-II BEGIN AGAIN #####################\r\n",
        "################### ########### ##################### #######"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id2cMR8PquqB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Vb9eM6qvVE"
      },
      "source": [
        "#### Call libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON_KK40g_oMv"
      },
      "source": [
        "## Part II: Load saved abstract features and proceed\r\n",
        "#           with modeling and prediction\r\n",
        "# Start ipython #\r\n",
        "\r\n",
        "# 8.0 Call libraries\r\n",
        "%reset -f\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, Softmax\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras import applications\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import time, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1luUFXHtqzsH"
      },
      "source": [
        "#### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DjQq1Nh_viF"
      },
      "source": [
        "# 8.1 Hyperparameters/Constants\r\n",
        "# 8.2 Dimensions of our images.\r\n",
        "img_width, img_height = 75,75  # 150, 150\r\n",
        "nb_train_samples = 2000\r\n",
        "nb_validation_samples = 800\r\n",
        "epochs = 50\r\n",
        "batch_size = 64\r\n",
        "num_classes = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEJznvUc_0Hz"
      },
      "source": [
        "# 8.3 Where are saved bottleneck features for train data?\r\n",
        "#bf_filename = '/home/ashok/.keras/models/bottleneck_features_train.npy'\r\n",
        "#bf_filename = 'C:\\\\Users\\\\ashok\\\\Desktop\\\\chmod\\\\3. veryDeepConvNets\\\\models\\\\bottleneck_features_train.npy'\r\n",
        "bf_filename = '/root/data/cats_dogs/bottleneck_features_train.npy'\r\n",
        "\r\n",
        "\r\n",
        "# 8.4 Validation-bottleneck features filename\r\n",
        "#val_filename = '/home/ashok/.keras/models/bottleneck_features_validation.npy'\r\n",
        "#val_filename = 'C:\\\\Users\\\\ashok\\\\Desktop\\\\chmod\\\\3. veryDeepConvNets\\\\models\\\\bottleneck_features_validation.npy'\r\n",
        "val_filename = '/root/data/cats_dogs/bottleneck_features_validation.npy'\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXgeLAkSpuXY"
      },
      "source": [
        "#### HDF5 storage for Python?\r\n",
        "\r\n",
        "The h5py package is a Pythonic interface to the HDF5 binary data format.\r\n",
        "\r\n",
        "HDF5 lets you store huge amounts of numerical data, and easily manipulate that data from NumPy. For example, you can slice into multi-terabyte datasets stored on disk, as if they were real NumPy arrays. Thousands of datasets can be stored in a single file, categorized and tagged however you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIxhmYeUpxPV"
      },
      "source": [
        "# 8.6 File to which FC model weights could be stored\r\n",
        "#top_model_weights_path = \"C:\\\\Users\\\\ashok\\\\Desktop\\\\chmod\\\\3. veryDeepConvNets\\\\models\\\\bottleneck_fc_model.h5\"\r\n",
        "#top_model_weights_path = '/home/ashok/.keras/models/bottleneck_fc_model.h5'\r\n",
        "top_model_weights_path = '/root/data/cats_dogs/bottleneck_fc_model.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEg0KtOmAEPL"
      },
      "source": [
        "#### Load Train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwKjS5yZ_5rb"
      },
      "source": [
        "# 9. Get train features\r\n",
        "train_data_features = np.load(open(bf_filename,'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY2LnIzh_oxc",
        "outputId": "ab16909c-b6f0-4250-ea3b-efdb923027f7"
      },
      "source": [
        "# 9.1\r\n",
        "train_data_features.shape      # (2000, 2, 2, 512)\r\n",
        "print(\"\\n-----------\\n\")\r\n",
        "\r\n",
        "# 9.2 Train lables. First half are of one kind and next half of other\r\n",
        "#     Remember we had put 'shuffle = False' in data generators\r\n",
        "#     1000 labels of one kind. Another 1000 labels of another kind\r\n",
        "train_labels = np.array([1] * 1000 + [2] * 1000)   # Try [0] * 3 + [1] * 5\r\n",
        "train_labels\r\n",
        "print(\"\\n-----------\\n\")\r\n",
        "\r\n",
        "# 9.2.1\r\n",
        "train_labels.shape     # (2000,)\r\n",
        "print(\"\\n-----------\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 2, 2, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4w1IkBDqYm3"
      },
      "source": [
        "##### Shuffle train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l95Ieg0zAS5k",
        "outputId": "d3d0310c-470e-4159-f0ea-b6f4f2330763"
      },
      "source": [
        "# 9.3 Shuffle train features(tiles) as also corresponding labels\r\n",
        "x = np.arange(2000)      # Generate 2000 indicies for shuffling\r\n",
        "np.random.shuffle(x)     # x is inplace shuffled\r\n",
        "x[:5]\r\n",
        "\r\n",
        "# 9.4\r\n",
        "train_data_features = train_data_features[x, :,:,:]\r\n",
        "train_labels = train_labels[x]\r\n",
        "train_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 906,  103, 1746, 1723, 1831])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGss4iZIqcZY"
      },
      "source": [
        "##### OneHotEncode train labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H46xOhwkAZWZ",
        "outputId": "fbd18595-1740-40e7-e337-c0cc239b3b29"
      },
      "source": [
        "# 9.5 One hot encode the labels\r\n",
        "#     For any classification problem, in Deep Learning\r\n",
        "#     softmax layer rather than sigmoid MUST be used\r\n",
        "#     See reasons at the end of code\r\n",
        "\r\n",
        "train_labels_cat = to_categorical(train_labels  )\r\n",
        "print(\"\\n-----------\\n\")\r\n",
        "\r\n",
        "train_labels_cat.shape    # (2000, 3)\r\n",
        "print(\"\\n-----------\\n\")\r\n",
        "\r\n",
        "train_labels_cat          # Ist column is constant"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdDI3C6-Aebp",
        "outputId": "dbfda617-7665-4dec-d783-a295a099706b"
      },
      "source": [
        "# 9.5.1 Ist constant column is of no use to us\r\n",
        "#       So forget it\r\n",
        "train_labels_cat = train_labels_cat[:, 1:]\r\n",
        "train_labels_cat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       ...,\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmXF0PgNqQdd"
      },
      "source": [
        "#### Load Validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09aC-lB3As5T",
        "outputId": "e83dfd41-a9c5-4d8f-dc08-e662571727a1"
      },
      "source": [
        "# 10. Read validation features\r\n",
        "validation_data_features = np.load(open(val_filename,'rb'))\r\n",
        "\r\n",
        "\r\n",
        "# 10.1\r\n",
        "print(\"\\n-----------\\n\")\r\n",
        "validation_data_features.shape       # (800, 2, 2, 512)\r\n",
        "\r\n",
        "# 10.2 Validation labels: half-half\r\n",
        "validation_labels = np.array([1] * 400 + [2] * 400)\r\n",
        "\r\n",
        "# 10.3 Convert to OHE\r\n",
        "validation_labels = to_categorical(validation_labels)\r\n",
        "validation_labels = validation_labels[:,1:]\r\n",
        "\r\n",
        "# 10.4\r\n",
        "print(\"\\n-----------\\n\")\r\n",
        "train_data_features.shape[1:]     # (2, 2, 512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 2, 2, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 2, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR321-P-Aw2p"
      },
      "source": [
        "#### Design our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYfuORwHNTFj"
      },
      "source": [
        "The gist of RMSprop is to:\r\n",
        "*    Maintain a moving (discounted) average of the square of gradients  \r\n",
        "*    Divide the gradient by the root of this average\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_W6K5UqktcW"
      },
      "source": [
        "# 11.0 Delete any existing model\r\n",
        "#      Perform model fitting with or without\r\n",
        "#      'ReduceLROnPlateau'\r\n",
        "# \r\n",
        "del model  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPGToOCOAzGU",
        "outputId": "dc851097-08f6-4950-9eaf-1fd54848ce2b"
      },
      "source": [
        "# 12. Plan model with FC layers only\r\n",
        "#    We use transformed features as input to FC model\r\n",
        "#    instead of actual train data\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Flatten(input_shape=train_data_features.shape[1:]))     # (2, 2, 512)\r\n",
        "model.add(Dense(256, activation='relu'))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "#model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.add(Dense(num_classes, activation='softmax'))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "# 12.1 Declare optimizer to use\r\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop\r\n",
        "opt = tf.keras.optimizers.RMSprop(\r\n",
        "                                   learning_rate=0.001,\r\n",
        "                                   rho=0.9,      # Discount factor for coming gradient.  \r\n",
        "                                   epsilon=1e-07,  # Small constant for numerical stability.\r\n",
        "                                   name='RMSprop'\r\n",
        "                                   )\r\n",
        "\r\n",
        "# 12.2\r\n",
        "model.compile(\r\n",
        "              optimizer=opt,\r\n",
        "              loss='binary_crossentropy',\r\n",
        "              metrics=['accuracy']\r\n",
        "              )\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_7 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 2)                 514       \n",
            "=================================================================\n",
            "Total params: 525,058\n",
            "Trainable params: 525,058\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I52p5lhRrDJ9"
      },
      "source": [
        "##### Reduce learning rate on plateau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4KRO4LNhgMb"
      },
      "source": [
        "# 12.3 Reduce learning rate when a metric has stopped improving.\r\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau\r\n",
        "reduce_lr=tf.keras.callbacks.ReduceLROnPlateau(\r\n",
        "                                               monitor='val_accuracy',\r\n",
        "                                               factor = 0.1, # factor by which the learning\r\n",
        "                                                             # rate will be reduced. \r\n",
        "                                                             # new_lr = lr * factor.\r\n",
        "                                               min_delta=0.0001, # threshold for measuring the\r\n",
        "                                                                 # new optimum, to only focus on \r\n",
        "                                                                 # significant changes. \r\n",
        "                                               patience=2, # number of epochs with no\r\n",
        "                                                           # improvement after which \r\n",
        "                                                           # learning rate will be reduced. \r\n",
        "                                               verbose=1\r\n",
        "                                              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyrW0UsQCEPJ",
        "outputId": "19609228-0f5f-4db2-8cc6-8148d2948f19"
      },
      "source": [
        "# 13.0 Fit model and make predictions on validation dataset\r\n",
        "#      Takes 2 minutes\r\n",
        "#      Watch Validation loss and Validation accuracy (around 81%)\r\n",
        "#\r\n",
        "start = time.time()\r\n",
        "history = model.fit(\r\n",
        "                    train_data_features, train_labels_cat,\r\n",
        "                    epochs=100,\r\n",
        "                    batch_size=batch_size,\r\n",
        "                    callbacks=[reduce_lr], \r\n",
        "                    validation_data=(validation_data_features, validation_labels),\r\n",
        "                    verbose =1\r\n",
        "                   )\r\n",
        "end = time.time()\r\n",
        "print(\"Time taken: \",(end - start)/60, \"minutes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 0.3452 - accuracy: 0.8545 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3456 - accuracy: 0.8510 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3466 - accuracy: 0.8575 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.0000001181490946e-25.\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3435 - accuracy: 0.8520 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3419 - accuracy: 0.8460 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000001428009978e-26.\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3435 - accuracy: 0.8550 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3420 - accuracy: 0.8530 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.000000142800998e-27.\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3482 - accuracy: 0.8465 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3488 - accuracy: 0.8490 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000001235416984e-28.\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3402 - accuracy: 0.8560 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3493 - accuracy: 0.8510 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000001235416985e-29.\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3410 - accuracy: 0.8560 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3448 - accuracy: 0.8540 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000001536343539e-30.\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3419 - accuracy: 0.8565 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3460 - accuracy: 0.8530 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.000000191250173e-31.\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3486 - accuracy: 0.8520 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3421 - accuracy: 0.8570 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000002147600601e-32.\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3430 - accuracy: 0.8535 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3447 - accuracy: 0.8505 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000002441474188e-33.\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3440 - accuracy: 0.8505 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3454 - accuracy: 0.8500 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000002074132203e-34.\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3454 - accuracy: 0.8535 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3487 - accuracy: 0.8470 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000001614954722e-35.\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3479 - accuracy: 0.8490 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3433 - accuracy: 0.8560 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000001614954723e-36.\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3444 - accuracy: 0.8570 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3421 - accuracy: 0.8585 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000001256222317e-37.\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3478 - accuracy: 0.8555 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3404 - accuracy: 0.8550 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000001032014561e-38.\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3438 - accuracy: 0.8525 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3436 - accuracy: 0.8520 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000751754869e-39.\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3446 - accuracy: 0.8540 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3464 - accuracy: 0.8520 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000002153053334e-40.\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3445 - accuracy: 0.8530 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 35/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3471 - accuracy: 0.8525 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.99994610111476e-42.\n",
            "Epoch 36/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3454 - accuracy: 0.8485 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 37/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3455 - accuracy: 0.8590 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.999665841421895e-43.\n",
            "Epoch 38/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3464 - accuracy: 0.8545 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 39/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3432 - accuracy: 0.8520 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0005271035279195e-43.\n",
            "Epoch 40/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3492 - accuracy: 0.8550 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 41/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3442 - accuracy: 0.8580 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.949219096706202e-45.\n",
            "Epoch 42/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3448 - accuracy: 0.8575 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 43/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3395 - accuracy: 0.8585 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.80908925027372e-46.\n",
            "Epoch 44/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3447 - accuracy: 0.8515 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 45/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3417 - accuracy: 0.8525 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.4012984643248171e-46.\n",
            "Epoch 46/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3465 - accuracy: 0.8515 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 47/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3462 - accuracy: 0.8550 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 48/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3387 - accuracy: 0.8525 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 49/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3421 - accuracy: 0.8610 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 50/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3459 - accuracy: 0.8580 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 51/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3445 - accuracy: 0.8585 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 52/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3462 - accuracy: 0.8540 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 53/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3420 - accuracy: 0.8530 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 54/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3423 - accuracy: 0.8505 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 55/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3405 - accuracy: 0.8605 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 56/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3405 - accuracy: 0.8510 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 57/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3429 - accuracy: 0.8530 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 58/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3435 - accuracy: 0.8515 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 59/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3434 - accuracy: 0.8575 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 60/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3442 - accuracy: 0.8555 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 61/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3409 - accuracy: 0.8580 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 62/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3458 - accuracy: 0.8570 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 63/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3425 - accuracy: 0.8560 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 64/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3450 - accuracy: 0.8510 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 65/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3454 - accuracy: 0.8575 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 66/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3464 - accuracy: 0.8555 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 67/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3382 - accuracy: 0.8555 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 68/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3422 - accuracy: 0.8535 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 69/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3447 - accuracy: 0.8595 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 70/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3416 - accuracy: 0.8585 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 71/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3408 - accuracy: 0.8590 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 72/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3404 - accuracy: 0.8545 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 73/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3421 - accuracy: 0.8590 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 74/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3420 - accuracy: 0.8540 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 75/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3457 - accuracy: 0.8560 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 76/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3430 - accuracy: 0.8610 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 77/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3465 - accuracy: 0.8530 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 78/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3424 - accuracy: 0.8565 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 79/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3420 - accuracy: 0.8485 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 80/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3419 - accuracy: 0.8600 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 81/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3437 - accuracy: 0.8590 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 82/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3407 - accuracy: 0.8555 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 83/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3455 - accuracy: 0.8515 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 84/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3427 - accuracy: 0.8490 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 85/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3427 - accuracy: 0.8515 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 86/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3420 - accuracy: 0.8505 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 87/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3424 - accuracy: 0.8480 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 88/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3417 - accuracy: 0.8535 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 89/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3439 - accuracy: 0.8555 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 90/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3446 - accuracy: 0.8540 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 91/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3403 - accuracy: 0.8570 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 92/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3490 - accuracy: 0.8560 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 93/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3405 - accuracy: 0.8610 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 94/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3461 - accuracy: 0.8560 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 95/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3399 - accuracy: 0.8575 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 96/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3419 - accuracy: 0.8585 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 97/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3435 - accuracy: 0.8540 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 98/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3432 - accuracy: 0.8490 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 99/100\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3414 - accuracy: 0.8500 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Epoch 100/100\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3445 - accuracy: 0.8540 - val_loss: 0.4309 - val_accuracy: 0.8075\n",
            "Time taken:  0.6707986831665039 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4NrT7PLCFqU",
        "outputId": "d31bf283-ef72-47e6-c681-92e941e65e7a"
      },
      "source": [
        "# 13.1\r\n",
        "print(\"\\n---History keys-----------\\n\")\r\n",
        "history.history.keys()   # dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\r\n",
        "\r\n",
        "# 13.1.1\r\n",
        "print(\"\\n---History tr accuracy length-----------\\n\")\r\n",
        "len(history.history['accuracy'])     # 50  Training accuracy:\r\n",
        "                                     # As many as number of epochs\r\n",
        "\r\n",
        "#13.1.2\r\n",
        "print(\"\\n---History val accuracy length-----------\\n\")\r\n",
        "len(history.history['val_accuracy']) # 50   Validation accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---History keys-----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---History tr accuracy length-----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---History val accuracy length-----------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "R8d3rg4lCJqV",
        "outputId": "f4fc645c-0795-4267-d4ec-c5177ecf705f"
      },
      "source": [
        "# 14.0 It is very wavy without\r\n",
        "#     ReduceLROnPlateau\r\n",
        "\r\n",
        "plot_learning_curve()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQcQSNrjCa85"
      },
      "source": [
        "#### Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHgi9UG5Cd3Y"
      },
      "source": [
        "# 15. Finally save model weights for later use\r\n",
        "model.save_weights(top_model_weights_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFekxDAuoHDv",
        "outputId": "cd9b8199-b07a-48e5-a1aa-8903787df1f8"
      },
      "source": [
        "# 15.1 Check if saved and size of file:\r\n",
        "! ls -la /root/data/cats_dogs/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 24496\n",
            "drwxrwxr-x 5 1000 1000     4096 Feb 24 00:39 .\n",
            "drwxr-xr-x 3 root root     4096 Feb 23 23:47 ..\n",
            "-rw-r--r-- 1 root root  2114080 Feb 24 01:21 bottleneck_fc_model.h5\n",
            "-rw-r--r-- 1 root root 16384128 Feb 24 00:26 bottleneck_features_train.npy\n",
            "-rw-r--r-- 1 root root  6553728 Feb 24 00:26 bottleneck_features_validation.npy\n",
            "drwxrwxr-x 3 1000 1000     4096 Feb  7  2018 test\n",
            "drwxrwxr-x 4 1000 1000     4096 Feb  7  2018 train\n",
            "drwxrwxr-x 4 1000 1000     4096 Feb  7  2018 validation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfmwfaLbCtSy"
      },
      "source": [
        "#### Learning curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-edkQyxcCrro"
      },
      "source": [
        "#######################################################\r\n",
        "\r\n",
        "#     How accuracy changes as epochs increase\r\n",
        "#     We will use this function agai and again\r\n",
        "#     in subsequent examples\r\n",
        "\r\n",
        "def plot_learning_curve():\r\n",
        "    val_acc = history.history['val_accuracy']\r\n",
        "    tr_acc=history.history['accuracy']\r\n",
        "    epochs = range(1, len(val_acc) +1)\r\n",
        "    plt.plot(epochs,val_acc, 'b', label = \"Validation accu\")\r\n",
        "    plt.plot(epochs, tr_acc, 'r', label = \"Training accu\")\r\n",
        "    plt.title(\"Training and validation accuracy\")\r\n",
        "    plt.xlabel(\"epochs-->\")\r\n",
        "    plt.ylabel(\"accuracy\")\r\n",
        "    plt.legend()\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "#########################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0RFjRseZL7M"
      },
      "source": [
        "# Use Random Forest\r\n",
        "Making predictions using extracted features using Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBrbwTv7ZQlx"
      },
      "source": [
        "# 16.0 Using RandomForestClassifier for classification\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X= train_data_features\r\n",
        "print(\"\\n----X.shape-------\\n\")\r\n",
        "X.shape      # (2000, 2, 2, 512)\r\n",
        "X = X.reshape(2000, -1)\r\n",
        "print(\"\\n----X.shape-------\\n\")\r\n",
        "X.shape     # (2000, 2048)\r\n",
        "y = train_labels\r\n",
        "print(\"\\n-----y.shape------\\n\")\r\n",
        "y.shape     # (2000, )\r\n",
        "\r\n",
        "# 16.1\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.25, shuffle = True)\r\n",
        "print(\"\\n----y_train-------\\n\")\r\n",
        "y_train[:5]     # It is now shuffled\r\n",
        "print(\"\\n----y_test-------\\n\")\r\n",
        "y_test[:5]      # It is now shuffled\r\n",
        "print(\"\\n-----Model------\\n\")\r\n",
        "\r\n",
        "# 16.2\r\n",
        "model = RandomForestClassifier()\r\n",
        "model.fit(X_train,y_train)\r\n",
        "print(\"\\n-----------\\n\")\r\n",
        "\r\n",
        "# 16.3\r\n",
        "y_pred = model.predict(X_test)\r\n",
        "print(\"\\n----y_pred-------\\n\")\r\n",
        "y_pred[:10]\r\n",
        "print(\"\\n-----------\\n\")\r\n",
        "np.sum(y_test==y_pred)/len(y_test)    # 79.8%\r\n",
        "#==============================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMcTZ9MgC6HZ"
      },
      "source": [
        "#########################\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "Sigmoid vs Softmax\r\n",
        "===================\r\n",
        "\r\n",
        "    Suppose there are 5 classes. So we may have either 5-sigmoid\r\n",
        "    neurons at the output or just one neuron.\r\n",
        "    If we have just one neuron, than the neuron will emit\r\n",
        "    numbers as 4.3, 3.6 instead of 4 or 3. In a regression problem\r\n",
        "    we can take exactly the value outputted that is 4.6 and subtract\r\n",
        "    from expected output to calculate error. But in a classification\r\n",
        "    problem, 4.6 is meaning less and we are not sure whether to consider\r\n",
        "    it as 4th class or 5th class.\r\n",
        "\r\n",
        "    If we want to have 5 neurons with sigmoid at the end then\r\n",
        "    the better option is to have softmax.\r\n",
        "\r\n",
        "    Therefore, in a classification problem, even if there\r\n",
        "    are two classes, it is better to use softmax rather than use\r\n",
        "    one sigmoid neuron at the end.\r\n",
        "\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GtQG2r4eeDe"
      },
      "source": [
        "############ I am done #############"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}