{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_functional.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QBKy2GzK76sc",
        "dYuF2BagAAYt"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNAILqf+1s8bnrrDMlj0iPW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/keras/blob/main/keras_functional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzHM-gho6Z4X"
      },
      "source": [
        "# Last amended: 16th Jan, 2021\r\n",
        "# Ref: Hands-On Machine Learningwith Scikit-Learn, Keras, and TensorFlow by Aurelien Geron\r\n",
        "#      Page: 308-312\r\n",
        "# Using keras functional API"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYl2nTZg7MRe"
      },
      "source": [
        "# 1.0 Import libraries\r\n",
        "import pandas as pd\r\n",
        "from sklearn.datasets import fetch_california_housing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "\r\n",
        "# Import tensorflow/keras \r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers \r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtYhfSLVGVC_"
      },
      "source": [
        "# 1.1 Display multiple outputs from a Cell\r\n",
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzwqXlRY6uH7",
        "outputId": "4500e600-42d9-47cf-9eeb-008b636b82fc"
      },
      "source": [
        "# 2.0 Get Data\r\n",
        "#     The data needs little processing\r\n",
        "housing = fetch_california_housing(return_X_y= False)\r\n",
        "type(housing)   # sklearn.utils.Bunch"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sklearn.utils.Bunch"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxD2EQQ56_Z4",
        "outputId": "74bffe1c-f9d0-4a1c-b165-0aa3a6f972d9"
      },
      "source": [
        "# 2.1 Seperate X,y\r\n",
        "X = housing.data\r\n",
        "y = housing.target\r\n",
        "X.shape   # (20640, 8)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20640, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha_HncyCmS9y"
      },
      "source": [
        "# 2.2 Normalize input data\r\n",
        "ss = StandardScaler()\r\n",
        "X = ss.fit_transform(X)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uogWyNdXGfm3",
        "outputId": "96aa8ab1-3d14-45ce-8014-3cdaa4530e98"
      },
      "source": [
        "# 2.3 Show data field names\r\n",
        "print(housing.DESCR)\r\n",
        "housing.feature_names"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block\n",
            "        - HouseAge      median house age in block\n",
            "        - AveRooms      average number of rooms\n",
            "        - AveBedrms     average number of bedrooms\n",
            "        - Population    block population\n",
            "        - AveOccup      average house occupancy\n",
            "        - Latitude      house block latitude\n",
            "        - Longitude     house block longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "http://lib.stat.cmu.edu/datasets/\n",
            "\n",
            "The target variable is the median house value for California districts.\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MedInc',\n",
              " 'HouseAge',\n",
              " 'AveRooms',\n",
              " 'AveBedrms',\n",
              " 'Population',\n",
              " 'AveOccup',\n",
              " 'Latitude',\n",
              " 'Longitude']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzWFuUd9Mf-0",
        "outputId": "ebb555fd-effc-4e9d-a830-94665926e18a"
      },
      "source": [
        "# 3.0 Split train/test data\r\n",
        "X_train,X_test, y_train,y_test = train_test_split(X,y,test_size = 0.2)\r\n",
        "X_train.shape   # (16512, 8)\r\n",
        "X_test.shape    # (4128, 8)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16512, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4128, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBKy2GzK76sc"
      },
      "source": [
        "# Wide and Deep Network--Ist version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JouACnATIIUF"
      },
      "source": [
        "# 3.1 Design model\r\n",
        "\r\n",
        "# 3.1.1 Inputs to model. Note that inputs is NOT\r\n",
        "#       a part of layers object\r\n",
        "inputs = tf.keras.Input(shape = X.shape[1:])\r\n",
        "# 3.1.2 Add layers\r\n",
        "x = layers.Dense(100, activation = 'relu')(inputs)\r\n",
        "x = layers.Dense(100, activation = 'relu')(x)\r\n",
        "x = tf.keras.layers.concatenate([x,inputs])\r\n",
        "out = layers.Dense(1,activation = 'sigmoid')(x)\r\n",
        "# 3.1.3 Create model now\r\n",
        "model = Model(inputs = [inputs], outputs = [out])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZmSNOwJJ2cf",
        "outputId": "c2bc469a-2d6c-4879-8a5e-53d7cc4768ec"
      },
      "source": [
        "# 3.2 Print model summary\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_14 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 100)          900         input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 100)          10100       dense_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 108)          0           dense_26[0][0]                   \n",
            "                                                                 input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 1)            109         concatenate_8[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 11,109\n",
            "Trainable params: 11,109\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBZwG9ht70_A",
        "outputId": "6c600044-b10c-48f2-862f-048ac0a284ad"
      },
      "source": [
        "# 3.3 `Model` groups layers into an object \r\n",
        "#       with training and inference features.\r\n",
        "help(Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Model in module tensorflow.python.keras.engine.training:\n",
            "\n",
            "class Model(tensorflow.python.keras.engine.base_layer.Layer, tensorflow.python.keras.utils.version_utils.ModelVersionSelector)\n",
            " |  `Model` groups layers into an object with training and inference features.\n",
            " |  \n",
            " |  Arguments:\n",
            " |      inputs: The input(s) of the model: a `keras.Input` object or list of\n",
            " |          `keras.Input` objects.\n",
            " |      outputs: The output(s) of the model. See Functional API example below.\n",
            " |      name: String, the name of the model.\n",
            " |  \n",
            " |  There are two ways to instantiate a `Model`:\n",
            " |  \n",
            " |  1 - With the \"Functional API\", where you start from `Input`,\n",
            " |  you chain layer calls to specify the model's forward pass,\n",
            " |  and finally you create your model from inputs and outputs:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  inputs = tf.keras.Input(shape=(3,))\n",
            " |  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n",
            " |  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
            " |  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
            " |  ```\n",
            " |  \n",
            " |  2 - By subclassing the `Model` class: in that case, you should define your\n",
            " |  layers in `__init__` and you should implement the model's forward pass\n",
            " |  in `call`.\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super(MyModel, self).__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |  \n",
            " |    def call(self, inputs):\n",
            " |      x = self.dense1(inputs)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  If you subclass `Model`, you can optionally have\n",
            " |  a `training` argument (boolean) in `call`, which you can use to specify\n",
            " |  a different behavior in training and inference:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super(MyModel, self).__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |      self.dropout = tf.keras.layers.Dropout(0.5)\n",
            " |  \n",
            " |    def call(self, inputs, training=False):\n",
            " |      x = self.dense1(inputs)\n",
            " |      if training:\n",
            " |        x = self.dropout(x, training=training)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  Once the model is created, you can config the model with losses and metrics\n",
            " |  with `model.compile()`, train the model with `model.fit()`, or use the model\n",
            " |  to do prediction with `model.predict()`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Model\n",
            " |      tensorflow.python.keras.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
            " |      tensorflow.python.keras.utils.version_utils.ModelVersionSelector\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *args, **kwargs)\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Builds the model based on input shapes received.\n",
            " |      \n",
            " |      This is to be used for subclassed models, which do not know at instantiation\n",
            " |      time what their inputs look like.\n",
            " |      \n",
            " |      This method only exists for users who want to call `model.build()` in a\n",
            " |      standalone way (as a substitute for calling the model on real data to\n",
            " |      build it). It will never be called by the framework (and thus it will\n",
            " |      never throw unexpected errors in an unrelated workflow).\n",
            " |      \n",
            " |      Args:\n",
            " |       input_shape: Single tuple, TensorShape, or list/dict of shapes, where\n",
            " |           shapes are tuples, integers, or TensorShapes.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError:\n",
            " |          1. In case of invalid user-provided data (not of type tuple,\n",
            " |             list, TensorShape, or dict).\n",
            " |          2. If the model requires call arguments that are agnostic\n",
            " |             to the input shapes (positional or kwarg in call signature).\n",
            " |          3. If not all layers were properly built.\n",
            " |          4. If float type inputs are not supported within the layers.\n",
            " |      \n",
            " |        In each of these cases, the user should build their model by calling it\n",
            " |        on real tensor data.\n",
            " |  \n",
            " |  call(self, inputs, training=None, mask=None)\n",
            " |      Calls the model on new inputs.\n",
            " |      \n",
            " |      In this case `call` just reapplies\n",
            " |      all ops in the graph to the new inputs\n",
            " |      (e.g. build a new computational graph from the provided inputs).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: A tensor or list of tensors.\n",
            " |          training: Boolean or boolean scalar tensor, indicating whether to run\n",
            " |            the `Network` in training mode or inference mode.\n",
            " |          mask: A mask or list of masks. A mask can be\n",
            " |              either a tensor or None (no mask).\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor if there is a single output, or\n",
            " |          a list of tensors if there are more than one outputs.\n",
            " |  \n",
            " |  compile(self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, **kwargs)\n",
            " |      Configures the model for training.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          optimizer: String (name of optimizer) or optimizer instance. See\n",
            " |            `tf.keras.optimizers`.\n",
            " |          loss: String (name of objective function), objective function or\n",
            " |            `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n",
            " |            function is any callable with the signature `loss = fn(y_true,\n",
            " |            y_pred)`, where y_true = ground truth values with shape =\n",
            " |            `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse\n",
            " |            categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.\n",
            " |            y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It\n",
            " |            returns a weighted loss float tensor. If a custom `Loss` instance is\n",
            " |            used and reduction is set to NONE, return value has the shape\n",
            " |            [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values;\n",
            " |            otherwise, it is a scalar. If the model has multiple outputs, you can\n",
            " |            use a different loss on each output by passing a dictionary or a list\n",
            " |            of losses. The loss value that will be minimized by the model will\n",
            " |            then be the sum of all individual losses.\n",
            " |          metrics: List of metrics to be evaluated by the model during training\n",
            " |            and testing. Each of this can be a string (name of a built-in\n",
            " |            function), function or a `tf.keras.metrics.Metric` instance. See\n",
            " |            `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n",
            " |            function is any callable with the signature `result = fn(y_true,\n",
            " |            y_pred)`. To specify different metrics for different outputs of a\n",
            " |            multi-output model, you could also pass a dictionary, such as\n",
            " |              `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n",
            " |                You can also pass a list (len = len(outputs)) of lists of metrics\n",
            " |                such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n",
            " |                `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
            " |                strings 'accuracy' or 'acc', we convert this to one of\n",
            " |                `tf.keras.metrics.BinaryAccuracy`,\n",
            " |                `tf.keras.metrics.CategoricalAccuracy`,\n",
            " |                `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n",
            " |                function used and the model output shape. We do a similar\n",
            " |                conversion for the strings 'crossentropy' and 'ce' as well.\n",
            " |          loss_weights: Optional list or dictionary specifying scalar coefficients\n",
            " |            (Python floats) to weight the loss contributions of different model\n",
            " |            outputs. The loss value that will be minimized by the model will then\n",
            " |            be the *weighted sum* of all individual losses, weighted by the\n",
            " |            `loss_weights` coefficients.\n",
            " |              If a list, it is expected to have a 1:1 mapping to the model's\n",
            " |                outputs. If a dict, it is expected to map output names (strings)\n",
            " |                to scalar coefficients.\n",
            " |          weighted_metrics: List of metrics to be evaluated and weighted by\n",
            " |            sample_weight or class_weight during training and testing.\n",
            " |          run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
            " |            logic will not be wrapped in a `tf.function`. Recommended to leave\n",
            " |            this as `None` unless your `Model` cannot be run inside a\n",
            " |            `tf.function`.\n",
            " |          steps_per_execution: Int. Defaults to 1. The number of batches to\n",
            " |            run during each `tf.function` call. Running multiple batches\n",
            " |            inside a single `tf.function` call can greatly improve performance\n",
            " |            on TPUs or small models with a large Python overhead.\n",
            " |            At most, one full epoch will be run each\n",
            " |            execution. If a number larger than the size of the epoch is passed,\n",
            " |            the execution will be truncated to the size of the epoch.\n",
            " |            Note that if `steps_per_execution` is set to `N`,\n",
            " |            `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n",
            " |            will only be called every `N` batches\n",
            " |            (i.e. before/after each `tf.function` execution).\n",
            " |          **kwargs: Arguments supported for backwards compatibility only.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: In case of invalid arguments for\n",
            " |              `optimizer`, `loss` or `metrics`.\n",
            " |  \n",
            " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False)\n",
            " |      Returns the loss value & metrics values for the model in test mode.\n",
            " |      \n",
            " |      Computation is done in batches (see the `batch_size` arg.)\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely). If\n",
            " |            `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\n",
            " |            should not be specified (since targets will be obtained from the\n",
            " |            iterator/dataset).\n",
            " |          batch_size: Integer or `None`. Number of samples per batch of\n",
            " |            computation. If unspecified, `batch_size` will default to 32. Do not\n",
            " |            specify the `batch_size` if your data is in the form of a dataset,\n",
            " |            generators, or `keras.utils.Sequence` instances (since they generate\n",
            " |            batches).\n",
            " |          verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n",
            " |          sample_weight: Optional Numpy array of weights for the test samples,\n",
            " |            used for weighting the loss function. You can either pass a flat (1D)\n",
            " |            Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples), or in the case of\n",
            " |                temporal data, you can pass a 2D array with shape `(samples,\n",
            " |                sequence_length)`, to apply a different weight to every timestep\n",
            " |                of every sample. This argument is not supported when `x` is a\n",
            " |                dataset, instead pass sample weights as the third element of `x`.\n",
            " |          steps: Integer or `None`. Total number of steps (batches of samples)\n",
            " |            before declaring the evaluation round finished. Ignored with the\n",
            " |            default value of `None`. If x is a `tf.data` dataset and `steps` is\n",
            " |            None, 'evaluate' will run until the dataset is exhausted. This\n",
            " |            argument is not supported with array inputs.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances. List of\n",
            " |            callbacks to apply during evaluation. See\n",
            " |            [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |            input only. Maximum size for the generator queue. If unspecified,\n",
            " |            `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |            only. Maximum number of processes to spin up when using process-based\n",
            " |            threading. If unspecified, `workers` will default to 1. If 0, will\n",
            " |            execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |            threading. If unspecified, `use_multiprocessing` will default to\n",
            " |            `False`. Note that because this implementation relies on\n",
            " |            multiprocessing, you should not pass non-picklable arguments to the\n",
            " |            generator as they can't be passed easily to children processes.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.evaluate` is wrapped in `tf.function`.\n",
            " |          ValueError: in case of invalid arguments.\n",
            " |  \n",
            " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Evaluates the model on a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.evaluate` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given below.\n",
            " |          y: Target data. Like the input data `x`,\n",
            " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
            " |            tensor targets, or inversely). If `x` is a dataset, generator,\n",
            " |            or `keras.utils.Sequence` instance, `y` should\n",
            " |            not be specified (since targets will be obtained from `x`).\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per gradient update.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          epochs: Integer. Number of epochs to train the model.\n",
            " |              An epoch is an iteration over the entire `x` and `y`\n",
            " |              data provided.\n",
            " |              Note that in conjunction with `initial_epoch`,\n",
            " |              `epochs` is to be understood as \"final epoch\".\n",
            " |              The model is not trained for a number of iterations\n",
            " |              given by `epochs`, but merely until the epoch\n",
            " |              of index `epochs` is reached.\n",
            " |          verbose: 0, 1, or 2. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            " |              Note that the progress bar is not particularly useful when\n",
            " |              logged to a file, so verbose=2 is recommended when not running\n",
            " |              interactively (eg, in a production environment).\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training.\n",
            " |              See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
            " |              and `tf.keras.callbacks.History` callbacks are created automatically\n",
            " |              and need not be passed into `model.fit`.\n",
            " |              `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
            " |              `verbose` argument to `model.fit`.\n",
            " |          validation_split: Float between 0 and 1.\n",
            " |              Fraction of the training data to be used as validation data.\n",
            " |              The model will set apart this fraction of the training data,\n",
            " |              will not train on it, and will evaluate\n",
            " |              the loss and any model metrics\n",
            " |              on this data at the end of each epoch.\n",
            " |              The validation data is selected from the last samples\n",
            " |              in the `x` and `y` data provided, before shuffling. This argument is\n",
            " |              not supported when `x` is a dataset, generator or\n",
            " |             `keras.utils.Sequence` instance.\n",
            " |          validation_data: Data on which to evaluate\n",
            " |              the loss and any model metrics at the end of each epoch.\n",
            " |              The model will not be trained on this data. Thus, note the fact\n",
            " |              that the validation loss of data provided using `validation_split`\n",
            " |              or `validation_data` is not affected by regularization layers like\n",
            " |              noise and dropout.\n",
            " |              `validation_data` will override `validation_split`.\n",
            " |              `validation_data` could be:\n",
            " |                - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
            " |                - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
            " |                - dataset\n",
            " |              For the first two cases, `batch_size` must be provided.\n",
            " |              For the last case, `validation_steps` could be provided.\n",
            " |              Note that `validation_data` does not support all the data types that\n",
            " |              are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
            " |          shuffle: Boolean (whether to shuffle the training data\n",
            " |              before each epoch) or str (for 'batch'). This argument is ignored\n",
            " |              when `x` is a generator. 'batch' is a special option for dealing\n",
            " |              with the limitations of HDF5 data; it shuffles in batch-sized\n",
            " |              chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers)\n",
            " |              to a weight (float) value, used for weighting the loss function\n",
            " |              (during training only).\n",
            " |              This can be useful to tell the model to\n",
            " |              \"pay more attention\" to samples from\n",
            " |              an under-represented class.\n",
            " |          sample_weight: Optional Numpy array of weights for\n",
            " |              the training samples, used for weighting the loss function\n",
            " |              (during training only). You can either pass a flat (1D)\n",
            " |              Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples),\n",
            " |              or in the case of temporal data,\n",
            " |              you can pass a 2D array with shape\n",
            " |              `(samples, sequence_length)`,\n",
            " |              to apply a different weight to every timestep of every sample. This\n",
            " |              argument is not supported when `x` is a dataset, generator, or\n",
            " |             `keras.utils.Sequence` instance, instead provide the sample_weights\n",
            " |              as the third element of `x`.\n",
            " |          initial_epoch: Integer.\n",
            " |              Epoch at which to start training\n",
            " |              (useful for resuming a previous training run).\n",
            " |          steps_per_epoch: Integer or `None`.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              before declaring one epoch finished and starting the\n",
            " |              next epoch. When training with input tensors such as\n",
            " |              TensorFlow data tensors, the default `None` is equal to\n",
            " |              the number of samples in your dataset divided by\n",
            " |              the batch size, or 1 if that cannot be determined. If x is a\n",
            " |              `tf.data` dataset, and 'steps_per_epoch'\n",
            " |              is None, the epoch will run until the input dataset is exhausted.\n",
            " |              When passing an infinitely repeating dataset, you must specify the\n",
            " |              `steps_per_epoch` argument. This argument is not supported with\n",
            " |              array inputs.\n",
            " |          validation_steps: Only relevant if `validation_data` is provided and\n",
            " |              is a `tf.data` dataset. Total number of steps (batches of\n",
            " |              samples) to draw before stopping when performing validation\n",
            " |              at the end of every epoch. If 'validation_steps' is None, validation\n",
            " |              will run until the `validation_data` dataset is exhausted. In the\n",
            " |              case of an infinitely repeated dataset, it will run into an\n",
            " |              infinite loop. If 'validation_steps' is specified and only part of\n",
            " |              the dataset will be consumed, the evaluation will start from the\n",
            " |              beginning of the dataset at each epoch. This ensures that the same\n",
            " |              validation samples are used every time.\n",
            " |          validation_batch_size: Integer or `None`.\n",
            " |              Number of samples per validation batch.\n",
            " |              If unspecified, will default to `batch_size`.\n",
            " |              Do not specify the `validation_batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          validation_freq: Only relevant if validation data is provided. Integer\n",
            " |              or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
            " |              If an integer, specifies how many training epochs to run before a\n",
            " |              new validation run is performed, e.g. `validation_freq=2` runs\n",
            " |              validation every 2 epochs. If a Container, specifies the epochs on\n",
            " |              which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            " |              validation at the end of the 1st, 2nd, and 10th epochs.\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up\n",
            " |              when using process-based threading. If unspecified, `workers`\n",
            " |              will default to 1. If 0, will execute the generator on the main\n",
            " |              thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      Unpacking behavior for iterator-like inputs:\n",
            " |          A common pattern is to pass a tf.data.Dataset, generator, or\n",
            " |        tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            " |        yield not only features (x) but optionally targets (y) and sample weights.\n",
            " |        Keras requires that the output of such iterator-likes be unambiguous. The\n",
            " |        iterator should return a tuple of length 1, 2, or 3, where the optional\n",
            " |        second and third elements will be used for y and sample_weight\n",
            " |        respectively. Any other type provided will be wrapped in a length one\n",
            " |        tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
            " |        should still adhere to the top-level tuple structure.\n",
            " |        e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            " |        features, targets, and weights from the keys of a single dict.\n",
            " |          A notable unsupported data type is the namedtuple. The reason is that\n",
            " |        it behaves like both an ordered datatype (tuple) and a mapping\n",
            " |        datatype (dict). So given a namedtuple of the form:\n",
            " |            `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            " |        it is ambiguous whether to reverse the order of the elements when\n",
            " |        interpreting the value. Even worse is a tuple of the form:\n",
            " |            `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            " |        where it is unclear if the tuple was intended to be unpacked into x, y,\n",
            " |        and sample_weight or passed through as a single element to `x`. As a\n",
            " |        result the data processing code will simply raise a ValueError if it\n",
            " |        encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          A `History` object. Its `History.history` attribute is\n",
            " |          a record of training loss values and metrics values\n",
            " |          at successive epochs, as well as validation loss values\n",
            " |          and validation metrics values (if applicable).\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: 1. If the model was never compiled or,\n",
            " |          2. If `model.fit` is  wrapped in `tf.function`.\n",
            " |      \n",
            " |          ValueError: In case of mismatch between the provided input data\n",
            " |              and what the model expects or when the input data is empty.\n",
            " |  \n",
            " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
            " |      Fits the model on data yielded batch-by-batch by a Python generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.fit` now supports generators, so there is no longer any need to use\n",
            " |        this endpoint.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  get_layer(self, name=None, index=None)\n",
            " |      Retrieves a layer based on either its name (unique) or index.\n",
            " |      \n",
            " |      If `name` and `index` are both provided, `index` will take precedence.\n",
            " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          name: String, name of layer.\n",
            " |          index: Integer, index of layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: In case of invalid layer name or index.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Retrieves the weights of the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A flat list of Numpy arrays.\n",
            " |  \n",
            " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None)\n",
            " |      Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n",
            " |      \n",
            " |      If `by_name` is False weights are loaded based on the network's\n",
            " |      topology. This means the architecture should be the same as when the weights\n",
            " |      were saved.  Note that layers that don't have weights are not taken into\n",
            " |      account in the topological ordering, so adding or removing layers is fine as\n",
            " |      long as they don't have weights.\n",
            " |      \n",
            " |      If `by_name` is True, weights are loaded into layers only if they share the\n",
            " |      same name. This is useful for fine-tuning or transfer-learning models where\n",
            " |      some of the layers have changed.\n",
            " |      \n",
            " |      Only topological loading (`by_name=False`) is supported when loading weights\n",
            " |      from the TensorFlow format. Note that topological loading differs slightly\n",
            " |      between TensorFlow and HDF5 formats for user-defined classes inheriting from\n",
            " |      `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n",
            " |      TensorFlow format loads based on the object-local names of attributes to\n",
            " |      which layers are assigned in the `Model`'s constructor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String, path to the weights file to load. For weight files in\n",
            " |              TensorFlow format, this is the file prefix (the same as was passed\n",
            " |              to `save_weights`).\n",
            " |          by_name: Boolean, whether to load weights by name or by topological\n",
            " |              order. Only topological loading is supported for weight files in\n",
            " |              TensorFlow format.\n",
            " |          skip_mismatch: Boolean, whether to skip loading of layers where there is\n",
            " |              a mismatch in the number of weights, or a mismatch in the shape of\n",
            " |              the weight (only valid when `by_name=True`).\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for loading weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          When loading a weight file in TensorFlow format, returns the same status\n",
            " |          object as `tf.train.Checkpoint.restore`. When graph building, restore\n",
            " |          ops are run automatically as soon as the network is built (on first call\n",
            " |          for user-defined classes inheriting from `Model`, immediately if it is\n",
            " |          already built).\n",
            " |      \n",
            " |          When loading weights in HDF5 format, returns `None`.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If h5py is not available and the weight file is in HDF5\n",
            " |              format.\n",
            " |          ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n",
            " |            `False`.\n",
            " |  \n",
            " |  make_predict_function(self)\n",
            " |      Creates a function that executes one step of inference.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.predict` and `Model.predict_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.predict_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.predict` or\n",
            " |      `Model.predict_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return the outputs of the `Model`.\n",
            " |  \n",
            " |  make_test_function(self)\n",
            " |      Creates a function that executes one step of evaluation.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.evaluate` and `Model.test_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.test_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.evaluate` or\n",
            " |      `Model.test_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_test_batch_end`.\n",
            " |  \n",
            " |  make_train_function(self)\n",
            " |      Creates a function that executes one step of training.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.fit` and `Model.train_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual training\n",
            " |      logic to `Model.train_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.fit` or\n",
            " |      `Model.train_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Generates output predictions for the input samples.\n",
            " |      \n",
            " |      Computation is done in batches. This method is designed for performance in\n",
            " |      large scale inputs. For small amount of inputs that fit in one batch,\n",
            " |      directly using `__call__` is recommended for faster execution, e.g.,\n",
            " |      `model(x)`, or `model(x, training=False)` if you have layers such as\n",
            " |      `tf.keras.layers.BatchNormalization` that behaves differently during\n",
            " |      inference. Also, note the fact that test loss is not affected by\n",
            " |      regularization layers like noise and dropout.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input samples. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A `tf.data` dataset.\n",
            " |            - A generator or `keras.utils.Sequence` instance.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per batch.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of dataset, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          verbose: Verbosity mode, 0 or 1.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              before declaring the prediction round finished.\n",
            " |              Ignored with the default value of `None`. If x is a `tf.data`\n",
            " |              dataset and `steps` is None, `predict` will\n",
            " |              run until the input dataset is exhausted.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during prediction.\n",
            " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up when using\n",
            " |              process-based threading. If unspecified, `workers` will default\n",
            " |              to 1. If 0, will execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`. Note that Model.predict uses the same interpretation rules as\n",
            " |      `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n",
            " |      three methods.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of mismatch between the provided\n",
            " |              input data and the model's expectations,\n",
            " |              or in case a stateful model receives a number of samples\n",
            " |              that is not a multiple of the batch size.\n",
            " |  \n",
            " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Generates predictions for the input samples from a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.predict` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  predict_on_batch(self, x)\n",
            " |      Returns predictions for a single batch of samples.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
            " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
            " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict_on_batch` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of mismatch between given number of inputs and\n",
            " |            expectations of the model.\n",
            " |  \n",
            " |  predict_step(self, data)\n",
            " |      The logic for one inference step.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.make_predict_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of inference.\n",
            " |      This typically includes the forward pass.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_predict_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The result of one inference step, typically the output of calling the\n",
            " |        `Model` on data.\n",
            " |  \n",
            " |  reset_metrics(self)\n",
            " |      Resets the state of all the metrics in the model.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> _ = model.fit(x, y, verbose=0)\n",
            " |      >>> assert all(float(m.result()) for m in model.metrics)\n",
            " |      \n",
            " |      >>> model.reset_metrics()\n",
            " |      >>> assert all(float(m.result()) == 0 for m in model.metrics)\n",
            " |  \n",
            " |  reset_states(self)\n",
            " |  \n",
            " |  save(self, filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None, save_traces=True)\n",
            " |      Saves the model to Tensorflow SavedModel or a single HDF5 file.\n",
            " |      \n",
            " |      Please see `tf.keras.models.save_model` or the\n",
            " |      [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)\n",
            " |      for details.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String, PathLike, path to SavedModel or H5 file to save the\n",
            " |              model.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          include_optimizer: If True, save optimizer's state together.\n",
            " |          save_format: Either `'tf'` or `'h5'`, indicating whether to save the\n",
            " |              model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,\n",
            " |              and 'h5' in TF 1.X.\n",
            " |          signatures: Signatures to save with the SavedModel. Applicable to the\n",
            " |              'tf' format only. Please see the `signatures` argument in\n",
            " |              `tf.saved_model.save` for details.\n",
            " |          options: (only applies to SavedModel format)\n",
            " |              `tf.saved_model.SaveOptions` object that specifies options for\n",
            " |              saving to SavedModel.\n",
            " |          save_traces: (only applies to SavedModel format) When enabled, the\n",
            " |              SavedModel will store the function traces for each layer. This\n",
            " |              can be disabled, so that only the configs of each layer are stored.\n",
            " |              Defaults to `True`. Disabling this will decrease serialization time\n",
            " |              and reduce file size, but it requires that all custom layers/models\n",
            " |              implement a `get_config()` method.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      from keras.models import load_model\n",
            " |      \n",
            " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
            " |      del model  # deletes the existing model\n",
            " |      \n",
            " |      # returns a compiled model\n",
            " |      # identical to the previous one\n",
            " |      model = load_model('my_model.h5')\n",
            " |      ```\n",
            " |  \n",
            " |  save_weights(self, filepath, overwrite=True, save_format=None, options=None)\n",
            " |      Saves all layer weights.\n",
            " |      \n",
            " |      Either saves in HDF5 or in TensorFlow format based on the `save_format`\n",
            " |      argument.\n",
            " |      \n",
            " |      When saving in HDF5 format, the weight file has:\n",
            " |        - `layer_names` (attribute), a list of strings\n",
            " |            (ordered names of model layers).\n",
            " |        - For every layer, a `group` named `layer.name`\n",
            " |            - For every such layer group, a group attribute `weight_names`,\n",
            " |                a list of strings\n",
            " |                (ordered names of weights tensor of the layer).\n",
            " |            - For every weight in the layer, a dataset\n",
            " |                storing the weight value, named after the weight tensor.\n",
            " |      \n",
            " |      When saving in TensorFlow format, all objects referenced by the network are\n",
            " |      saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n",
            " |      instances or `Optimizer` instances assigned to object attributes. For\n",
            " |      networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n",
            " |      outputs)`, `Layer` instances used by the network are tracked/saved\n",
            " |      automatically. For user-defined classes which inherit from `tf.keras.Model`,\n",
            " |      `Layer` instances must be assigned to object attributes, typically in the\n",
            " |      constructor. See the documentation of `tf.train.Checkpoint` and\n",
            " |      `tf.keras.Model` for details.\n",
            " |      \n",
            " |      While the formats are the same, do not mix `save_weights` and\n",
            " |      `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n",
            " |      loaded using `Model.load_weights`. Checkpoints saved using\n",
            " |      `tf.train.Checkpoint.save` should be restored using the corresponding\n",
            " |      `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n",
            " |      `save_weights` for training checkpoints.\n",
            " |      \n",
            " |      The TensorFlow format matches objects and variables by starting at a root\n",
            " |      object, `self` for `save_weights`, and greedily matching attribute\n",
            " |      names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n",
            " |      is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n",
            " |      means saving a `tf.keras.Model` using `save_weights` and loading into a\n",
            " |      `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n",
            " |      the `Model`'s variables. See the [guide to training\n",
            " |      checkpoints](https://www.tensorflow.org/guide/checkpoint) for details\n",
            " |      on the TensorFlow format.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String or PathLike, path to the file to save the weights to.\n",
            " |              When saving in TensorFlow format, this is the prefix used for\n",
            " |              checkpoint files (multiple files are generated). Note that the '.h5'\n",
            " |              suffix causes weights to be saved in HDF5 format.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n",
            " |              '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n",
            " |              `None` defaults to 'tf'.\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for saving weights.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If h5py is not available when attempting to save in HDF5\n",
            " |              format.\n",
            " |          ValueError: For invalid/unknown format arguments.\n",
            " |  \n",
            " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
            " |      Prints a string summary of the network.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          line_length: Total length of printed lines\n",
            " |              (e.g. set this to adapt the display to different\n",
            " |              terminal window sizes).\n",
            " |          positions: Relative or absolute positions of log elements\n",
            " |              in each line. If not provided,\n",
            " |              defaults to `[.33, .55, .67, 1.]`.\n",
            " |          print_fn: Print function to use. Defaults to `print`.\n",
            " |              It will be called on each line of the summary.\n",
            " |              You can set it to a custom function\n",
            " |              in order to capture the string summary.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if `summary()` is called before the model is built.\n",
            " |  \n",
            " |  test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Test the model on a single batch of samples.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
            " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
            " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors, if\n",
            " |            the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of invalid user-provided arguments.\n",
            " |  \n",
            " |  test_step(self, data)\n",
            " |      The logic for one evaluation step.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.make_test_function`.\n",
            " |      \n",
            " |      This function should contain the mathematical logic for one step of\n",
            " |      evaluation.\n",
            " |      This typically includes the forward pass, loss calculation, and metrics\n",
            " |      updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_test_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned.\n",
            " |  \n",
            " |  to_json(self, **kwargs)\n",
            " |      Returns a JSON string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a JSON save file, use\n",
            " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `json.dumps()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A JSON string.\n",
            " |  \n",
            " |  to_yaml(self, **kwargs)\n",
            " |      Returns a yaml string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a yaml save file, use\n",
            " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
            " |      \n",
            " |      `custom_objects` should be a dictionary mapping\n",
            " |      the names of custom losses / layers / etc to the corresponding\n",
            " |      functions / classes.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `yaml.dump()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A YAML string.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: if yaml module is not found.\n",
            " |  \n",
            " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Runs a single gradient update on a single batch of data.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |                if the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers) to a\n",
            " |            weight (float) to apply to the model's loss for the samples from this\n",
            " |            class during training. This can be useful to tell the model to \"pay\n",
            " |            more attention\" to samples from an under-represented class.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar training loss\n",
            " |          (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If `model.train_on_batch` is wrapped in `tf.function`.\n",
            " |        ValueError: In case of invalid user-provided arguments.\n",
            " |  \n",
            " |  train_step(self, data)\n",
            " |      The logic for one training step.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.make_train_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of training.\n",
            " |      This typically includes the forward pass, loss calculation, backpropagation,\n",
            " |      and metric updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_train_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned. Example:\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_config(config, custom_objects=None) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  distribute_strategy\n",
            " |      The `tf.distribute.Strategy` this model was created under.\n",
            " |  \n",
            " |  layers\n",
            " |  \n",
            " |  metrics\n",
            " |      Returns the model's metrics added using `compile`, `add_metric` APIs.\n",
            " |      \n",
            " |      Note: Metrics passed to `compile()` are available only after a `keras.Model`\n",
            " |      has been trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.add_metric(\n",
            " |      ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc', 'mean']\n",
            " |  \n",
            " |  metrics_names\n",
            " |      Returns the model's display labels for all outputs.\n",
            " |      \n",
            " |      Note: `metrics_names` are available only after a `keras.Model` has been\n",
            " |      trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> model.metrics_names\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc']\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Non-trainable weights are *not* updated during training. They are expected\n",
            " |      to be updated manually in `call()`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of non-trainable variables.\n",
            " |  \n",
            " |  run_eagerly\n",
            " |      Settable attribute indicating whether the model should run eagerly.\n",
            " |      \n",
            " |      Running eagerly means that your model will be run step by step,\n",
            " |      like Python code. Your model might run slower, but it should become easier\n",
            " |      for you to debug it by stepping into individual layer calls.\n",
            " |      \n",
            " |      By default, we will attempt to compile your model to a static graph to\n",
            " |      deliver the best execution performance.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Boolean, whether the model should run eagerly.\n",
            " |  \n",
            " |  state_updates\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Returns the `updates` from all layers that are stateful.\n",
            " |      \n",
            " |      This is useful for separating training updates and\n",
            " |      state updates, e.g. when we need to update a layer's internal state\n",
            " |      during prediction.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of update ops.\n",
            " |  \n",
            " |  trainable_weights\n",
            " |      List of all trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Trainable weights are updated via gradient descent during training.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of trainable variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        *args: Positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
            " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_loss(self, losses, **kwargs)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be dependent\n",
            " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
            " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
            " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(self, inputs):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any loss Tensors passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      losses become part of the model's topology and are tracked in `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Activity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss references\n",
            " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
            " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
            " |      topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      d = tf.keras.layers.Dense(10)\n",
            " |      x = d(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      Arguments:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
            " |          may also be zero-argument callables which create a loss tensor.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |            inputs - Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_metric(self, value, name=None, **kwargs)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      This method can be used inside the `call()` method of a subclassed layer\n",
            " |      or model.\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
            " |        def __init__(self):\n",
            " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
            " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
            " |      \n",
            " |        def call(self, inputs):\n",
            " |          self.add_metric(self.mean(x))\n",
            " |          self.add_metric(tf.reduce_sum(x), name='metric_2')\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any tensor passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      metrics become part of the model's topology and are tracked when you\n",
            " |      save the model via `save()`.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
            " |      Functional Model, as shown in the example below, is not supported. This is\n",
            " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        name: String metric name.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |          `aggregation` - When the `value` tensor provided is not the result of\n",
            " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
            " |          using a `keras.Metric.Mean`.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Add update op(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and variance\n",
            " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
            " |      when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case, variable\n",
            " |      updates are run on the fly and thus do not need to be tracked for later\n",
            " |      execution).\n",
            " |      \n",
            " |      Arguments:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |        inputs: Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        use_resource: Whether to use `ResourceVariable`.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            " |          `AUTO` and the current `DistributionStrategy` chooses\n",
            " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
            " |          `trainable` must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The variable created.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
            " |  \n",
            " |  apply(self, inputs, *args, **kwargs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      This is an alias of `self.__call__`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      If the layer has not been built, this method will call `build` on the\n",
            " |      layer. This assumes that the layer will later be used with inputs that\n",
            " |      match the input shape provided here.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
            " |          how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves losses relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of loss tensors of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves updates relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of update ops of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from Numpy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      sets the weight values from numpy arrays. The weight values should be\n",
            " |      passed in the order they are created by the layer. Note that the layer's\n",
            " |      weights must be instantiated before calling this function by calling\n",
            " |      the layer.\n",
            " |      \n",
            " |      For example, a Dense layer returns a list of two values-- per-output\n",
            " |      weights and the bias value. These can be used to set the weights of another\n",
            " |      Dense layer:\n",
            " |      \n",
            " |      >>> a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b.set_weights(a.get_weights())\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Arguments:\n",
            " |          weights: a list of Numpy arrays. The number\n",
            " |              of arrays and their shape must match\n",
            " |              number of the dimensions of the weights\n",
            " |              of the layer (i.e. it should match the\n",
            " |              output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the provided weights list does not match the\n",
            " |              layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  compute_dtype\n",
            " |      The dtype of the layer's computations.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
            " |      the weights.\n",
            " |      \n",
            " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
            " |      computations and the output to be in the compute dtype as well. This is done\n",
            " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
            " |      these casts if implementing your own layer.\n",
            " |      \n",
            " |      Layers often perform certain internal computations in higher precision when\n",
            " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
            " |      will still typically be float16 or bfloat16 in such cases.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The layer's compute dtype.\n",
            " |  \n",
            " |  dtype\n",
            " |      The dtype of the layer weights.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
            " |      dtype of the layer's computations.\n",
            " |  \n",
            " |  dtype_policy\n",
            " |      The dtype policy associated with this layer.\n",
            " |      \n",
            " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
            " |  \n",
            " |  dynamic\n",
            " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  input_spec\n",
            " |      `InputSpec` instance(s) describing the input format for this layer.\n",
            " |      \n",
            " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
            " |      the layer to run input compatibility checks when it is called.\n",
            " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
            " |      of rank 4. As such, you can set, in `__init__()`:\n",
            " |      \n",
            " |      ```python\n",
            " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
            " |      ```\n",
            " |      \n",
            " |      Now, if you try to call the layer on an input that isn't rank 4\n",
            " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
            " |      error:\n",
            " |      \n",
            " |      ```\n",
            " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
            " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
            " |      ```\n",
            " |      \n",
            " |      Input checks that can be specified via `input_spec` include:\n",
            " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
            " |      - Shape\n",
            " |      - Rank (ndim)\n",
            " |      - Dtype\n",
            " |      \n",
            " |      For more information, see `tf.keras.layers.InputSpec`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
            " |  \n",
            " |  losses\n",
            " |      List of losses added using the `add_loss()` API.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is accessed,\n",
            " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
            " |      propagate gradients back to the corresponding variables.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
            " |      ...   def call(self, inputs):\n",
            " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |      ...     return inputs\n",
            " |      >>> l = MyLayer()\n",
            " |      >>> l(np.ones((10, 1)))\n",
            " |      >>> l.losses\n",
            " |      [1.0]\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Activity regularization.\n",
            " |      >>> len(model.losses)\n",
            " |      0\n",
            " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      >>> len(model.losses)\n",
            " |      1\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
            " |      >>> x = d(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Weight regularization.\n",
            " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      >>> model.losses\n",
            " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  name\n",
            " |      Name of the layer (string), set in the constructor.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  supports_masking\n",
            " |      Whether this layer supports computing a mask using `compute_mask`.\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variable_dtype\n",
            " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      >>> class MyModule(tf.Module):\n",
            " |      ...   @tf.Module.with_name_scope\n",
            " |      ...   def __call__(self, x):\n",
            " |      ...     if not hasattr(self, 'w'):\n",
            " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
            " |      ...     return tf.matmul(x, self.w)\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      >>> mod = MyModule()\n",
            " |      >>> mod(tf.ones([1, 2]))\n",
            " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
            " |      >>> mod.w\n",
            " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
            " |      numpy=..., dtype=float32)>\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      >>> a = tf.Module()\n",
            " |      >>> b = tf.Module()\n",
            " |      >>> c = tf.Module()\n",
            " |      >>> a.b = b\n",
            " |      >>> b.c = c\n",
            " |      >>> list(a.submodules) == [b, c]\n",
            " |      True\n",
            " |      >>> list(b.submodules) == [c]\n",
            " |      True\n",
            " |      >>> list(c.submodules) == []\n",
            " |      True\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "2ex5iFJiHkDR",
        "outputId": "0ef6bc23-e7db-47f1-e27c-bca93fa986dc"
      },
      "source": [
        "# 3.4 Display model now\r\n",
        "# Ref: https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model\r\n",
        "plot_model(model, show_shapes= True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7X1WpoaJ8vZ",
        "outputId": "5cf50d05-5574-4e91-dad9-5e0ae71c6bc8"
      },
      "source": [
        "# 3.5 Compile model\r\n",
        "model.compile(loss = \"mean_squared_error\")\r\n",
        "history = model.fit(\r\n",
        "                    X_train,\r\n",
        "                    y_train,\r\n",
        "                    epochs = 10,\r\n",
        "                    verbose = 1\r\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5214\n",
            "Epoch 2/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4380\n",
            "Epoch 3/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5411\n",
            "Epoch 4/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4837\n",
            "Epoch 5/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4958\n",
            "Epoch 6/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4681\n",
            "Epoch 7/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4758\n",
            "Epoch 8/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4888\n",
            "Epoch 9/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4339\n",
            "Epoch 10/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QQ8xAgGLTOo",
        "outputId": "e958e9a0-cc59-45e4-9cd2-0388954fbfec"
      },
      "source": [
        "3.6 model.evaluate(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129/129 [==============================] - 0s 863us/step - loss: 2.4378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4377660751342773"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYuF2BagAAYt"
      },
      "source": [
        "# Wide and Deep Network--IInd version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYrB04GJNwrN"
      },
      "source": [
        "# 4.0 We have two inputs\r\n",
        "inputsA = tf.keras.Input(shape = X_train[:,:4].shape[1:])\r\n",
        "inputsB = tf.keras.Input(shape = X_train[:,1:8].shape[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1iVJhhcRTIi"
      },
      "source": [
        "# 4.1 One arm of network\r\n",
        "x = layers.Dense(100, activation = 'relu')(inputsB)\r\n",
        "x = layers.Dense(100,activation= 'relu')(x)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6iLjl8gSa69"
      },
      "source": [
        "# 4.2 Concatenate one input with output of another arm\r\n",
        "concat = layers.concatenate([x,inputsA])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG3SQHycS1Tu"
      },
      "source": [
        "# 4.3 Output layer\r\n",
        "out = layers.Dense(1,activation = 'sigmoid')(concat)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x90xNBumS_bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d320e37-778a-4ed9-aebf-19ce933f06eb"
      },
      "source": [
        "# 4.4 Create model and show summary\r\n",
        "model2 = Model(inputs = [inputsA,inputsB], outputs = [out])\r\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_18 (InputLayer)           [(None, 7)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 100)          800         input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 100)          10100       dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_17 (InputLayer)           [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 104)          0           dense_29[0][0]                   \n",
            "                                                                 input_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 1)            105         concatenate_9[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 11,005\n",
            "Trainable params: 11,005\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWwLnTskTPDi",
        "outputId": "5f8a2cf2-8fd6-492b-c513-1514f58fd354"
      },
      "source": [
        "# 4.5 Compile model\r\n",
        "model2.compile(\r\n",
        "               optimizer='rmsprop', \r\n",
        "               loss = 'mean_squared_error'\r\n",
        "               )\r\n",
        "\r\n",
        "# 4.6 Train the model now.\r\n",
        "#     Note the two train inputs\r\n",
        "model2.fit(\r\n",
        "            [X_train[:,:4], X_train[:,1:8]],\r\n",
        "            y_train,\r\n",
        "            epochs = 100\r\n",
        "           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.6578\n",
            "Epoch 2/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4948\n",
            "Epoch 3/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4901\n",
            "Epoch 4/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4797\n",
            "Epoch 5/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4541\n",
            "Epoch 6/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4691\n",
            "Epoch 7/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4731\n",
            "Epoch 8/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4998\n",
            "Epoch 9/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5091\n",
            "Epoch 10/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4542\n",
            "Epoch 11/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4512\n",
            "Epoch 12/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4789\n",
            "Epoch 13/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4811\n",
            "Epoch 14/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4869\n",
            "Epoch 15/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5363\n",
            "Epoch 16/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4781\n",
            "Epoch 17/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5037\n",
            "Epoch 18/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4610\n",
            "Epoch 19/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4809\n",
            "Epoch 20/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5035\n",
            "Epoch 21/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4914\n",
            "Epoch 22/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4429\n",
            "Epoch 23/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4965\n",
            "Epoch 24/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4645\n",
            "Epoch 25/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4459\n",
            "Epoch 26/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4714\n",
            "Epoch 27/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4697\n",
            "Epoch 28/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5143\n",
            "Epoch 29/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4575\n",
            "Epoch 30/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5729\n",
            "Epoch 31/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4698\n",
            "Epoch 32/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5025\n",
            "Epoch 33/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5378\n",
            "Epoch 34/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4445\n",
            "Epoch 35/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4885\n",
            "Epoch 36/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4371\n",
            "Epoch 37/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4203\n",
            "Epoch 38/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4827\n",
            "Epoch 39/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5119\n",
            "Epoch 40/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4790\n",
            "Epoch 41/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4559\n",
            "Epoch 42/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4504\n",
            "Epoch 43/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5163\n",
            "Epoch 44/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4440\n",
            "Epoch 45/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4931\n",
            "Epoch 46/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5242\n",
            "Epoch 47/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5359\n",
            "Epoch 48/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4938\n",
            "Epoch 49/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5103\n",
            "Epoch 50/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4931\n",
            "Epoch 51/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5341\n",
            "Epoch 52/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5141\n",
            "Epoch 53/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5196\n",
            "Epoch 54/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4987\n",
            "Epoch 55/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5157\n",
            "Epoch 56/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5070\n",
            "Epoch 57/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4943\n",
            "Epoch 58/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4640\n",
            "Epoch 59/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5108\n",
            "Epoch 60/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5197\n",
            "Epoch 61/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4418\n",
            "Epoch 62/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4828\n",
            "Epoch 63/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4737\n",
            "Epoch 64/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4872\n",
            "Epoch 65/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4442\n",
            "Epoch 66/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4393\n",
            "Epoch 67/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4949\n",
            "Epoch 68/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4189\n",
            "Epoch 69/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4954\n",
            "Epoch 70/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4745\n",
            "Epoch 71/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4598\n",
            "Epoch 72/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4241\n",
            "Epoch 73/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5375\n",
            "Epoch 74/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5019\n",
            "Epoch 75/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4823\n",
            "Epoch 76/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5538\n",
            "Epoch 77/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4443\n",
            "Epoch 78/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4503\n",
            "Epoch 79/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4703\n",
            "Epoch 80/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5018\n",
            "Epoch 81/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4757\n",
            "Epoch 82/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4117\n",
            "Epoch 83/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4730\n",
            "Epoch 84/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5220\n",
            "Epoch 85/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4587\n",
            "Epoch 86/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4905\n",
            "Epoch 87/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5263\n",
            "Epoch 88/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4712\n",
            "Epoch 89/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4886\n",
            "Epoch 90/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4464\n",
            "Epoch 91/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4934\n",
            "Epoch 92/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5552\n",
            "Epoch 93/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4752\n",
            "Epoch 94/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5213\n",
            "Epoch 95/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4983\n",
            "Epoch 96/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4724\n",
            "Epoch 97/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4810\n",
            "Epoch 98/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4708\n",
            "Epoch 99/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4802\n",
            "Epoch 100/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe6993e0668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "F5MvbvlUTYd3",
        "outputId": "e3d494d5-8388-4ab8-8504-8e0cc280d043"
      },
      "source": [
        "# 4.7 Plot our model\r\n",
        "plot_model(model2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqjA96pAERaO"
      },
      "source": [
        "# Two inputs and two outputs model--IIIrd ver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Bg8Lm2Ei0y",
        "outputId": "9932c417-9ce6-4c01-9ba8-f1676a69e98a"
      },
      "source": [
        "# 5.0 We have two inputs.\r\n",
        "#     To distiguish them, we give names to each\r\n",
        "inputsA = tf.keras.Input(\r\n",
        "                          shape = X_train[:,:4].shape[1:],\r\n",
        "                          name = \"in_a\"\r\n",
        "                         )\r\n",
        "\r\n",
        "# 5.1\r\n",
        "inputsB = tf.keras.Input(\r\n",
        "                          shape = X_train[:,1:8].shape[1:],\r\n",
        "                          name = \"in_b\"\r\n",
        "                        )\r\n",
        "\r\n",
        "\r\n",
        "# 5.2 One arm of network\r\n",
        "x = layers.Dense(100, activation = 'relu')(inputsB)\r\n",
        "x = layers.Dense(100,activation= 'relu')(x)\r\n",
        "\r\n",
        "# 5.3 Concatenate an input with output of one arm\r\n",
        "concat = layers.concatenate([x,inputsA])\r\n",
        "\r\n",
        "# 5.4 Output layers\r\n",
        "#     We have two output layers. To distiguish them, we give names to each\r\n",
        "out_x = layers.Dense(1,activation = 'sigmoid' , name = \"out_a\")(concat)\r\n",
        "out_y = layers.Dense(1,activation = 'sigmoid', name = \"out_b\")(x)\r\n",
        "\r\n",
        "# 5.5 Create model and show summary\r\n",
        "#     While outputs are two, model is one\r\n",
        "main_model = Model(inputs = [inputsA,inputsB], outputs = [out_x, out_y])\r\n",
        "main_model.summary()\r\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "in_b (InputLayer)               [(None, 7)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_43 (Dense)                (None, 100)          800         in_b[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_44 (Dense)                (None, 100)          10100       dense_43[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "in_a (InputLayer)               [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 104)          0           dense_44[0][0]                   \n",
            "                                                                 in_a[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "out_a (Dense)                   (None, 1)            105         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "out_b (Dense)                   (None, 1)            101         dense_44[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 11,106\n",
            "Trainable params: 11,106\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "BOmnKemAlzJ6",
        "outputId": "83722899-46ea-4611-fade-f5a3c73ef060"
      },
      "source": [
        "# 5.6 Plot the model now\r\n",
        "plot_model(main_model, show_shapes = True)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvp16EsGHulp",
        "outputId": "c32df922-e7a4-44e7-ebe7-37f436bba0ea"
      },
      "source": [
        "help(Model)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Model in module tensorflow.python.keras.engine.training:\n",
            "\n",
            "class Model(tensorflow.python.keras.engine.base_layer.Layer, tensorflow.python.keras.utils.version_utils.ModelVersionSelector)\n",
            " |  `Model` groups layers into an object with training and inference features.\n",
            " |  \n",
            " |  Arguments:\n",
            " |      inputs: The input(s) of the model: a `keras.Input` object or list of\n",
            " |          `keras.Input` objects.\n",
            " |      outputs: The output(s) of the model. See Functional API example below.\n",
            " |      name: String, the name of the model.\n",
            " |  \n",
            " |  There are two ways to instantiate a `Model`:\n",
            " |  \n",
            " |  1 - With the \"Functional API\", where you start from `Input`,\n",
            " |  you chain layer calls to specify the model's forward pass,\n",
            " |  and finally you create your model from inputs and outputs:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  inputs = tf.keras.Input(shape=(3,))\n",
            " |  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n",
            " |  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
            " |  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
            " |  ```\n",
            " |  \n",
            " |  2 - By subclassing the `Model` class: in that case, you should define your\n",
            " |  layers in `__init__` and you should implement the model's forward pass\n",
            " |  in `call`.\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super(MyModel, self).__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |  \n",
            " |    def call(self, inputs):\n",
            " |      x = self.dense1(inputs)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  If you subclass `Model`, you can optionally have\n",
            " |  a `training` argument (boolean) in `call`, which you can use to specify\n",
            " |  a different behavior in training and inference:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super(MyModel, self).__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |      self.dropout = tf.keras.layers.Dropout(0.5)\n",
            " |  \n",
            " |    def call(self, inputs, training=False):\n",
            " |      x = self.dense1(inputs)\n",
            " |      if training:\n",
            " |        x = self.dropout(x, training=training)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  Once the model is created, you can config the model with losses and metrics\n",
            " |  with `model.compile()`, train the model with `model.fit()`, or use the model\n",
            " |  to do prediction with `model.predict()`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Model\n",
            " |      tensorflow.python.keras.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
            " |      tensorflow.python.keras.utils.version_utils.ModelVersionSelector\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *args, **kwargs)\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Builds the model based on input shapes received.\n",
            " |      \n",
            " |      This is to be used for subclassed models, which do not know at instantiation\n",
            " |      time what their inputs look like.\n",
            " |      \n",
            " |      This method only exists for users who want to call `model.build()` in a\n",
            " |      standalone way (as a substitute for calling the model on real data to\n",
            " |      build it). It will never be called by the framework (and thus it will\n",
            " |      never throw unexpected errors in an unrelated workflow).\n",
            " |      \n",
            " |      Args:\n",
            " |       input_shape: Single tuple, TensorShape, or list/dict of shapes, where\n",
            " |           shapes are tuples, integers, or TensorShapes.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError:\n",
            " |          1. In case of invalid user-provided data (not of type tuple,\n",
            " |             list, TensorShape, or dict).\n",
            " |          2. If the model requires call arguments that are agnostic\n",
            " |             to the input shapes (positional or kwarg in call signature).\n",
            " |          3. If not all layers were properly built.\n",
            " |          4. If float type inputs are not supported within the layers.\n",
            " |      \n",
            " |        In each of these cases, the user should build their model by calling it\n",
            " |        on real tensor data.\n",
            " |  \n",
            " |  call(self, inputs, training=None, mask=None)\n",
            " |      Calls the model on new inputs.\n",
            " |      \n",
            " |      In this case `call` just reapplies\n",
            " |      all ops in the graph to the new inputs\n",
            " |      (e.g. build a new computational graph from the provided inputs).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: A tensor or list of tensors.\n",
            " |          training: Boolean or boolean scalar tensor, indicating whether to run\n",
            " |            the `Network` in training mode or inference mode.\n",
            " |          mask: A mask or list of masks. A mask can be\n",
            " |              either a tensor or None (no mask).\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor if there is a single output, or\n",
            " |          a list of tensors if there are more than one outputs.\n",
            " |  \n",
            " |  compile(self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, **kwargs)\n",
            " |      Configures the model for training.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          optimizer: String (name of optimizer) or optimizer instance. See\n",
            " |            `tf.keras.optimizers`.\n",
            " |          loss: String (name of objective function), objective function or\n",
            " |            `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n",
            " |            function is any callable with the signature `loss = fn(y_true,\n",
            " |            y_pred)`, where y_true = ground truth values with shape =\n",
            " |            `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse\n",
            " |            categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.\n",
            " |            y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It\n",
            " |            returns a weighted loss float tensor. If a custom `Loss` instance is\n",
            " |            used and reduction is set to NONE, return value has the shape\n",
            " |            [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values;\n",
            " |            otherwise, it is a scalar. If the model has multiple outputs, you can\n",
            " |            use a different loss on each output by passing a dictionary or a list\n",
            " |            of losses. The loss value that will be minimized by the model will\n",
            " |            then be the sum of all individual losses.\n",
            " |          metrics: List of metrics to be evaluated by the model during training\n",
            " |            and testing. Each of this can be a string (name of a built-in\n",
            " |            function), function or a `tf.keras.metrics.Metric` instance. See\n",
            " |            `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n",
            " |            function is any callable with the signature `result = fn(y_true,\n",
            " |            y_pred)`. To specify different metrics for different outputs of a\n",
            " |            multi-output model, you could also pass a dictionary, such as\n",
            " |              `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n",
            " |                You can also pass a list (len = len(outputs)) of lists of metrics\n",
            " |                such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n",
            " |                `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
            " |                strings 'accuracy' or 'acc', we convert this to one of\n",
            " |                `tf.keras.metrics.BinaryAccuracy`,\n",
            " |                `tf.keras.metrics.CategoricalAccuracy`,\n",
            " |                `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n",
            " |                function used and the model output shape. We do a similar\n",
            " |                conversion for the strings 'crossentropy' and 'ce' as well.\n",
            " |          loss_weights: Optional list or dictionary specifying scalar coefficients\n",
            " |            (Python floats) to weight the loss contributions of different model\n",
            " |            outputs. The loss value that will be minimized by the model will then\n",
            " |            be the *weighted sum* of all individual losses, weighted by the\n",
            " |            `loss_weights` coefficients.\n",
            " |              If a list, it is expected to have a 1:1 mapping to the model's\n",
            " |                outputs. If a dict, it is expected to map output names (strings)\n",
            " |                to scalar coefficients.\n",
            " |          weighted_metrics: List of metrics to be evaluated and weighted by\n",
            " |            sample_weight or class_weight during training and testing.\n",
            " |          run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
            " |            logic will not be wrapped in a `tf.function`. Recommended to leave\n",
            " |            this as `None` unless your `Model` cannot be run inside a\n",
            " |            `tf.function`.\n",
            " |          steps_per_execution: Int. Defaults to 1. The number of batches to\n",
            " |            run during each `tf.function` call. Running multiple batches\n",
            " |            inside a single `tf.function` call can greatly improve performance\n",
            " |            on TPUs or small models with a large Python overhead.\n",
            " |            At most, one full epoch will be run each\n",
            " |            execution. If a number larger than the size of the epoch is passed,\n",
            " |            the execution will be truncated to the size of the epoch.\n",
            " |            Note that if `steps_per_execution` is set to `N`,\n",
            " |            `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n",
            " |            will only be called every `N` batches\n",
            " |            (i.e. before/after each `tf.function` execution).\n",
            " |          **kwargs: Arguments supported for backwards compatibility only.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: In case of invalid arguments for\n",
            " |              `optimizer`, `loss` or `metrics`.\n",
            " |  \n",
            " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False)\n",
            " |      Returns the loss value & metrics values for the model in test mode.\n",
            " |      \n",
            " |      Computation is done in batches (see the `batch_size` arg.)\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely). If\n",
            " |            `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\n",
            " |            should not be specified (since targets will be obtained from the\n",
            " |            iterator/dataset).\n",
            " |          batch_size: Integer or `None`. Number of samples per batch of\n",
            " |            computation. If unspecified, `batch_size` will default to 32. Do not\n",
            " |            specify the `batch_size` if your data is in the form of a dataset,\n",
            " |            generators, or `keras.utils.Sequence` instances (since they generate\n",
            " |            batches).\n",
            " |          verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n",
            " |          sample_weight: Optional Numpy array of weights for the test samples,\n",
            " |            used for weighting the loss function. You can either pass a flat (1D)\n",
            " |            Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples), or in the case of\n",
            " |                temporal data, you can pass a 2D array with shape `(samples,\n",
            " |                sequence_length)`, to apply a different weight to every timestep\n",
            " |                of every sample. This argument is not supported when `x` is a\n",
            " |                dataset, instead pass sample weights as the third element of `x`.\n",
            " |          steps: Integer or `None`. Total number of steps (batches of samples)\n",
            " |            before declaring the evaluation round finished. Ignored with the\n",
            " |            default value of `None`. If x is a `tf.data` dataset and `steps` is\n",
            " |            None, 'evaluate' will run until the dataset is exhausted. This\n",
            " |            argument is not supported with array inputs.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances. List of\n",
            " |            callbacks to apply during evaluation. See\n",
            " |            [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |            input only. Maximum size for the generator queue. If unspecified,\n",
            " |            `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |            only. Maximum number of processes to spin up when using process-based\n",
            " |            threading. If unspecified, `workers` will default to 1. If 0, will\n",
            " |            execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |            threading. If unspecified, `use_multiprocessing` will default to\n",
            " |            `False`. Note that because this implementation relies on\n",
            " |            multiprocessing, you should not pass non-picklable arguments to the\n",
            " |            generator as they can't be passed easily to children processes.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.evaluate` is wrapped in `tf.function`.\n",
            " |          ValueError: in case of invalid arguments.\n",
            " |  \n",
            " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Evaluates the model on a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.evaluate` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given below.\n",
            " |          y: Target data. Like the input data `x`,\n",
            " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
            " |            tensor targets, or inversely). If `x` is a dataset, generator,\n",
            " |            or `keras.utils.Sequence` instance, `y` should\n",
            " |            not be specified (since targets will be obtained from `x`).\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per gradient update.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          epochs: Integer. Number of epochs to train the model.\n",
            " |              An epoch is an iteration over the entire `x` and `y`\n",
            " |              data provided.\n",
            " |              Note that in conjunction with `initial_epoch`,\n",
            " |              `epochs` is to be understood as \"final epoch\".\n",
            " |              The model is not trained for a number of iterations\n",
            " |              given by `epochs`, but merely until the epoch\n",
            " |              of index `epochs` is reached.\n",
            " |          verbose: 0, 1, or 2. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            " |              Note that the progress bar is not particularly useful when\n",
            " |              logged to a file, so verbose=2 is recommended when not running\n",
            " |              interactively (eg, in a production environment).\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training.\n",
            " |              See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
            " |              and `tf.keras.callbacks.History` callbacks are created automatically\n",
            " |              and need not be passed into `model.fit`.\n",
            " |              `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
            " |              `verbose` argument to `model.fit`.\n",
            " |          validation_split: Float between 0 and 1.\n",
            " |              Fraction of the training data to be used as validation data.\n",
            " |              The model will set apart this fraction of the training data,\n",
            " |              will not train on it, and will evaluate\n",
            " |              the loss and any model metrics\n",
            " |              on this data at the end of each epoch.\n",
            " |              The validation data is selected from the last samples\n",
            " |              in the `x` and `y` data provided, before shuffling. This argument is\n",
            " |              not supported when `x` is a dataset, generator or\n",
            " |             `keras.utils.Sequence` instance.\n",
            " |          validation_data: Data on which to evaluate\n",
            " |              the loss and any model metrics at the end of each epoch.\n",
            " |              The model will not be trained on this data. Thus, note the fact\n",
            " |              that the validation loss of data provided using `validation_split`\n",
            " |              or `validation_data` is not affected by regularization layers like\n",
            " |              noise and dropout.\n",
            " |              `validation_data` will override `validation_split`.\n",
            " |              `validation_data` could be:\n",
            " |                - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
            " |                - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
            " |                - dataset\n",
            " |              For the first two cases, `batch_size` must be provided.\n",
            " |              For the last case, `validation_steps` could be provided.\n",
            " |              Note that `validation_data` does not support all the data types that\n",
            " |              are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
            " |          shuffle: Boolean (whether to shuffle the training data\n",
            " |              before each epoch) or str (for 'batch'). This argument is ignored\n",
            " |              when `x` is a generator. 'batch' is a special option for dealing\n",
            " |              with the limitations of HDF5 data; it shuffles in batch-sized\n",
            " |              chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers)\n",
            " |              to a weight (float) value, used for weighting the loss function\n",
            " |              (during training only).\n",
            " |              This can be useful to tell the model to\n",
            " |              \"pay more attention\" to samples from\n",
            " |              an under-represented class.\n",
            " |          sample_weight: Optional Numpy array of weights for\n",
            " |              the training samples, used for weighting the loss function\n",
            " |              (during training only). You can either pass a flat (1D)\n",
            " |              Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples),\n",
            " |              or in the case of temporal data,\n",
            " |              you can pass a 2D array with shape\n",
            " |              `(samples, sequence_length)`,\n",
            " |              to apply a different weight to every timestep of every sample. This\n",
            " |              argument is not supported when `x` is a dataset, generator, or\n",
            " |             `keras.utils.Sequence` instance, instead provide the sample_weights\n",
            " |              as the third element of `x`.\n",
            " |          initial_epoch: Integer.\n",
            " |              Epoch at which to start training\n",
            " |              (useful for resuming a previous training run).\n",
            " |          steps_per_epoch: Integer or `None`.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              before declaring one epoch finished and starting the\n",
            " |              next epoch. When training with input tensors such as\n",
            " |              TensorFlow data tensors, the default `None` is equal to\n",
            " |              the number of samples in your dataset divided by\n",
            " |              the batch size, or 1 if that cannot be determined. If x is a\n",
            " |              `tf.data` dataset, and 'steps_per_epoch'\n",
            " |              is None, the epoch will run until the input dataset is exhausted.\n",
            " |              When passing an infinitely repeating dataset, you must specify the\n",
            " |              `steps_per_epoch` argument. This argument is not supported with\n",
            " |              array inputs.\n",
            " |          validation_steps: Only relevant if `validation_data` is provided and\n",
            " |              is a `tf.data` dataset. Total number of steps (batches of\n",
            " |              samples) to draw before stopping when performing validation\n",
            " |              at the end of every epoch. If 'validation_steps' is None, validation\n",
            " |              will run until the `validation_data` dataset is exhausted. In the\n",
            " |              case of an infinitely repeated dataset, it will run into an\n",
            " |              infinite loop. If 'validation_steps' is specified and only part of\n",
            " |              the dataset will be consumed, the evaluation will start from the\n",
            " |              beginning of the dataset at each epoch. This ensures that the same\n",
            " |              validation samples are used every time.\n",
            " |          validation_batch_size: Integer or `None`.\n",
            " |              Number of samples per validation batch.\n",
            " |              If unspecified, will default to `batch_size`.\n",
            " |              Do not specify the `validation_batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          validation_freq: Only relevant if validation data is provided. Integer\n",
            " |              or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
            " |              If an integer, specifies how many training epochs to run before a\n",
            " |              new validation run is performed, e.g. `validation_freq=2` runs\n",
            " |              validation every 2 epochs. If a Container, specifies the epochs on\n",
            " |              which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            " |              validation at the end of the 1st, 2nd, and 10th epochs.\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up\n",
            " |              when using process-based threading. If unspecified, `workers`\n",
            " |              will default to 1. If 0, will execute the generator on the main\n",
            " |              thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      Unpacking behavior for iterator-like inputs:\n",
            " |          A common pattern is to pass a tf.data.Dataset, generator, or\n",
            " |        tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            " |        yield not only features (x) but optionally targets (y) and sample weights.\n",
            " |        Keras requires that the output of such iterator-likes be unambiguous. The\n",
            " |        iterator should return a tuple of length 1, 2, or 3, where the optional\n",
            " |        second and third elements will be used for y and sample_weight\n",
            " |        respectively. Any other type provided will be wrapped in a length one\n",
            " |        tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
            " |        should still adhere to the top-level tuple structure.\n",
            " |        e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            " |        features, targets, and weights from the keys of a single dict.\n",
            " |          A notable unsupported data type is the namedtuple. The reason is that\n",
            " |        it behaves like both an ordered datatype (tuple) and a mapping\n",
            " |        datatype (dict). So given a namedtuple of the form:\n",
            " |            `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            " |        it is ambiguous whether to reverse the order of the elements when\n",
            " |        interpreting the value. Even worse is a tuple of the form:\n",
            " |            `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            " |        where it is unclear if the tuple was intended to be unpacked into x, y,\n",
            " |        and sample_weight or passed through as a single element to `x`. As a\n",
            " |        result the data processing code will simply raise a ValueError if it\n",
            " |        encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          A `History` object. Its `History.history` attribute is\n",
            " |          a record of training loss values and metrics values\n",
            " |          at successive epochs, as well as validation loss values\n",
            " |          and validation metrics values (if applicable).\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: 1. If the model was never compiled or,\n",
            " |          2. If `model.fit` is  wrapped in `tf.function`.\n",
            " |      \n",
            " |          ValueError: In case of mismatch between the provided input data\n",
            " |              and what the model expects or when the input data is empty.\n",
            " |  \n",
            " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
            " |      Fits the model on data yielded batch-by-batch by a Python generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.fit` now supports generators, so there is no longer any need to use\n",
            " |        this endpoint.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  get_layer(self, name=None, index=None)\n",
            " |      Retrieves a layer based on either its name (unique) or index.\n",
            " |      \n",
            " |      If `name` and `index` are both provided, `index` will take precedence.\n",
            " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          name: String, name of layer.\n",
            " |          index: Integer, index of layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: In case of invalid layer name or index.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Retrieves the weights of the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A flat list of Numpy arrays.\n",
            " |  \n",
            " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None)\n",
            " |      Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n",
            " |      \n",
            " |      If `by_name` is False weights are loaded based on the network's\n",
            " |      topology. This means the architecture should be the same as when the weights\n",
            " |      were saved.  Note that layers that don't have weights are not taken into\n",
            " |      account in the topological ordering, so adding or removing layers is fine as\n",
            " |      long as they don't have weights.\n",
            " |      \n",
            " |      If `by_name` is True, weights are loaded into layers only if they share the\n",
            " |      same name. This is useful for fine-tuning or transfer-learning models where\n",
            " |      some of the layers have changed.\n",
            " |      \n",
            " |      Only topological loading (`by_name=False`) is supported when loading weights\n",
            " |      from the TensorFlow format. Note that topological loading differs slightly\n",
            " |      between TensorFlow and HDF5 formats for user-defined classes inheriting from\n",
            " |      `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n",
            " |      TensorFlow format loads based on the object-local names of attributes to\n",
            " |      which layers are assigned in the `Model`'s constructor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String, path to the weights file to load. For weight files in\n",
            " |              TensorFlow format, this is the file prefix (the same as was passed\n",
            " |              to `save_weights`).\n",
            " |          by_name: Boolean, whether to load weights by name or by topological\n",
            " |              order. Only topological loading is supported for weight files in\n",
            " |              TensorFlow format.\n",
            " |          skip_mismatch: Boolean, whether to skip loading of layers where there is\n",
            " |              a mismatch in the number of weights, or a mismatch in the shape of\n",
            " |              the weight (only valid when `by_name=True`).\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for loading weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          When loading a weight file in TensorFlow format, returns the same status\n",
            " |          object as `tf.train.Checkpoint.restore`. When graph building, restore\n",
            " |          ops are run automatically as soon as the network is built (on first call\n",
            " |          for user-defined classes inheriting from `Model`, immediately if it is\n",
            " |          already built).\n",
            " |      \n",
            " |          When loading weights in HDF5 format, returns `None`.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If h5py is not available and the weight file is in HDF5\n",
            " |              format.\n",
            " |          ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n",
            " |            `False`.\n",
            " |  \n",
            " |  make_predict_function(self)\n",
            " |      Creates a function that executes one step of inference.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.predict` and `Model.predict_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.predict_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.predict` or\n",
            " |      `Model.predict_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return the outputs of the `Model`.\n",
            " |  \n",
            " |  make_test_function(self)\n",
            " |      Creates a function that executes one step of evaluation.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.evaluate` and `Model.test_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.test_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.evaluate` or\n",
            " |      `Model.test_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_test_batch_end`.\n",
            " |  \n",
            " |  make_train_function(self)\n",
            " |      Creates a function that executes one step of training.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.fit` and `Model.train_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual training\n",
            " |      logic to `Model.train_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.fit` or\n",
            " |      `Model.train_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Generates output predictions for the input samples.\n",
            " |      \n",
            " |      Computation is done in batches. This method is designed for performance in\n",
            " |      large scale inputs. For small amount of inputs that fit in one batch,\n",
            " |      directly using `__call__` is recommended for faster execution, e.g.,\n",
            " |      `model(x)`, or `model(x, training=False)` if you have layers such as\n",
            " |      `tf.keras.layers.BatchNormalization` that behaves differently during\n",
            " |      inference. Also, note the fact that test loss is not affected by\n",
            " |      regularization layers like noise and dropout.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input samples. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A `tf.data` dataset.\n",
            " |            - A generator or `keras.utils.Sequence` instance.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per batch.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of dataset, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          verbose: Verbosity mode, 0 or 1.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              before declaring the prediction round finished.\n",
            " |              Ignored with the default value of `None`. If x is a `tf.data`\n",
            " |              dataset and `steps` is None, `predict` will\n",
            " |              run until the input dataset is exhausted.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during prediction.\n",
            " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up when using\n",
            " |              process-based threading. If unspecified, `workers` will default\n",
            " |              to 1. If 0, will execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`. Note that Model.predict uses the same interpretation rules as\n",
            " |      `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n",
            " |      three methods.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of mismatch between the provided\n",
            " |              input data and the model's expectations,\n",
            " |              or in case a stateful model receives a number of samples\n",
            " |              that is not a multiple of the batch size.\n",
            " |  \n",
            " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Generates predictions for the input samples from a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.predict` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  predict_on_batch(self, x)\n",
            " |      Returns predictions for a single batch of samples.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
            " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
            " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict_on_batch` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of mismatch between given number of inputs and\n",
            " |            expectations of the model.\n",
            " |  \n",
            " |  predict_step(self, data)\n",
            " |      The logic for one inference step.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.make_predict_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of inference.\n",
            " |      This typically includes the forward pass.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_predict_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The result of one inference step, typically the output of calling the\n",
            " |        `Model` on data.\n",
            " |  \n",
            " |  reset_metrics(self)\n",
            " |      Resets the state of all the metrics in the model.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> _ = model.fit(x, y, verbose=0)\n",
            " |      >>> assert all(float(m.result()) for m in model.metrics)\n",
            " |      \n",
            " |      >>> model.reset_metrics()\n",
            " |      >>> assert all(float(m.result()) == 0 for m in model.metrics)\n",
            " |  \n",
            " |  reset_states(self)\n",
            " |  \n",
            " |  save(self, filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None, save_traces=True)\n",
            " |      Saves the model to Tensorflow SavedModel or a single HDF5 file.\n",
            " |      \n",
            " |      Please see `tf.keras.models.save_model` or the\n",
            " |      [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)\n",
            " |      for details.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String, PathLike, path to SavedModel or H5 file to save the\n",
            " |              model.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          include_optimizer: If True, save optimizer's state together.\n",
            " |          save_format: Either `'tf'` or `'h5'`, indicating whether to save the\n",
            " |              model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,\n",
            " |              and 'h5' in TF 1.X.\n",
            " |          signatures: Signatures to save with the SavedModel. Applicable to the\n",
            " |              'tf' format only. Please see the `signatures` argument in\n",
            " |              `tf.saved_model.save` for details.\n",
            " |          options: (only applies to SavedModel format)\n",
            " |              `tf.saved_model.SaveOptions` object that specifies options for\n",
            " |              saving to SavedModel.\n",
            " |          save_traces: (only applies to SavedModel format) When enabled, the\n",
            " |              SavedModel will store the function traces for each layer. This\n",
            " |              can be disabled, so that only the configs of each layer are stored.\n",
            " |              Defaults to `True`. Disabling this will decrease serialization time\n",
            " |              and reduce file size, but it requires that all custom layers/models\n",
            " |              implement a `get_config()` method.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      from keras.models import load_model\n",
            " |      \n",
            " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
            " |      del model  # deletes the existing model\n",
            " |      \n",
            " |      # returns a compiled model\n",
            " |      # identical to the previous one\n",
            " |      model = load_model('my_model.h5')\n",
            " |      ```\n",
            " |  \n",
            " |  save_weights(self, filepath, overwrite=True, save_format=None, options=None)\n",
            " |      Saves all layer weights.\n",
            " |      \n",
            " |      Either saves in HDF5 or in TensorFlow format based on the `save_format`\n",
            " |      argument.\n",
            " |      \n",
            " |      When saving in HDF5 format, the weight file has:\n",
            " |        - `layer_names` (attribute), a list of strings\n",
            " |            (ordered names of model layers).\n",
            " |        - For every layer, a `group` named `layer.name`\n",
            " |            - For every such layer group, a group attribute `weight_names`,\n",
            " |                a list of strings\n",
            " |                (ordered names of weights tensor of the layer).\n",
            " |            - For every weight in the layer, a dataset\n",
            " |                storing the weight value, named after the weight tensor.\n",
            " |      \n",
            " |      When saving in TensorFlow format, all objects referenced by the network are\n",
            " |      saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n",
            " |      instances or `Optimizer` instances assigned to object attributes. For\n",
            " |      networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n",
            " |      outputs)`, `Layer` instances used by the network are tracked/saved\n",
            " |      automatically. For user-defined classes which inherit from `tf.keras.Model`,\n",
            " |      `Layer` instances must be assigned to object attributes, typically in the\n",
            " |      constructor. See the documentation of `tf.train.Checkpoint` and\n",
            " |      `tf.keras.Model` for details.\n",
            " |      \n",
            " |      While the formats are the same, do not mix `save_weights` and\n",
            " |      `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n",
            " |      loaded using `Model.load_weights`. Checkpoints saved using\n",
            " |      `tf.train.Checkpoint.save` should be restored using the corresponding\n",
            " |      `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n",
            " |      `save_weights` for training checkpoints.\n",
            " |      \n",
            " |      The TensorFlow format matches objects and variables by starting at a root\n",
            " |      object, `self` for `save_weights`, and greedily matching attribute\n",
            " |      names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n",
            " |      is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n",
            " |      means saving a `tf.keras.Model` using `save_weights` and loading into a\n",
            " |      `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n",
            " |      the `Model`'s variables. See the [guide to training\n",
            " |      checkpoints](https://www.tensorflow.org/guide/checkpoint) for details\n",
            " |      on the TensorFlow format.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String or PathLike, path to the file to save the weights to.\n",
            " |              When saving in TensorFlow format, this is the prefix used for\n",
            " |              checkpoint files (multiple files are generated). Note that the '.h5'\n",
            " |              suffix causes weights to be saved in HDF5 format.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n",
            " |              '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n",
            " |              `None` defaults to 'tf'.\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for saving weights.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If h5py is not available when attempting to save in HDF5\n",
            " |              format.\n",
            " |          ValueError: For invalid/unknown format arguments.\n",
            " |  \n",
            " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
            " |      Prints a string summary of the network.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          line_length: Total length of printed lines\n",
            " |              (e.g. set this to adapt the display to different\n",
            " |              terminal window sizes).\n",
            " |          positions: Relative or absolute positions of log elements\n",
            " |              in each line. If not provided,\n",
            " |              defaults to `[.33, .55, .67, 1.]`.\n",
            " |          print_fn: Print function to use. Defaults to `print`.\n",
            " |              It will be called on each line of the summary.\n",
            " |              You can set it to a custom function\n",
            " |              in order to capture the string summary.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if `summary()` is called before the model is built.\n",
            " |  \n",
            " |  test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Test the model on a single batch of samples.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
            " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
            " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors, if\n",
            " |            the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of invalid user-provided arguments.\n",
            " |  \n",
            " |  test_step(self, data)\n",
            " |      The logic for one evaluation step.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.make_test_function`.\n",
            " |      \n",
            " |      This function should contain the mathematical logic for one step of\n",
            " |      evaluation.\n",
            " |      This typically includes the forward pass, loss calculation, and metrics\n",
            " |      updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_test_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned.\n",
            " |  \n",
            " |  to_json(self, **kwargs)\n",
            " |      Returns a JSON string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a JSON save file, use\n",
            " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `json.dumps()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A JSON string.\n",
            " |  \n",
            " |  to_yaml(self, **kwargs)\n",
            " |      Returns a yaml string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a yaml save file, use\n",
            " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
            " |      \n",
            " |      `custom_objects` should be a dictionary mapping\n",
            " |      the names of custom losses / layers / etc to the corresponding\n",
            " |      functions / classes.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `yaml.dump()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A YAML string.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: if yaml module is not found.\n",
            " |  \n",
            " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Runs a single gradient update on a single batch of data.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |                if the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers) to a\n",
            " |            weight (float) to apply to the model's loss for the samples from this\n",
            " |            class during training. This can be useful to tell the model to \"pay\n",
            " |            more attention\" to samples from an under-represented class.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar training loss\n",
            " |          (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If `model.train_on_batch` is wrapped in `tf.function`.\n",
            " |        ValueError: In case of invalid user-provided arguments.\n",
            " |  \n",
            " |  train_step(self, data)\n",
            " |      The logic for one training step.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.make_train_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of training.\n",
            " |      This typically includes the forward pass, loss calculation, backpropagation,\n",
            " |      and metric updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_train_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned. Example:\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_config(config, custom_objects=None) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  distribute_strategy\n",
            " |      The `tf.distribute.Strategy` this model was created under.\n",
            " |  \n",
            " |  layers\n",
            " |  \n",
            " |  metrics\n",
            " |      Returns the model's metrics added using `compile`, `add_metric` APIs.\n",
            " |      \n",
            " |      Note: Metrics passed to `compile()` are available only after a `keras.Model`\n",
            " |      has been trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.add_metric(\n",
            " |      ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc', 'mean']\n",
            " |  \n",
            " |  metrics_names\n",
            " |      Returns the model's display labels for all outputs.\n",
            " |      \n",
            " |      Note: `metrics_names` are available only after a `keras.Model` has been\n",
            " |      trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> model.metrics_names\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc']\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Non-trainable weights are *not* updated during training. They are expected\n",
            " |      to be updated manually in `call()`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of non-trainable variables.\n",
            " |  \n",
            " |  run_eagerly\n",
            " |      Settable attribute indicating whether the model should run eagerly.\n",
            " |      \n",
            " |      Running eagerly means that your model will be run step by step,\n",
            " |      like Python code. Your model might run slower, but it should become easier\n",
            " |      for you to debug it by stepping into individual layer calls.\n",
            " |      \n",
            " |      By default, we will attempt to compile your model to a static graph to\n",
            " |      deliver the best execution performance.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Boolean, whether the model should run eagerly.\n",
            " |  \n",
            " |  state_updates\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Returns the `updates` from all layers that are stateful.\n",
            " |      \n",
            " |      This is useful for separating training updates and\n",
            " |      state updates, e.g. when we need to update a layer's internal state\n",
            " |      during prediction.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of update ops.\n",
            " |  \n",
            " |  trainable_weights\n",
            " |      List of all trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Trainable weights are updated via gradient descent during training.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of trainable variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        *args: Positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
            " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_loss(self, losses, **kwargs)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be dependent\n",
            " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
            " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
            " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(self, inputs):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any loss Tensors passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      losses become part of the model's topology and are tracked in `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Activity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss references\n",
            " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
            " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
            " |      topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      d = tf.keras.layers.Dense(10)\n",
            " |      x = d(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      Arguments:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
            " |          may also be zero-argument callables which create a loss tensor.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |            inputs - Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_metric(self, value, name=None, **kwargs)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      This method can be used inside the `call()` method of a subclassed layer\n",
            " |      or model.\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
            " |        def __init__(self):\n",
            " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
            " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
            " |      \n",
            " |        def call(self, inputs):\n",
            " |          self.add_metric(self.mean(x))\n",
            " |          self.add_metric(tf.reduce_sum(x), name='metric_2')\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any tensor passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      metrics become part of the model's topology and are tracked when you\n",
            " |      save the model via `save()`.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
            " |      Functional Model, as shown in the example below, is not supported. This is\n",
            " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        name: String metric name.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |          `aggregation` - When the `value` tensor provided is not the result of\n",
            " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
            " |          using a `keras.Metric.Mean`.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Add update op(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and variance\n",
            " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
            " |      when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case, variable\n",
            " |      updates are run on the fly and thus do not need to be tracked for later\n",
            " |      execution).\n",
            " |      \n",
            " |      Arguments:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |        inputs: Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        use_resource: Whether to use `ResourceVariable`.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            " |          `AUTO` and the current `DistributionStrategy` chooses\n",
            " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
            " |          `trainable` must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The variable created.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
            " |  \n",
            " |  apply(self, inputs, *args, **kwargs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      This is an alias of `self.__call__`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      If the layer has not been built, this method will call `build` on the\n",
            " |      layer. This assumes that the layer will later be used with inputs that\n",
            " |      match the input shape provided here.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
            " |          how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves losses relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of loss tensors of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves updates relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of update ops of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from Numpy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      sets the weight values from numpy arrays. The weight values should be\n",
            " |      passed in the order they are created by the layer. Note that the layer's\n",
            " |      weights must be instantiated before calling this function by calling\n",
            " |      the layer.\n",
            " |      \n",
            " |      For example, a Dense layer returns a list of two values-- per-output\n",
            " |      weights and the bias value. These can be used to set the weights of another\n",
            " |      Dense layer:\n",
            " |      \n",
            " |      >>> a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b.set_weights(a.get_weights())\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Arguments:\n",
            " |          weights: a list of Numpy arrays. The number\n",
            " |              of arrays and their shape must match\n",
            " |              number of the dimensions of the weights\n",
            " |              of the layer (i.e. it should match the\n",
            " |              output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the provided weights list does not match the\n",
            " |              layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  compute_dtype\n",
            " |      The dtype of the layer's computations.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
            " |      the weights.\n",
            " |      \n",
            " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
            " |      computations and the output to be in the compute dtype as well. This is done\n",
            " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
            " |      these casts if implementing your own layer.\n",
            " |      \n",
            " |      Layers often perform certain internal computations in higher precision when\n",
            " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
            " |      will still typically be float16 or bfloat16 in such cases.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The layer's compute dtype.\n",
            " |  \n",
            " |  dtype\n",
            " |      The dtype of the layer weights.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
            " |      dtype of the layer's computations.\n",
            " |  \n",
            " |  dtype_policy\n",
            " |      The dtype policy associated with this layer.\n",
            " |      \n",
            " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
            " |  \n",
            " |  dynamic\n",
            " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  input_spec\n",
            " |      `InputSpec` instance(s) describing the input format for this layer.\n",
            " |      \n",
            " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
            " |      the layer to run input compatibility checks when it is called.\n",
            " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
            " |      of rank 4. As such, you can set, in `__init__()`:\n",
            " |      \n",
            " |      ```python\n",
            " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
            " |      ```\n",
            " |      \n",
            " |      Now, if you try to call the layer on an input that isn't rank 4\n",
            " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
            " |      error:\n",
            " |      \n",
            " |      ```\n",
            " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
            " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
            " |      ```\n",
            " |      \n",
            " |      Input checks that can be specified via `input_spec` include:\n",
            " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
            " |      - Shape\n",
            " |      - Rank (ndim)\n",
            " |      - Dtype\n",
            " |      \n",
            " |      For more information, see `tf.keras.layers.InputSpec`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
            " |  \n",
            " |  losses\n",
            " |      List of losses added using the `add_loss()` API.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is accessed,\n",
            " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
            " |      propagate gradients back to the corresponding variables.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
            " |      ...   def call(self, inputs):\n",
            " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |      ...     return inputs\n",
            " |      >>> l = MyLayer()\n",
            " |      >>> l(np.ones((10, 1)))\n",
            " |      >>> l.losses\n",
            " |      [1.0]\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Activity regularization.\n",
            " |      >>> len(model.losses)\n",
            " |      0\n",
            " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      >>> len(model.losses)\n",
            " |      1\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
            " |      >>> x = d(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Weight regularization.\n",
            " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      >>> model.losses\n",
            " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  name\n",
            " |      Name of the layer (string), set in the constructor.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  supports_masking\n",
            " |      Whether this layer supports computing a mask using `compute_mask`.\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variable_dtype\n",
            " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      >>> class MyModule(tf.Module):\n",
            " |      ...   @tf.Module.with_name_scope\n",
            " |      ...   def __call__(self, x):\n",
            " |      ...     if not hasattr(self, 'w'):\n",
            " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
            " |      ...     return tf.matmul(x, self.w)\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      >>> mod = MyModule()\n",
            " |      >>> mod(tf.ones([1, 2]))\n",
            " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
            " |      >>> mod.w\n",
            " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
            " |      numpy=..., dtype=float32)>\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      >>> a = tf.Module()\n",
            " |      >>> b = tf.Module()\n",
            " |      >>> c = tf.Module()\n",
            " |      >>> a.b = b\n",
            " |      >>> b.c = c\n",
            " |      >>> list(a.submodules) == [b, c]\n",
            " |      True\n",
            " |      >>> list(b.submodules) == [c]\n",
            " |      True\n",
            " |      >>> list(c.submodules) == []\n",
            " |      True\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVEiWb3bHy_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc78494-ebff-4395-e2e1-5171d37872e2"
      },
      "source": [
        "help(main_model.fit)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method fit in module tensorflow.python.keras.engine.training:\n",
            "\n",
            "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) method of tensorflow.python.keras.engine.functional.Functional instance\n",
            "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            "    \n",
            "    Arguments:\n",
            "        x: Input data. It could be:\n",
            "          - A Numpy array (or array-like), or a list of arrays\n",
            "            (in case the model has multiple inputs).\n",
            "          - A TensorFlow tensor, or a list of tensors\n",
            "            (in case the model has multiple inputs).\n",
            "          - A dict mapping input names to the corresponding array/tensors,\n",
            "            if the model has named inputs.\n",
            "          - A `tf.data` dataset. Should return a tuple\n",
            "            of either `(inputs, targets)` or\n",
            "            `(inputs, targets, sample_weights)`.\n",
            "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            "            or `(inputs, targets, sample_weights)`.\n",
            "          A more detailed description of unpacking behavior for iterator types\n",
            "          (Dataset, generator, Sequence) is given below.\n",
            "        y: Target data. Like the input data `x`,\n",
            "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
            "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
            "          or `keras.utils.Sequence` instance, `y` should\n",
            "          not be specified (since targets will be obtained from `x`).\n",
            "        batch_size: Integer or `None`.\n",
            "            Number of samples per gradient update.\n",
            "            If unspecified, `batch_size` will default to 32.\n",
            "            Do not specify the `batch_size` if your data is in the\n",
            "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
            "            (since they generate batches).\n",
            "        epochs: Integer. Number of epochs to train the model.\n",
            "            An epoch is an iteration over the entire `x` and `y`\n",
            "            data provided.\n",
            "            Note that in conjunction with `initial_epoch`,\n",
            "            `epochs` is to be understood as \"final epoch\".\n",
            "            The model is not trained for a number of iterations\n",
            "            given by `epochs`, but merely until the epoch\n",
            "            of index `epochs` is reached.\n",
            "        verbose: 0, 1, or 2. Verbosity mode.\n",
            "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            "            Note that the progress bar is not particularly useful when\n",
            "            logged to a file, so verbose=2 is recommended when not running\n",
            "            interactively (eg, in a production environment).\n",
            "        callbacks: List of `keras.callbacks.Callback` instances.\n",
            "            List of callbacks to apply during training.\n",
            "            See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
            "            and `tf.keras.callbacks.History` callbacks are created automatically\n",
            "            and need not be passed into `model.fit`.\n",
            "            `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
            "            `verbose` argument to `model.fit`.\n",
            "        validation_split: Float between 0 and 1.\n",
            "            Fraction of the training data to be used as validation data.\n",
            "            The model will set apart this fraction of the training data,\n",
            "            will not train on it, and will evaluate\n",
            "            the loss and any model metrics\n",
            "            on this data at the end of each epoch.\n",
            "            The validation data is selected from the last samples\n",
            "            in the `x` and `y` data provided, before shuffling. This argument is\n",
            "            not supported when `x` is a dataset, generator or\n",
            "           `keras.utils.Sequence` instance.\n",
            "        validation_data: Data on which to evaluate\n",
            "            the loss and any model metrics at the end of each epoch.\n",
            "            The model will not be trained on this data. Thus, note the fact\n",
            "            that the validation loss of data provided using `validation_split`\n",
            "            or `validation_data` is not affected by regularization layers like\n",
            "            noise and dropout.\n",
            "            `validation_data` will override `validation_split`.\n",
            "            `validation_data` could be:\n",
            "              - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
            "              - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
            "              - dataset\n",
            "            For the first two cases, `batch_size` must be provided.\n",
            "            For the last case, `validation_steps` could be provided.\n",
            "            Note that `validation_data` does not support all the data types that\n",
            "            are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
            "        shuffle: Boolean (whether to shuffle the training data\n",
            "            before each epoch) or str (for 'batch'). This argument is ignored\n",
            "            when `x` is a generator. 'batch' is a special option for dealing\n",
            "            with the limitations of HDF5 data; it shuffles in batch-sized\n",
            "            chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
            "        class_weight: Optional dictionary mapping class indices (integers)\n",
            "            to a weight (float) value, used for weighting the loss function\n",
            "            (during training only).\n",
            "            This can be useful to tell the model to\n",
            "            \"pay more attention\" to samples from\n",
            "            an under-represented class.\n",
            "        sample_weight: Optional Numpy array of weights for\n",
            "            the training samples, used for weighting the loss function\n",
            "            (during training only). You can either pass a flat (1D)\n",
            "            Numpy array with the same length as the input samples\n",
            "            (1:1 mapping between weights and samples),\n",
            "            or in the case of temporal data,\n",
            "            you can pass a 2D array with shape\n",
            "            `(samples, sequence_length)`,\n",
            "            to apply a different weight to every timestep of every sample. This\n",
            "            argument is not supported when `x` is a dataset, generator, or\n",
            "           `keras.utils.Sequence` instance, instead provide the sample_weights\n",
            "            as the third element of `x`.\n",
            "        initial_epoch: Integer.\n",
            "            Epoch at which to start training\n",
            "            (useful for resuming a previous training run).\n",
            "        steps_per_epoch: Integer or `None`.\n",
            "            Total number of steps (batches of samples)\n",
            "            before declaring one epoch finished and starting the\n",
            "            next epoch. When training with input tensors such as\n",
            "            TensorFlow data tensors, the default `None` is equal to\n",
            "            the number of samples in your dataset divided by\n",
            "            the batch size, or 1 if that cannot be determined. If x is a\n",
            "            `tf.data` dataset, and 'steps_per_epoch'\n",
            "            is None, the epoch will run until the input dataset is exhausted.\n",
            "            When passing an infinitely repeating dataset, you must specify the\n",
            "            `steps_per_epoch` argument. This argument is not supported with\n",
            "            array inputs.\n",
            "        validation_steps: Only relevant if `validation_data` is provided and\n",
            "            is a `tf.data` dataset. Total number of steps (batches of\n",
            "            samples) to draw before stopping when performing validation\n",
            "            at the end of every epoch. If 'validation_steps' is None, validation\n",
            "            will run until the `validation_data` dataset is exhausted. In the\n",
            "            case of an infinitely repeated dataset, it will run into an\n",
            "            infinite loop. If 'validation_steps' is specified and only part of\n",
            "            the dataset will be consumed, the evaluation will start from the\n",
            "            beginning of the dataset at each epoch. This ensures that the same\n",
            "            validation samples are used every time.\n",
            "        validation_batch_size: Integer or `None`.\n",
            "            Number of samples per validation batch.\n",
            "            If unspecified, will default to `batch_size`.\n",
            "            Do not specify the `validation_batch_size` if your data is in the\n",
            "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
            "            (since they generate batches).\n",
            "        validation_freq: Only relevant if validation data is provided. Integer\n",
            "            or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
            "            If an integer, specifies how many training epochs to run before a\n",
            "            new validation run is performed, e.g. `validation_freq=2` runs\n",
            "            validation every 2 epochs. If a Container, specifies the epochs on\n",
            "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
            "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            "            input only. Maximum size for the generator queue.\n",
            "            If unspecified, `max_queue_size` will default to 10.\n",
            "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            "            only. Maximum number of processes to spin up\n",
            "            when using process-based threading. If unspecified, `workers`\n",
            "            will default to 1. If 0, will execute the generator on the main\n",
            "            thread.\n",
            "        use_multiprocessing: Boolean. Used for generator or\n",
            "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            "            threading. If unspecified, `use_multiprocessing` will default to\n",
            "            `False`. Note that because this implementation relies on\n",
            "            multiprocessing, you should not pass non-picklable arguments to\n",
            "            the generator as they can't be passed easily to children processes.\n",
            "    \n",
            "    Unpacking behavior for iterator-like inputs:\n",
            "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
            "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            "      yield not only features (x) but optionally targets (y) and sample weights.\n",
            "      Keras requires that the output of such iterator-likes be unambiguous. The\n",
            "      iterator should return a tuple of length 1, 2, or 3, where the optional\n",
            "      second and third elements will be used for y and sample_weight\n",
            "      respectively. Any other type provided will be wrapped in a length one\n",
            "      tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
            "      should still adhere to the top-level tuple structure.\n",
            "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            "      features, targets, and weights from the keys of a single dict.\n",
            "        A notable unsupported data type is the namedtuple. The reason is that\n",
            "      it behaves like both an ordered datatype (tuple) and a mapping\n",
            "      datatype (dict). So given a namedtuple of the form:\n",
            "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            "      it is ambiguous whether to reverse the order of the elements when\n",
            "      interpreting the value. Even worse is a tuple of the form:\n",
            "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            "      where it is unclear if the tuple was intended to be unpacked into x, y,\n",
            "      and sample_weight or passed through as a single element to `x`. As a\n",
            "      result the data processing code will simply raise a ValueError if it\n",
            "      encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
            "    \n",
            "    Returns:\n",
            "        A `History` object. Its `History.history` attribute is\n",
            "        a record of training loss values and metrics values\n",
            "        at successive epochs, as well as validation loss values\n",
            "        and validation metrics values (if applicable).\n",
            "    \n",
            "    Raises:\n",
            "        RuntimeError: 1. If the model was never compiled or,\n",
            "        2. If `model.fit` is  wrapped in `tf.function`.\n",
            "    \n",
            "        ValueError: In case of mismatch between the provided input data\n",
            "            and what the model expects or when the input data is empty.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qGf1FQf7AkD"
      },
      "source": [
        "# 6.0 Compile model now\r\n",
        "# Ref: Model.compile: \r\n",
        "#      https://wwwa.tensorflow.org/api_docs/python/tf/keras/Model\r\n",
        "\r\n",
        "main_model.compile(\r\n",
        "                     loss = ['mse', 'mse'],        # Could also be in dict() format\r\n",
        "                     metrics = \"mse\",\r\n",
        "                     loss_weights= {\"out_a\": 0.9,   # More weight to error here\r\n",
        "                                    \"out_b\" : 0.1   # Less weight to error here\r\n",
        "                                    }\r\n",
        "                   )"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEeYQ5aD7oJV",
        "outputId": "ad303073-ce25-42b9-d3af-610513562d33"
      },
      "source": [
        "# 6.1\r\n",
        "main_model.fit(\r\n",
        "               {                            #[X_train[:,:4],X_train[:,1:8]]\r\n",
        "                   \"in_a\" : X_train[:,:4],  # One input\r\n",
        "                   \"in_b\" : X_train[:,1:8]  # IInd input     \r\n",
        "               },              \r\n",
        "               y_train,\r\n",
        "               epochs = 100\r\n",
        "           )"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "516/516 [==============================] - 2s 1ms/step - loss: 2.5488 - out_a_loss: 2.5488 - out_b_loss: 2.5488 - out_a_mse: 2.5488 - out_b_mse: 2.5488\n",
            "Epoch 2/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4603 - out_a_loss: 2.4603 - out_b_loss: 2.4603 - out_a_mse: 2.4603 - out_b_mse: 2.4603\n",
            "Epoch 3/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5025 - out_a_loss: 2.5025 - out_b_loss: 2.5025 - out_a_mse: 2.5025 - out_b_mse: 2.5025\n",
            "Epoch 4/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5580 - out_a_loss: 2.5580 - out_b_loss: 2.5580 - out_a_mse: 2.5580 - out_b_mse: 2.5580\n",
            "Epoch 5/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4811 - out_a_loss: 2.4811 - out_b_loss: 2.4811 - out_a_mse: 2.4811 - out_b_mse: 2.4811\n",
            "Epoch 6/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4688 - out_a_loss: 2.4688 - out_b_loss: 2.4688 - out_a_mse: 2.4688 - out_b_mse: 2.4688\n",
            "Epoch 7/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5224 - out_a_loss: 2.5224 - out_b_loss: 2.5224 - out_a_mse: 2.5224 - out_b_mse: 2.5224\n",
            "Epoch 8/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4594 - out_a_loss: 2.4594 - out_b_loss: 2.4594 - out_a_mse: 2.4594 - out_b_mse: 2.4594\n",
            "Epoch 9/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4649 - out_a_loss: 2.4649 - out_b_loss: 2.4649 - out_a_mse: 2.4649 - out_b_mse: 2.4649\n",
            "Epoch 10/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4432 - out_a_loss: 2.4432 - out_b_loss: 2.4432 - out_a_mse: 2.4432 - out_b_mse: 2.4432\n",
            "Epoch 11/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5508 - out_a_loss: 2.5508 - out_b_loss: 2.5508 - out_a_mse: 2.5508 - out_b_mse: 2.5508\n",
            "Epoch 12/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5079 - out_a_loss: 2.5079 - out_b_loss: 2.5079 - out_a_mse: 2.5079 - out_b_mse: 2.5079\n",
            "Epoch 13/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5235 - out_a_loss: 2.5235 - out_b_loss: 2.5235 - out_a_mse: 2.5235 - out_b_mse: 2.5235\n",
            "Epoch 14/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4501 - out_a_loss: 2.4501 - out_b_loss: 2.4501 - out_a_mse: 2.4501 - out_b_mse: 2.4501\n",
            "Epoch 15/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5152 - out_a_loss: 2.5152 - out_b_loss: 2.5152 - out_a_mse: 2.5152 - out_b_mse: 2.5152\n",
            "Epoch 16/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4807 - out_a_loss: 2.4807 - out_b_loss: 2.4807 - out_a_mse: 2.4807 - out_b_mse: 2.4807\n",
            "Epoch 17/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5178 - out_a_loss: 2.5178 - out_b_loss: 2.5178 - out_a_mse: 2.5178 - out_b_mse: 2.5178\n",
            "Epoch 18/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4430 - out_a_loss: 2.4430 - out_b_loss: 2.4430 - out_a_mse: 2.4430 - out_b_mse: 2.4430\n",
            "Epoch 19/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5098 - out_a_loss: 2.5098 - out_b_loss: 2.5098 - out_a_mse: 2.5098 - out_b_mse: 2.5098\n",
            "Epoch 20/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4622 - out_a_loss: 2.4622 - out_b_loss: 2.4622 - out_a_mse: 2.4622 - out_b_mse: 2.4622\n",
            "Epoch 21/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4697 - out_a_mse: 2.4697 - out_b_mse: 2.4697\n",
            "Epoch 22/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4806 - out_a_loss: 2.4806 - out_b_loss: 2.4806 - out_a_mse: 2.4806 - out_b_mse: 2.4806\n",
            "Epoch 23/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4793 - out_a_loss: 2.4793 - out_b_loss: 2.4793 - out_a_mse: 2.4793 - out_b_mse: 2.4793\n",
            "Epoch 24/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4937 - out_a_loss: 2.4937 - out_b_loss: 2.4937 - out_a_mse: 2.4937 - out_b_mse: 2.4937\n",
            "Epoch 25/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5079 - out_a_loss: 2.5079 - out_b_loss: 2.5079 - out_a_mse: 2.5079 - out_b_mse: 2.5079\n",
            "Epoch 26/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5472 - out_a_loss: 2.5472 - out_b_loss: 2.5472 - out_a_mse: 2.5472 - out_b_mse: 2.5472\n",
            "Epoch 27/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4169 - out_a_loss: 2.4169 - out_b_loss: 2.4169 - out_a_mse: 2.4169 - out_b_mse: 2.4169\n",
            "Epoch 28/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.3888 - out_a_loss: 2.3888 - out_b_loss: 2.3888 - out_a_mse: 2.3888 - out_b_mse: 2.3888\n",
            "Epoch 29/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4496 - out_a_loss: 2.4496 - out_b_loss: 2.4496 - out_a_mse: 2.4496 - out_b_mse: 2.4496\n",
            "Epoch 30/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5119 - out_a_loss: 2.5119 - out_b_loss: 2.5119 - out_a_mse: 2.5119 - out_b_mse: 2.5119\n",
            "Epoch 31/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4842 - out_a_loss: 2.4842 - out_b_loss: 2.4842 - out_a_mse: 2.4842 - out_b_mse: 2.4842\n",
            "Epoch 32/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5055 - out_a_loss: 2.5055 - out_b_loss: 2.5055 - out_a_mse: 2.5055 - out_b_mse: 2.5055\n",
            "Epoch 33/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5447 - out_a_loss: 2.5447 - out_b_loss: 2.5447 - out_a_mse: 2.5447 - out_b_mse: 2.5447\n",
            "Epoch 34/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4896 - out_a_loss: 2.4896 - out_b_loss: 2.4896 - out_a_mse: 2.4896 - out_b_mse: 2.4896\n",
            "Epoch 35/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4565 - out_a_loss: 2.4565 - out_b_loss: 2.4565 - out_a_mse: 2.4565 - out_b_mse: 2.4565\n",
            "Epoch 36/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5151 - out_a_loss: 2.5151 - out_b_loss: 2.5151 - out_a_mse: 2.5151 - out_b_mse: 2.5151\n",
            "Epoch 37/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5106 - out_a_loss: 2.5106 - out_b_loss: 2.5106 - out_a_mse: 2.5106 - out_b_mse: 2.5106\n",
            "Epoch 38/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5202 - out_a_loss: 2.5202 - out_b_loss: 2.5202 - out_a_mse: 2.5202 - out_b_mse: 2.5202\n",
            "Epoch 39/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4465 - out_a_loss: 2.4465 - out_b_loss: 2.4465 - out_a_mse: 2.4465 - out_b_mse: 2.4465\n",
            "Epoch 40/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4692 - out_a_loss: 2.4692 - out_b_loss: 2.4692 - out_a_mse: 2.4692 - out_b_mse: 2.4692\n",
            "Epoch 41/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4552 - out_a_loss: 2.4552 - out_b_loss: 2.4552 - out_a_mse: 2.4552 - out_b_mse: 2.4552\n",
            "Epoch 42/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4774 - out_a_loss: 2.4774 - out_b_loss: 2.4774 - out_a_mse: 2.4774 - out_b_mse: 2.4774\n",
            "Epoch 43/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4885 - out_a_loss: 2.4885 - out_b_loss: 2.4885 - out_a_mse: 2.4885 - out_b_mse: 2.4885\n",
            "Epoch 44/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4720 - out_a_loss: 2.4720 - out_b_loss: 2.4720 - out_a_mse: 2.4720 - out_b_mse: 2.4720\n",
            "Epoch 45/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4993 - out_a_loss: 2.4993 - out_b_loss: 2.4993 - out_a_mse: 2.4993 - out_b_mse: 2.4993\n",
            "Epoch 46/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5412 - out_a_loss: 2.5412 - out_b_loss: 2.5412 - out_a_mse: 2.5412 - out_b_mse: 2.5412\n",
            "Epoch 47/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4853 - out_a_loss: 2.4853 - out_b_loss: 2.4853 - out_a_mse: 2.4853 - out_b_mse: 2.4853\n",
            "Epoch 48/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4820 - out_a_loss: 2.4820 - out_b_loss: 2.4820 - out_a_mse: 2.4820 - out_b_mse: 2.4820\n",
            "Epoch 49/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4480 - out_a_loss: 2.4480 - out_b_loss: 2.4480 - out_a_mse: 2.4480 - out_b_mse: 2.4480\n",
            "Epoch 50/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5310 - out_a_loss: 2.5310 - out_b_loss: 2.5310 - out_a_mse: 2.5310 - out_b_mse: 2.5310\n",
            "Epoch 51/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4750 - out_a_loss: 2.4750 - out_b_loss: 2.4750 - out_a_mse: 2.4750 - out_b_mse: 2.4750\n",
            "Epoch 52/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4972 - out_a_loss: 2.4972 - out_b_loss: 2.4972 - out_a_mse: 2.4972 - out_b_mse: 2.4972\n",
            "Epoch 53/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5212 - out_a_loss: 2.5212 - out_b_loss: 2.5212 - out_a_mse: 2.5212 - out_b_mse: 2.5212\n",
            "Epoch 54/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5395 - out_a_loss: 2.5395 - out_b_loss: 2.5395 - out_a_mse: 2.5395 - out_b_mse: 2.5395\n",
            "Epoch 55/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5109 - out_a_loss: 2.5109 - out_b_loss: 2.5109 - out_a_mse: 2.5109 - out_b_mse: 2.5109\n",
            "Epoch 56/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5063 - out_a_loss: 2.5063 - out_b_loss: 2.5063 - out_a_mse: 2.5063 - out_b_mse: 2.5063\n",
            "Epoch 57/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4835 - out_a_loss: 2.4835 - out_b_loss: 2.4835 - out_a_mse: 2.4835 - out_b_mse: 2.4835\n",
            "Epoch 58/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5260 - out_a_loss: 2.5260 - out_b_loss: 2.5260 - out_a_mse: 2.5260 - out_b_mse: 2.5260\n",
            "Epoch 59/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4398 - out_a_loss: 2.4398 - out_b_loss: 2.4398 - out_a_mse: 2.4398 - out_b_mse: 2.4398\n",
            "Epoch 60/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5225 - out_a_loss: 2.5225 - out_b_loss: 2.5225 - out_a_mse: 2.5225 - out_b_mse: 2.5225\n",
            "Epoch 61/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4972 - out_a_loss: 2.4972 - out_b_loss: 2.4972 - out_a_mse: 2.4972 - out_b_mse: 2.4972\n",
            "Epoch 62/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4215 - out_a_loss: 2.4215 - out_b_loss: 2.4215 - out_a_mse: 2.4215 - out_b_mse: 2.4215\n",
            "Epoch 63/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4418 - out_a_loss: 2.4418 - out_b_loss: 2.4418 - out_a_mse: 2.4418 - out_b_mse: 2.4418\n",
            "Epoch 64/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4589 - out_a_loss: 2.4589 - out_b_loss: 2.4589 - out_a_mse: 2.4589 - out_b_mse: 2.4589\n",
            "Epoch 65/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4833 - out_a_loss: 2.4833 - out_b_loss: 2.4833 - out_a_mse: 2.4833 - out_b_mse: 2.4833\n",
            "Epoch 66/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4716 - out_a_loss: 2.4716 - out_b_loss: 2.4716 - out_a_mse: 2.4716 - out_b_mse: 2.4716\n",
            "Epoch 67/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5026 - out_a_loss: 2.5026 - out_b_loss: 2.5026 - out_a_mse: 2.5026 - out_b_mse: 2.5026\n",
            "Epoch 68/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5344 - out_a_loss: 2.5344 - out_b_loss: 2.5344 - out_a_mse: 2.5344 - out_b_mse: 2.5344\n",
            "Epoch 69/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5199 - out_a_loss: 2.5199 - out_b_loss: 2.5199 - out_a_mse: 2.5199 - out_b_mse: 2.5199\n",
            "Epoch 70/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4823 - out_a_loss: 2.4823 - out_b_loss: 2.4823 - out_a_mse: 2.4823 - out_b_mse: 2.4823\n",
            "Epoch 71/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4702 - out_a_loss: 2.4702 - out_b_loss: 2.4702 - out_a_mse: 2.4702 - out_b_mse: 2.4702\n",
            "Epoch 72/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4932 - out_a_loss: 2.4932 - out_b_loss: 2.4932 - out_a_mse: 2.4932 - out_b_mse: 2.4932\n",
            "Epoch 73/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5091 - out_a_loss: 2.5091 - out_b_loss: 2.5091 - out_a_mse: 2.5091 - out_b_mse: 2.5091\n",
            "Epoch 74/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4774 - out_a_loss: 2.4774 - out_b_loss: 2.4774 - out_a_mse: 2.4774 - out_b_mse: 2.4774\n",
            "Epoch 75/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4647 - out_a_loss: 2.4647 - out_b_loss: 2.4647 - out_a_mse: 2.4647 - out_b_mse: 2.4647\n",
            "Epoch 76/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4171 - out_a_loss: 2.4171 - out_b_loss: 2.4171 - out_a_mse: 2.4171 - out_b_mse: 2.4171\n",
            "Epoch 77/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4444 - out_a_loss: 2.4444 - out_b_loss: 2.4444 - out_a_mse: 2.4444 - out_b_mse: 2.4444\n",
            "Epoch 78/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4385 - out_a_loss: 2.4385 - out_b_loss: 2.4385 - out_a_mse: 2.4385 - out_b_mse: 2.4385\n",
            "Epoch 79/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5312 - out_a_loss: 2.5312 - out_b_loss: 2.5312 - out_a_mse: 2.5312 - out_b_mse: 2.5312\n",
            "Epoch 80/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5314 - out_a_loss: 2.5314 - out_b_loss: 2.5314 - out_a_mse: 2.5314 - out_b_mse: 2.5314\n",
            "Epoch 81/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4786 - out_a_loss: 2.4786 - out_b_loss: 2.4786 - out_a_mse: 2.4786 - out_b_mse: 2.4786\n",
            "Epoch 82/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4467 - out_a_loss: 2.4467 - out_b_loss: 2.4467 - out_a_mse: 2.4467 - out_b_mse: 2.4467\n",
            "Epoch 83/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4404 - out_a_loss: 2.4404 - out_b_loss: 2.4404 - out_a_mse: 2.4404 - out_b_mse: 2.4404\n",
            "Epoch 84/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4665 - out_a_loss: 2.4665 - out_b_loss: 2.4665 - out_a_mse: 2.4665 - out_b_mse: 2.4665\n",
            "Epoch 85/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4307 - out_a_loss: 2.4307 - out_b_loss: 2.4307 - out_a_mse: 2.4307 - out_b_mse: 2.4307\n",
            "Epoch 86/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4031 - out_a_loss: 2.4031 - out_b_loss: 2.4031 - out_a_mse: 2.4031 - out_b_mse: 2.4031\n",
            "Epoch 87/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4946 - out_a_loss: 2.4946 - out_b_loss: 2.4946 - out_a_mse: 2.4946 - out_b_mse: 2.4946\n",
            "Epoch 88/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5193 - out_a_loss: 2.5193 - out_b_loss: 2.5193 - out_a_mse: 2.5193 - out_b_mse: 2.5193\n",
            "Epoch 89/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4847 - out_a_loss: 2.4847 - out_b_loss: 2.4847 - out_a_mse: 2.4847 - out_b_mse: 2.4847\n",
            "Epoch 90/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4556 - out_a_loss: 2.4556 - out_b_loss: 2.4556 - out_a_mse: 2.4556 - out_b_mse: 2.4556\n",
            "Epoch 91/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5159 - out_a_loss: 2.5159 - out_b_loss: 2.5159 - out_a_mse: 2.5159 - out_b_mse: 2.5159\n",
            "Epoch 92/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4855 - out_a_loss: 2.4855 - out_b_loss: 2.4855 - out_a_mse: 2.4855 - out_b_mse: 2.4855\n",
            "Epoch 93/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4776 - out_a_loss: 2.4776 - out_b_loss: 2.4776 - out_a_mse: 2.4776 - out_b_mse: 2.4776\n",
            "Epoch 94/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4449 - out_a_loss: 2.4449 - out_b_loss: 2.4449 - out_a_mse: 2.4449 - out_b_mse: 2.4449\n",
            "Epoch 95/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4946 - out_a_loss: 2.4946 - out_b_loss: 2.4946 - out_a_mse: 2.4946 - out_b_mse: 2.4946\n",
            "Epoch 96/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4938 - out_a_loss: 2.4938 - out_b_loss: 2.4938 - out_a_mse: 2.4938 - out_b_mse: 2.4938\n",
            "Epoch 97/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4953 - out_a_loss: 2.4953 - out_b_loss: 2.4953 - out_a_mse: 2.4953 - out_b_mse: 2.4953\n",
            "Epoch 98/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4987 - out_a_loss: 2.4987 - out_b_loss: 2.4987 - out_a_mse: 2.4987 - out_b_mse: 2.4987\n",
            "Epoch 99/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5154 - out_a_loss: 2.5154 - out_b_loss: 2.5154 - out_a_mse: 2.5154 - out_b_mse: 2.5154\n",
            "Epoch 100/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5415 - out_a_loss: 2.5415 - out_b_loss: 2.5415 - out_a_mse: 2.5415 - out_b_mse: 2.5415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9f0a102240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqDTxCZr8fxs",
        "outputId": "9954f9d2-f5fa-4feb-a995-59c1f2bf9e92"
      },
      "source": [
        "# 7.0 To evaluate, we must also supply two inputs\r\n",
        "main_model.evaluate(\r\n",
        "                     [X_test[:,:4],X_test[:,1:8]],\r\n",
        "                      y_test\r\n",
        "                    )"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129/129 [==============================] - 0s 2ms/step - loss: 2.4225 - out_a_loss: 2.4225 - out_b_loss: 2.4225 - out_a_mse: 2.4225 - out_b_mse: 2.4225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.4225142002105713,\n",
              " 2.4225142002105713,\n",
              " 2.4225142002105713,\n",
              " 2.4225142002105713,\n",
              " 2.4225142002105713]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MhQF056p09N"
      },
      "source": [
        "########### It is done ##############"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}