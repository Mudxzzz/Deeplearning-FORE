{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_functional.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QBKy2GzK76sc",
        "dYuF2BagAAYt"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNClWZdleG/yVj9COInDuE1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/deeplearning/blob/main/keras_functional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhHxdIbdcR6p"
      },
      "source": [
        "<table align=\"left\">\r\n",
        "  <td>\r\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/harnalashok/deeplearning/blob/main/keras_functional.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\r\n",
        "  </td>\r\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzHM-gho6Z4X"
      },
      "source": [
        "# Last amended: 16th Jan, 2021\r\n",
        "# Ref: Hands-On Machine Learningwith Scikit-Learn, Keras, and TensorFlow by Aurelien Geron\r\n",
        "#      Page: 308-312\r\n",
        "# Using keras functional API"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYl2nTZg7MRe"
      },
      "source": [
        "# 1.0 Import libraries\r\n",
        "import pandas as pd\r\n",
        "from sklearn.datasets import fetch_california_housing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "\r\n",
        "# Import tensorflow/keras \r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers \r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtYhfSLVGVC_"
      },
      "source": [
        "# 1.1 Display multiple outputs from a Cell\r\n",
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzwqXlRY6uH7",
        "outputId": "99f4173d-026f-4673-a569-d63553f74b83"
      },
      "source": [
        "# 2.0 Get Data\r\n",
        "#     The data needs little processing\r\n",
        "housing = fetch_california_housing(return_X_y= False)\r\n",
        "type(housing)   # sklearn.utils.Bunch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sklearn.utils.Bunch"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxD2EQQ56_Z4",
        "outputId": "81ea0c11-9765-46f4-e916-4ed4db5f1982"
      },
      "source": [
        "# 2.1 Seperate X,y\r\n",
        "X = housing.data\r\n",
        "y = housing.target\r\n",
        "X.shape   # (20640, 8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20640, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha_HncyCmS9y"
      },
      "source": [
        "# 2.2 Normalize input data\r\n",
        "ss = StandardScaler()\r\n",
        "X = ss.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uogWyNdXGfm3"
      },
      "source": [
        "# 2.3 Show data field names\r\n",
        "print(housing.DESCR)\r\n",
        "housing.feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzWFuUd9Mf-0",
        "outputId": "7e54bdaa-4165-4682-f781-513964ac894d"
      },
      "source": [
        "# 3.0 Split train/test data\r\n",
        "X_train,X_test, y_train,y_test = train_test_split(X,y,test_size = 0.2)\r\n",
        "X_train.shape   # (16512, 8)\r\n",
        "X_test.shape    # (4128, 8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16512, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4128, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkMLviCT1DnL"
      },
      "source": [
        "# We will construct the following two models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75OODwon02aX"
      },
      "source": [
        "![deep1.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7i5qmBX08kY"
      },
      "source": [
        "![deep2.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAeMAAAGgCAYAAACZuw/6AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsIAAA7CARUoSoAAAGOTSURBVHhe7d0HfE3nGwfwpyIyjdh771F7F7XboiiqQwdValRLWx1aRbcOihpViqpRSs1/jdp77xF7SyQRIZFI8L+/J+fGzXVDEuSu39fnfO45596b3ETufc77vs/7vI/dNhEichpXrlyRr7/+Wvr37y/ZsmUzzhKRM3nsscfEMvymM26JyEns27dPA/L27duNM0Tk7BiMiZzIzZs3ZceOHXpFvW3bNj0mIufHYEzkRIKCguT48eO6f/78eTl9+rTuE5FzYzAmciJ79uyRqKgo3Y+NjZWtW7fqPhE5NwZjIicRHR0tu3btMo7iYfz42rVrxhEROSsGYyIngW7ps2fPGkfxQkJC5NixY8YRETkrBmMiJ4CErZ07d2rXNGTIkEFvkcCF87du3dJjInJODMZETsC6i7p169bGnsjBgwd1qhMROS8GYyIngK7o4OBg3c+ZM6f8888/kj9/fj0ODw/XgExEzovBmMjBoYt68+bNxpFIpkyZ9FzGjBmNM6JzjuPi4owjInI2DMZEDg5Tmfbu3av7np6e2kJG4D1z5ox4eXnp+aNHj0pYWJjuE5HzYTAmcnC7d++W69ev6z5axeaqWzExMZI1a9aEfXNlLiJyPgzGRA5u06ZNxp5Irly5JDIyUveRWR0QEKD7YNmVTUTOhcGYyIFduHBBTp48qfvoksaUpipVqiRsOPb19dX7MQ8ZXddE5Hy4hCKRA1u8eLHMmzfPOLq/Ro0aSceOHY0jInJUXEKRyEncuHFDC3qkBOYim8eXich5sGVM5KCQNT1r1iwNymZI3kK3NcaL06VLJ0WLFpX06dMb94p4eHhImzZtpGDBgsYZInJE1i1jBmMiB4USl9ZlLrEoxNChQyU0NFR8fHzks88+k8yZMxv3xkOQxkZEjovd1EROAgEVrV7rDW9iwK2t+xmIiZwP37VERER2xmBMRERkZwzGREREdsZgTEREZGcMxkRERHbGYExERGRnDMZERER2xmBMRERkZwzGREREdsZgTEREZGcMxkRERHbGhSLcFBYgiIiISLQiEDm+yMhIGTdunFy+fFm8vb2lT58+kjFjRuNecgZeXl7i5+endcTJfXHVJjeG/+qgoCDZunWrBAYGyqVLlyQuLs64l5wB/g8RkM1vW39//4SFI8g5eHp66gVUyZIlpUaNGlKgQAH+H7ohBmM3tnr1apk/f74uw0dE9ofejZo1a0q7du20xUzug8HYDcXExMjkyZNl+/btxpn4P4T0nhnEg11lRGnq1s2bEhd7I9Fa1cWKFZOePXtqTwe5BwZjN4M3/JIlS2TevHkJ//E58uSXijUbSO4CRcQ/UxZ2kRGloWsR4RISdE4O7NgkZ44dSgjK1apVk86dO3Ms2U0wGLuZEydOyIgRIyQqKkqPS1esIbUatRAfP16BE9lTXFys7Nu6XjavWqytZQThN954Q6pUqWI8glyZdTDm1CYXhitujBObA3GuvAXlieZtGYiJHED69J7yeI16UrJCNT1GMuWaNWvkpikwk/thMHZhCMJoGZvVadZaPDNkMI6IyN7SeXhI1bpN9BbOnTsnYWFhuk/uhcHYhSEYYyoTZMycVbLnzqf7ROQ4MmYJkOy54t+bmPuPjdwPg7GLM49JZMmeU9Kl4383kaPB2CEulsm98dPZTcQHYmZNEzmi9J6exh65KwZjIiIiO2MwJiIisjMGY6JUwLSx69ej5GrEFYm8dlVioq/L7dt3KioREaUEgzFRMiEZ7kr4Zfnvf/NkSP/e0rltU2nXuLo837y2vNmxhXw9oJ+sXrZYA/Sj5mgLfOB3c5OLjhClGoMxUTJt37ROenZqLR+89YrM++sPOXxgj1wKuiDnz5ySvTu3yuypE6Rv1xdMj2kjB/buNJ71cEVFXpM50yebgv4i44z9XQkPk0ljhsm+3XdqnxNRyjAYEyXD0oVz5L1uL8mBPTvFy9tHmrZoKwOHjpIp81bI5H/+kw+H/CC16zXShTf27dom73TuIGuW/8949sOBBT+++uRd+fqTd0wXAReNs/Z17WqEvPtGRxn9wxfaXU9EqcNgTHQfWzeska8+fkciroRL/kJFZPTUf+TrkROk7QuvSYXK1eXxKjXkhde7y/CJf8mI32dLloBsEhIcJIP795b9D7G1iFrGJ44edqhyidHXr5teU6BxRESpxWBMdA9o7Y0aOlgDca48+WT8jEVSuXptrStsLYOXl9Sq11CGTZgpGTJ4SVhIsAz/eqBERUUajyAiso3BmOgetmxYLft2xbdue/f/XHLnK6D7SUE1pYpVa0inN3vr8Y4t6+XA7h26b4bx3r+mjJfd2zcbZxK7HhWl92M7f/a0nkNL+/iRQ5q1DeimPnr4gJw5eVyPAS1U82OQ6Y3XPXPyr/LnhNGyYfV/EnopONEaupbmz5qq3+9kEq3coAvnEl4TuqYBr+3ksUBTSz1OE7hwjNcUdP6s3k9EycdgTJQEBK5VSxaabm9Kztx5pX6Tp4177g0BuX2nLuKfKbMujbdSv8adIDj997Hyzaf9TOcXGGcSQ2sc92M7cmifnpvy6wh59dmGpmB7WI8n/vKDdGhaU97r9rIew5vPPy0vt2wg61YukwF93pAu7ZvLt5+9Jz8M/lB6v9pWOrdrKgtnT7PZzT3yu0H6/fbs3GqcSezksSMJryn8cqie+/qTd+XNjs/ItYgI/ZroysdrGv71Z3o/ESUfgzFREqIiI+Xgvt26X6ZCJfFNwdKTWbPlkJJlyus+Mq1vxMTofmqVLve4NGnRVjJlzqLHZSpUlqdad5C6DZvpsVlcbKwpGH6qU6yq16mniWXvDfxGXz9a0d+YgvOiuTMSapY/iGq16knD5q3E0zODXoBUq11PX1PFajWNRxBRcjEYEyXhxo0YCTGylpG4hcXfkyuDl7fkK1BI98+eOvHAwfiZth1l0A+jJV/Bwnr8bIeX5ZuRE+WdjwfrsRmSvDDVqnPPfvLjr9M0sezlN3rJqClz5Om2z0v09Sj5/vP+EhZyyXhG6r3es6988tUw8fX3Fw8PD/2eeE0vvP6W8QgiSi4GY6IkxN64IZfDQnTf3z+T3iYXWorZc+bWfXyNuJuxup8WkOHd9e3+4u3to8d4LQFZs8vbH3wuOXLn0THfxf/M1PuIyDEwGBM9IgiC9tC0ZVvx9okPxJby5C8oZcpX0v3N61bqLRE5BgZjoiSggIeff0bdT2n5SYzJXg6Nb1WjVWprKtSjkN4zg5QoXc44uluxkmX09tjhg3pLRI6BwZgoCd7e3pI7b37dP3XiSIoCckx0tGYgQ87cecQzjdarxbh2xkzxSV62ZM4SoLcxMdF6S0SOgcGYKAk+vn5SukJ8t+7OLRsS5tcmR+ilIDm0Pz4Tu1KNOuLt46v79hYbm3Zj10SUfAzGRElAhnDLdi/q1B0UzJgz7XfjnnvDnOJZUyfI9ahI8czgJU89297m+HFSBTgunj9j7KUcpjZdDk06U/rs6RN6W7hYSb219iheExHdH4Mx0T2g9OUTjZvr/vgR38mOzet1/162rFsl038fo/v1GjWTshUq676ZOTBfMKprWcJY8w5TKzy1MLVp66a1NucRR4Rflj3bt+h+hSo19NbsXq8JRU92bbVdLYyIHg4GY6J78PLylrf6fiJ58hXQRRE+7PW6LFs01+a8YXRjz546UT7u00Xvz1+wsLzVb4DWrLaUr2ARvd22ca1cOJe4xRl4YK/8Of4X4yhpWEoxKYtmT5fD+/cYR/FQsnLeX1Pl1ImjWje7VbsXjHvimV/TupVLJDwsvsIWoKWM1af++3eeccY2PA49AUSUOh6DTIx9cjGRkZGycmX8FJYs2XJI8XKV7Tbdxplly5FTylWsIpvWrtD1i1ctWyRbNqyS0yeOSUjwRTm0b5csmjNDxv70ldZ4RlBC8B7y0zgpX6ma8VXuuHX7lqz4d74G1HWrlurjz506If/M/ENGDR0kcabA6evnpzWqUdGqcNES+jz81/07b7ZcNAXwC+dOmx6TUQuKFCpSXNKlSydTxo3Qr4WvixZ8pswBOsUJr3Hqb6Nk0thhOne642vdtfvd8m/hakS4bFzzn9bA3m5qmeN7o/Tm9ImjZeIvP0mmTJk1gQ3d4C916aFfGx57LJ3MmDROvyeei9eBBTLyFSjMv7UUOHF4n4QGndf9unXrStasWXWfXNfgwYPFMvw+dvth1MUjhxQcHCyffRZfJ7hQibLyVIfO+mFJqXPm1HEZ9uUAUzBeLLeTGFvF77dB02fk3QFfSsHCxYyziSGoTR4zTMaYgrd1nWgE/m9G/S7ffvqeLvowfOJMadDkGeNekd9HD5MR3w40jkRXkpoyb6VmbDepUkxCTYHwyWYtZPO6VVpBzNfXT1ut5pZ0qw4vS/9BQxOmbJnhsVgreb6p9WytTPmK8vaHg+TTvt20cteCdXtMrf74ljQM7NtdFvw9zTgS0wVIdfl1xkLx8XWMpDVnsGL+DDm8J74ueP/+/aVYMdt/O+Q6cLFqGX4ZjF0Yg/HDh4UfjgYelP8Wz5MDe3aYgl98shSCaKmyFaRJizZSsnR5SefhoeeTgrfdqeNHZOHf0yXw4D6djoTVntASRgDdaGqFo9u7ao06ktMUcM0wJQkrJ634d4F2m2c3fV+UpERBD3MwRou8aMnSMvXXkbJ/z06d7oRa1m1eeEWq1KiriWm24MJg/67tsmTh39rqz5Y9p9Sq30gaNmupY9Gb166UGFPQbtDk6UR1uiNNgf7X4d9qxjmytQsXKyGffjtC/FJQy9vdMRi7HwZjN8Jg7F7MwfiLYb9qNzQ5DwZj92MdjPnJTEREZGcMxkRERHbGYOwmbqKUI0ckiBySvj/JrTEYuzCMD5vX4A27dEHnmpLrKl66nJQuX1EyGfWnyTlg3DAk6Kzu4z3LvA73xP91F+br6yu5cuXS/ahrV+X86eO6T65p7LT5Mn3xOqnf+CnjDDmD4PNnJOJyfKGVgIAA3cj9MBi7MB8fHylRIr5gBGxYNl+io6KMIyKytxvR0ab35byEmuCFCxeWzJkz6z65FwZjF4bU+YYNGyZcaYeHBsvKhTPkSlj8OrtEZD/XIsJl7b9zJOjcKT1GT1bz5s31fUvuh/OMXRz+e9evXy9//vlnwtW3f6bMUrZKbclfpKT4aiUmvvmJ0kq0lj89Kvu3b0jonoa2bdsyGLsRFv1wQ6istHDhQlm6dGniBfJNfwx82xOlLf3AtfjYRcJWvXr1pGPHjklWRyPXw2Dsxnbv3i1z5syRixcvGmeIyJ5y5MghLVu2lBo1ajCL2s0wGLu5iIgI2b9/vxw5ckTOnDmT0HVNRGknf/78Urx4cSlfvjyzp90UgzEliI6OTvTHQESPHj6Evb29jSNyVwzGRC4iNDRUsmXLZhwRkTOxDsYcpCByQhhemDhxoty4ccM4Q0TOjMGYyAkdPnxYjh8/LocOHTLOEJEzYzAmckIbNmzQ1vGOHTuMM0TkzBiMiZwMMuIPHDig+2gZX758WfeJyHkxGBM5GUxNi4yM1H0E4oMHD+o+ETkvBmMiJ4Ku6S1btiTKwty+fbtWWSMi58VgTOREUD3t5MmTxlG8Y8eOSVBQkHFERM6IwZjIiezdu1eirJbBvH79uuzbt884IiJnxGBM5CRQMW3nzp3GkUi5cuWMPdGs6piYGOOIiJwNgzGRk0AXNeqJQ4YMGSQ2Nlb8/f31GF3XXACEyHkxGBM5ASRsbdu2LWEJTCwwcOLECcmcObMe4/6tW7cmSuwiIufBYEzkBDAujCUwzS5duqQtYwRj1LgFBGNmVRM5JwZjIieAVnBwcLDuYw1cc5f0uXPnJFeuXLofHh4ugYGBuk9EzoXBmMjBoet58+bNxpHpTZsuXUILGJnVfn5+ug8bN2409ojImTAYEzk4ZEnv2rVL9z08PMTX11eLfwC6qtFl7enpqccok4lymUTkXBiMiRwcpjOZpy2hlYwSmFmzZk3YEKDNLWWUyeScYyLnw2BM5MCsu6jRIsbYcFhYWMKG4GxuKZuzqs3HROQcGIyJHFhoaKgma6Fr2rz5+PgY94pmUuPY8n4keqHrmoicx2OmK2lOTCRyUOieRkvY0tWrV2XkyJFakStLlizSvXv3RElcEBAQoIVBiMgx4ULaMvyyZUzkwLy8vHTqkuWGqU3IqIb06dNLzpw573oMAzGRc2EwJiIisjMGYyIiIjtjMCYiIrIzBmMiIiI7YzAmIiKyMwZjIiIiO2MwJiIisjMGYyIiIjtjMCYiIrIzBmMiIiI7YzAmIiKyMwZjIiIiO+OqTW4oLi5OgoKC5MSJE3LmzBnjLDkLrOS0ZcsWuXnzpnh7e0uVKlW4MIST8fDw0AU+ihYtKnnz5tUFP8i9WK/axGDsZhCE58+fL/v375fr168bZ4nIHnAxVaxYMWnZsqUGZnIfDMZubPPmzTJnzpy71sclIvvy8fGRdu3aSb169Ywz5OoYjN3UgQMHZNSoUdq1CR4e6aVg/iJSsEAx8fXx13NElDaioq7JmXMnTNtxiY2N1XPoqu7UqZPUqlVLP6jJtTEYu6GIiAgZPny4nDt3To+zZc0h9es8JYULFZd0jzGHj8hegi5dkBWrF8r5i6f1OGvWrNK3b18dTybXZh2M+UnsBv7777+EQOzvl0laPfWiFC1ckoGYyM5y5cgjzz7zomQLyKHHYWFhsnbt2kQf0uQe+Gns4pB5i2QteMz0r3aNhtoyJiLH4OfrL0/UbmocCZMr3RSDsYsLCQmRS5cu6X6GDF5SuuTjuk9EjqNggaLi75tR9y9evCiRkZG6T+6DwdjFRUdH6wY5sueWDJ6cj0rkaDzSpZeAgOy6jyTL0NBQ3Sf3wWDsRpihSeSY8N705IWyW2MwJiIisjMGYyIiIjtjMCYiIrIzBmOiZLpx44Zs275JvvtxsLzWtb080+YJafdiM+nz3hsy6Y9xcu7cabl165bxaPfG3wNRyjAYE90HCjDs2bdTuvfuJC93bi0TJ4+WTVvWyrHjR2Tf/t2yZNlC+eb7gdKy3ZMyfNR3ci3yqvFM93PjRoysWfefbkSUfAzGRPeAQLx4yTx5s+dLsmHTavH19ZNGTzaXT/p/Ib8MnyQ/fTdWXnmpq+TJnU+ioiJl3G/DZdCXH0ps7A3jK7gP/Mw/DP9Ser7zupw8ddw4S0TJwWBMdA+7dm+TL78ZIOHhYVKkcDEZO/IPGfnTRHmtUzdp0uhpafF0Wxnw4Zcy9fd/pGnjFvqchYvnyKw5f+q+O8GCBzt2bXXLCxGiB8VgTJSE6OjrMnzUtxJ2OUSyZA4wBeHfpWb1unctBI85ovnzFZTPB3wjJYqV0tb0uAkjJCwsxHgEEdG9MRgTJWH3nu2yacs63e/etY8UL1ZS95OSI3su6fRiV/Hy8tYEpsCjh4177sD5S5eCdJx51NgfZcQv38nif/+R8xfOJixvae3I0UNyOPCAXhzgMafPnJT5i/6WqdMnyPIV/5MLF8/fM2EKFwfXTc/dsWuL/D5lrAwb+Y1M+XO87N23U8d4k4Lvt//gXpk7b6aMHP29Pu+v2X/I3v277mr9RsdEy9WrEabXFxd/bHpuRMQVtx4/J0oJLqHo4o4dOyZDhw7V/QL5isjzbbvoPt0b3hZDvv5Yps38XbJlzS4zpy6WAvkLGfcmLTLympw6fcIUuEtJhgyJKyqFXwmX8aYW8wxTQLt2LcI4Gw+Lyz//3CvSs3s/yZIlwDgbr86T5STqepSMGzlVVq5ZKjNmTZHrpmMzjGO/1fVdea3Tm+Lt7WOcvWPtuhXy08iv5YApsFpCi75GtToy6NOhUrRIceNs/AXD6rXL5acRX0vgkYPG2TvwvCfqPCmffPilFC0c/zyMFY+fOFL3LRUrWlIW/7PWOKKk4Hc+b/E0OX4y/gIOyyiWLl1a98k14X1kGX7ZMiayAR+OaBkDAmuuXHl0/378/PylbJkKdwViBM/+n/SS3yb9oq3GalVqyds9PpD33v1UatV8QsdbJ//5q3xgegxasdbi4uJk+Khv5I9pE0yBs4S8+Pzr0rZ1R8mXt4Amjg0zBds582YkenPD+o2r5N0P3tRAjK70lzp2lk/6f6m36HrfvHW9fDLwHQkPv2w8Q+S/lf/K+x/30kBcvlwleb1Td/n4gyHybu+PTK+1nqRLl07Wrl8p337/ucSYWsSAr12lUg29MABcuFSrWkvKl62ox0R0b2wZuzi2jFMHwa9uw/Km1uxladfmJfl6yDDjntQZ8+swHX/28vKSfn0GSMf2r2prGBCoZ5pay2iJIri9984A6fZGH70P0DIODQvRRT7e7tVfXjIFYn//jBp40Qp/u18XDZxlSpeXuTOX6xU3oJv4+U7PyImTR6Vq5Zry3VcjE1r3uNjYsm2D9O7bWbuX+7/3uXR5tYc+p3O3DrL/4B5pUK+J/PjdGMnon0mfA5hr/bPp58BFBVrhs6cv0XFywEXBq13bafc3gvfrr3TX83R/bBm7H7aMiZIJgRg8PT31NrWuXbsq0/+arPtPN28tnV58IyEQg4+Pr3R66Q155qnWejztr0n6HGuVK1UztVK7aSAGvJkLFyoqbVo9r8cYS7aEqVinTcHa29tbg61lNztatzWq1dZpWrBq9TK9vRIRLjdib4iv6TV16/J2okAMaPG3f+5l3UcL/15jzkSUfAzGRI/YvgO7NSMbq/K0btnhrmxsSO+RXlo+/ZzpPk9N8Nqzd4dxzx01qtXVNamtFS9aQm8xXo0WltmWbRvl5q2bUrxYaalYoYpx9o506Tzkw/cGydrlu2XiuJl6DgF77l/LZfPaQ1K1Sk09Zwnd6ZYXJ+fPnzX2iOhBMBgT2YCe3hw5cul+XFys3qbWhQvnNIihizp//oLG2buhlettegy6yI+fPGqcvSOpBDIEcLPgSxeNPZFjxwP1tojp65q7rq0hOS1nztx6oYDHYPM0fT0EfbTO0Q2OFjaSxr4f9oW82fNFee6FZsaziehhYTAmssUUlPLlya+7qCaVku5YtGwRUK15eHiIj41sZ7NcRlAEjOOmhmXL2JyUlSlTZr1NLgThCZNGS6cubeXZdk9K527Py+dffCC//T5Ktu/YLFkDshqPJKKHhcGYyIZ0j6XTbGA4amphBgXdaXHeC5Kx3uz1srz2ZjuZM2+6cTZ5bpoCqTmdw9YUpZR6LF382zup+cu2IIHsy+8GyA/Dv5BDh/dJQNZsmsjV+dUeMmTg9zJ9ykIZN2qq8WgielgYjIlsQHdtw/rNTLfptBTmug0rjXvubd2GVZrZvG37ZomMjJ8LnDFjJh2fRSbypZBgPWcLxl/RAsf3zp07r3E29fIYXwNFQZJy5cplnf504NBeLdixdv0qmb9wtrawu7/xjsyZsUxG/zxZPnp/kGaAly9XUdKnTzxti4geHIMxURLKlXtcqlapofsTJo+Rc/dJVrpsCtqjf/1Jg1r27DnlqaYt9XyRwsU1oxmt5q3bN+o5W7ZsW6+PQVe2uZjGg8B8ZDh0eL8md9myZt0K6drjRXnx1VY6v3n9plXaksb85Tde7yFZA7LdlXB2OHC/sUdEDwuDMVESfLx9pUfXd3Ua0pmzJ3U+LwKb5bgsYK7ghYvntGIXimugZfvGaz0SEsCKFS0hFcpV0v3pMyfL2XNndN8SxpmnzZykX6t06fIJgfRBNG74lHZ3ozU+e+400+tO3F19zRSgsaAFfp7aNetpwY4bMfFj4x4eCMB3J33hYgHjyfeDFj5LGBAlH4Mx0T2g4tSbnd/W6Tz7D+zWohYYU130v7nayl2xeqnWl36ly3NaYxoQBF9+MXFxFRTyQLBDAY73PnpLK1+hkAfmMmOlo/c/7qn1pzHnuN/bnySaPpRalR6vKs2atNAgPOKXoRrsUQMbhT2OHjssPwwboj8DAnYX08UDxskfN6ZA4eJjyrTxcvlyqB4joWzn7m3St3832bZjk85T1vMWZT3Rgvb28tb9hYvnavc3HpuSMWsid8UKXC6OFbgeHMZ6/1nwl4wa84MEBV8wzt4NC0S0f+4leafnh5I5cxbjbDy8zf63dL58+c0nGoTR6s6eI6d4pPPQ6UioXoXylJ9+9JW0atHOeFY8cwWuoV+NktatOhhn71i/YZV0eauj7q/4d5t2MZshiH70WR9dUAIBFItZ4KIA4+DoVkcgfr/vZ1rVC9neYabg27d/d9m0ea228AsWKCyZM2XRbu4LQefllimwvmS60Fi3fqWOjfd66z3p07O/8d1Ef0cjx3xvHImWEf172pKEXgKyDb0TrMDlXvD+sgy/HoNMjH1yQZcvX5b169frfuZMAVKuTGXdp+RDkEK9aaxdnCd3fm0BouWK8pSohoXVnJo3bSUffTBYnmv9ginY+RrPvANvPNS4btQgfo4uEqdOnT6pQTFb1hzydPNnZfBn30utGk/oYy1hgYnSpcpJ7ZpP2KyRjTnM8Hj5ylKvTsNEmdiY29ywflNTgM5vahFHyEVTQMUFBV57jep15aP3B0uLp9rozwhomTeo11h/BlwAnDp93HSxEKxJaHjtAwd8K88+005b26jGlTFjZq2zbR5XLl26nNw2BZYwBHovH8mRLac0aviUXmhQ0vChfPjIXtMFUnxPRO3atSV79uy6T65p8ODBYhl+2TJ2cWwZP1x4u9y+fSt+GpLx1kH3LoKZdRBNCp4fF3czYQwXU5BQgcvc9fso4LXi+6HLGPt4rRgXNgdha3gM5kqbXyOywfFY82vEz2D8+Pq1LH92889n+k2J6R4N1Mn93bgrtozdD94T5s8QeHTvfiIXhDcQApNWqTK1LrGlNNhguhRa1ujWxoav8SgDMeD1IfiispZ+T9NtUoEY8HjL14h9y9eInwHH2Kx/dvPPh58Ltyn53RC5KwZjIiIiO2MwJiIisjMGYzfC5e6IHNOt27c0o97sXkMI5JoYjF1cpkyZdIOQsGCJuHZF94nIcaCYyqWQ+GlzGTNmZCa1G2IwdnEBAQGSN298jWKUadyybY1mbhKRY0CreP3m/3TtacifP7/4+/vrPrkPBmMXh0zfunXrJnR77T+4Qw4F7mZVJCIHgKljh4/sk4OHd+sx3qe1atXSLHRyLwzGbqBKlSpSqVJ8beQ4U+t42ar5smLNQgkLD0k0z42I0gbed5dN778VaxbJ0hX/JMznrlq1qm7kflj0w02EhYXJTz/9JJcuXTLOiFZIypQxi845JaK0cyM2RiIiwiU65rpxRiR37tzSu3dvyZEjh3GGXBnm31uGXwZjNxIeHi6zZs2SHTt2cNyYyEGgcErlypWlffv2kjVrVuMsuToGYzeHseKtW7fK2rVrJSgoSGKMJfMobeAi6EHG663frvggT8mFFT4AzJut6lmUdlA3PE+ePJrTUaNGDf3/IPfBYEwKH+BoKWMhCUo7p0+fltmzZ98VVJML/2/m5yIXoGbNmrJ48WI5deqUnrsffOCba0zjFgEBGfd+fn7SuHFjXfyBHj18EGPKIVrCDMLuicGYyI6wctL3338vwcHBxpmUQxBt1KiRtGrVSoMpejjGjx8vZ86cMR6RciVKlNDFCcxZ90T0aFkHY16SEaUhFHSoUKGCcRQfWNEyTWqzbjWh5dqxY0dp06aNBmLIlSuXdOvWLWE+uVmpUqX08dWqVZOSJUtqlyi+vy1NmjRhC43IjvjuI0pDuBpG97K5O9jHx0dy5sypQwaWG8aVsYQeHm+WOXNmDbr169dPWD/YDF+jT58+GnDNTp48KfXq1ZN9+/bpUppoQV+7dk3vw9cwK1asmJQpUybR9yKitMVgTJTG0II1lzuMjIzUrirLDS1etHY3btyYkOyFqkzvvPOOtqqTCppoSWNqTIECBfQYyXlr1qzRheoRvM3jzQULFpRt27bpY/C10OVtbmUTkX0wGBOlAQTBkJAQWb9+vUyYMCFhvjfOYw64ORiiOAsSe44cOaLHgBYyWr358uUzziQNQb5r164JXdaxsbEa1DF1xgwt7KioKN1HpScE7ejoaD0mIvtgAhfRI3T9+nU5evSotkQPHTqkXdDWkMmMoIhAim7kixcv6nmM4T7xxBM6PozHpASytn/77TftmgZ0hyPgIxAjicwyix7j1gj0Tz75pFZ/8vb2Nu4hokcFvVKW4ZfBmOghw1sKmc2Yz71r1y4JDQ29a24xuo0RHNFNDUWKFNHWsnlMF/e3bNlSE6tSW6f47NmzMnr0aP3+4Ovrq99n//79emwNwR9VoDDOjK5tBHAiejQYjIkeAQTbK1euyM6dO7Vb+Pz58zYDMFqgKPCAFmhgYKBMnDjRuPcOtII7deqkXcsPmlSFVvbw4cNtzidv0aKFxMXF6etFa9kS5r82aNBAAzOC+IO+DiJKjMGY6CFBQhRasgiqaAUfOHBAbty4YdwbD13ACGyPP/64FuhAcpV5ChHGavv3759ovNY85osW7MNy7tw5GTt2bKK5zVmyZJHPPvtMl+pDIEaiF8azEbQtPxLQrY2gjAuIbNmycfoT0UPCYEz0gDAOjIpX27dv12lDSMCyhmCHub3mOb62unzx1psyZYps2LBBjzHFqHPnzo9koQDrMeRnn31WW8aWrl69Kps2bZJ169bp4yw/GpBUhq5rXFAgOYwtZaIHw2BMlApoBaOFiW7o3bt3a/cvungtYe4wAioCMObtokV8v6B18OBB+fnnn/U5HTp00Jboo2IOyGjNDxkyJMkF7NFS3rNnj6xatequql7oskYXe8OGDRmUiR4AgzFRCqC1iACMbGi0hm1NAUJQQjBFMQ90M6ck4QpfD1+7evXqaTLXFwEZ3eqoQ32/QIoeACR7LVu2TAuIWEJLv1y5ctK8eXPtemdQJkoZBmOie8DbAWO5mOeLMVSMA1uvbIU3EcpKIsGqVq1aWkQDY8OpCUjmt19aBTN8P2zJHfvFY9EDgBb80qVL5fjx44kS05CUhl4AdHmbfw9EdH8MxkRW8BZA4hUyoNFKxXQkFOiwhm7dokWLagAuW7as2039QVc9Lk5WrlyprWvLZDV8sKCljO5rjJFz9Seie2MwJjKghYe5vRgf3bFjh3bhWk9HQtcxumHRCq5YseIjSa5yNgjKCMbIwEZwRne2GVrGxYsX1ylRKN3JAiJEtjEYk1vDnzsSlBBE0Ao+ceJEQuENS1hwAQEY5Smxz5be3XDhggSv1atX67i6ZVDGB03hwoV1QQr8DpH4RUR3MBiTW0KNZqxctHnzZk1KQkC2/tNHwMB8YMypRSBJaQlKd4WWMuYwr1ixQrZs2ZIoKJureqHUJn6vrOpFFI/BmNwGEo8QJFCQA0EC84EROCwh8xnTkerWrSvly5fXblUWtkgdfJSg9jaCMpLfrHscMPe6WbNmOlcZFzr4MCJyVwzG5NLQdYoqUijGgW5oLNJg/SeOLme01jBfFhumIzEwPFyYEoYxZRQQsa7qhQIiaCkjKKOqF3/35I4YjMkl4cMfSUWoioXVkaxbZWjt4oMfSUWYE4xpOKldgIGSD/W60TOBoHzhwgXjbDwEZcyvRrnNnDlzMiiTW2EwJpeBcWAkYCETGuPAlrWXzTAdqVSpUjpeiWlJmB/MD/20h4ulvXv3ahe2dVUv/J8gUx3TorCQBv9/yB0wGJPTw3QktLYQhFFD2XpxBrSCsdACAjASslBiksUoHENUVJQG5f/++08rmllCAh16LrBsJKt6katjMCang6Qr84c4FjJAVjRaxZYQgDH2i25PBGHMB8Y5fqA7HnzkILkO4/ootYneDcvEOlw4YTpUo0aN9KKKF1LkihiMySngzxIBGDWRMR0JH9y2xoHR6kU5RgRhdEfzg9v5YIgBpTYRlC1Lj+L/F1W9kOxVokSJNKndTZRWGIzJoaHFizFFlKREZSzrpB/AtBjz6kilS5d+pCsdUdpAFjzqXmNMGXWwrQuIIBhjcQtccHGuMrkCBmNySBgHxtKEGAc+e/bsXYszoMVbqFAhrYqFcUVk37IV7HrQfY0a4QjK+FuwbikjCx6JXkj4YlAmZ8ZgTA4DrR+MA2M+MMaBsc6uNawJbF6eEEsVsqvSPaCljOQ8rKmMPAHroJwrVy4dU65duzanqJFTYjAmu8GfGlo+WJBhw4YN2hJGALb+E0RWLcYKsToSVgDChy3+cMn9ILELRUPWrl2rNbCRR2CGvwlU9UL3NYIyq3qRM2EwpjSHcWDMAcZiAuh6PHfunHHPHehyRBckWsHIpEVBCCJLKCCCoLxx40YJDQ1N9EGGoFynTh154okntDeFQZkcHYMxpQm0aFALGsk4qIqFbmjr+cAY80XXM4IvNuyjC5LoXhCUUWscQdn6wg7JfLigw2pR6MpmUCZHxWBMjxTGgVGOEgEY5SnxwWkNLRckYaEuNFrDTMSh1MBUN+QboAY2kv4soaoX8gywrnL+/PkZlMnhMBjTQ4dkG4wDIwCjK9rW6khIvMJ8YCwOgMXnWZaSHgZ8fOECENPgkIFtXdULF3rIwEf9ayyLSeQoGIzpoUAARqsXLROUpkR3Ic5ZQjc0yhoiAKOVgnFgdkPTo4CPMfz9YVhk4cKFenFoeUGYPn16LY3atGlTnSLHaXFkbwzGlGr4U8Gi/OiGRhA+cOCAZkdbwoccylKiNYIAjGDsagEYCWnIAsfcaOseALI//J8cPnxYK7chE9sSPgBROATJXiwW43zw/xcQEKA9Hsied+bPFgZjSjF0A6IsJVrACMDWH3CANwimIaEsJW5dcT4wfg9YNB/VwdATYDnNhojSBoIY8k4w3FW3bl29dcaeDgZjShZ0+ZnLUqIwB6oiWbcCvb299Y2AFnDZsmV1egn+wFwRLkbmzJmjLS4icgy46EcvR+vWrZ0uEZTBmO4J3dCYC4xWMDJUo6OjjXvuQNczMqERhHGF6uoVkDAOOWHCBF2T18zT00MK5csuvr6sCEaUVmJjb0rY5WsSFJJ4lgbKo3br1k1zA5wFgzHZhCIKaPlhdSTrAIw/GmQ/IwAjGQvBGN1CrtoKtoSLk59//jlh6oxXhvTSvGElqV+7jPj6eJl+B3qaiNIAohV66C4Gh8v8pdtk74EzCQHt6aef1hays3wuMRiTTVOmTNHxUEsIwFgtB+PA5cuXlwwZMhj3uAd01eMCZfny5Xrs7+ct77z5tBTMn0OPich+4uJuyqLlO+R//+3SoIYyuv3795c8efIYj3Bs1sGY80xIWSYjYT7mCy+8IB999JF0795du6PdLRADikpgkQLAG6ddy5oMxEQOIn16D3mqYSUpWSw++OIzDJXZnBWDMd2lZcuWukwdpii5M3RNm1eSyp83q1R5vKjuE5Fj8PLylAZ1yko6o2saCZbOOt2QwZgoCRcuXDD2RIoXzq3jxUTkWArlz64JlRAeHq49Ws6IwZgoCZbjOX6+SNZithaRo8kWkFE8POJDGRajQT0AZ8RgTERETgsXya5wocxgTEREZGcMxkRERHbGYExERGRnDMZElCyDvhkn5Wp0kLpNuxhn7m3Zys36eGz7Dx4zzsZr9/IH4p+7nnzw2c/GmeQ5duJswte8cOGScTZ5Bn41Vp/X96MfjTOODQmEEVcjZeyE2dKkVQ/JVaypZMxTT0pVaSedewyWdRt3yc2bXDXMVTAYE1GyXAwKlQOHTkjg0dPGmXu7di1KH48tOvqGcTZedMwNiYy6LjduxBpnkgePN3/N2LjE62ffz4WLIfq8s+eDjTOObeeew1Ktfifp0fdbWbl2u/68/v5+cubcRZn05wJp3OotvUCKtVrGlJwTgzERpbkypQpL7RqPS+FCeY0zZCk8/Kq82m2gHDl2RoqYfkfjfv5ENiybKLvW/yn/+3uEtGn5pLaKv/p+osz8e6nxLHJmDMZElOZ++KqvbFg+Ufr2fMk4Q5Zmz/tPW/GZMvnLlF8HS9fX2kjpkoUlV85s0uCJqjJ53GBp2rCmdmUP+vpXiY1l69jZMRgTETmY/y1br4G2Ts3HpW6tSsbZOzJl9JM3Xm0t6dKlk9NnL5pa0MkbOiDHxWBMRGlu2Kg/NQnpz7/+Nc7cgdV4ps/6Vxq37CHZCjWS7IUbS9Nne8ryVVvuWXcYweuMKTC998kwKVOtvfjnqSclKz8n73z4o1wKuWw8yjY89+ixM/K+6bmV674kmfM1kJxFm0qjlm/JhCnz5Hp0jPHIxEaMnaE/x/pNu+Tq1UjTzzVN6j/1phSv2EZqNX5devb7VpPO8PVTokzJItLqqXrSpkWDJAta5MgeoGUg8bWR6EXOjcGYiNIcAiuSkLZs32eciXctMkre6D1EXnrjU1mxZqsG5vQeHrJ2405p2eFd+WX8LOORd0N2cd1mb8hPpkB/9PgZ8fXxltCwcBlpCpgI7CdPnTcemRiWysRrqf7kq/Kj6bkHA0+It3cGDXKr1+2Qrr2/kGc79pVTZ+7UKjdbsXqrPnfN+h3StHUv6ffJT7J950EJvhQmm7fukzG/zZbajTvL4qWJlye9ny8H9pT5fw2T7l3aGWfutmnrXomJiTUF5PRSolhB4yw5KwZjIkoRBK9Tpy/cdwsJDTeekTy3bt2W74ZNlqkzFotneg/p9WYHWf2/X2X72qkyb8ZPUq50UQ1utiDDuFufL00t4yApUjifjqluWTVFNv73uwz6uLucMAViXADYsmzlFunT/wcJv3JVmjepLYtm/SzbVk+VzSsny6gfPpBcObLKctNj3n7/e4mKijaeldi3P03WruKBH3aVZfNHy8rF4+Snb/pJlsz+2iof/M2v+vUflnMXgmXiH/N1v1a1CpIta2bdJ+fFYExEKXLlyjWda3y/7eNBo4xnJM/psxdk0tQFGpS7mVqEw759Tyo9Xkry5c0pzRvXNrUUf5IypYoYj05s+qwlcijwlGT095NZk7+Vlzo8JYUL5pGSxQvJwI+6yvDv3jMemdjVa5HyzQ8TdRpW4wbVZeakr6XxkzWkQP5cUtQU1Ht07SCz/vhOvL0yyL/LN8iSFRuNZyaG50/4ZaAMHvCWjvNWrVRG+vZ6SS8EANPBzqdwXnRSYmJuaBY1vmaGDJ7ymennI+fHYExEKXLr9m1Ty+zSfbfQsCvGM5Jn7YZd+jx0u37yXme9tZQ3dw558/U2xtEdmK88f/Ea3X/u2YZSuWIp3bf0/HNNpVSJQsbRHQjgG7bs0f3PTK3azJky6r6lWtUryJP1q2rG8rSZ/2rXubXHy5eQp5rWMY7ueKZZXb3FmO6ViAcf10WvBIqXjJ80V4/f7/OK1K9TWffJuTEYE1GKoOt176aZ991Gfv+B8YzkWbthp47TljW1fnPmCDDO3oFEJkzrsRYSEq7TgKDBE1U0w9ian6+31Khazji6A4lXCK758uaQQqaWdGTk9bu2mBux2hUMu/YeNgXVa7pvqWyZopLB6uIBcuXKqrf4uZDg9SBQKKXfx8Pk+5//0DnGyKb+5P3O4uERv5YvOTcGYyJKkfTp00v5ssXuu6F7OSXOGZWxChbIbTOgQo5sWSRn9sSBGpnOl8MjdL9wQdtFRPD1ypQsbBzdcfzkOb09d/6SFCn/rGZgW28oQTnom1/1cWjhxsbeXTUscyZ/m6/ZMhM69HLKegosBQWHyitdP9Ps7fTpPaR3t+dlxND3TRcZPsYjyNkxGBORQ7lXS8/LK4NkzOhnHN3tXs/NbhXE4fr1+ClLPj7eWlQDY9L32koWL5jmLdGDh09Iu5f7ayEQjF0P+rib/PDVu+LLQOxSGIyJyCH4+HjpbdT16CTn5WLcFvdb8vBIJxk8PXU/qWxnsNW9bP6eCLLL5v2iVcHutS2c9XOaZS7jd4Bu9Gc79pP1m3eLv7+vjB/1qXzU73VN3CLXwmBMRA6hpDFXFlnCyKi25VJouI4RW/L385XcubLp/qHA+LFjaygWcijwpHF0R7Ei+fX23PkgbfFmyZzR5nYt8roulIGksqS60B+23fsCpcOrH+mcaYyhT/vtS3mpw9Np9v0pbfF/lYgcQsMG1cXLy1OLc6DIhzUE6LnzV9y1SlH2bFmkZrX45KwZfy+1uRLU5StX5d/ld09LwmIVSLwKCb0is+Yut9kix9fDHOZKdV+SZ9q9o4VJHrWwyxHyUpcButJUoQK5ZdHsEdLy6XqmQGy7Ghc5PwZjInII9WpXlifrVdP9dz/8UQt4WNqwebeM+vUv4+gOtFZ7d+8ofn4+snnbPhk+errE3LizZGN0dIwM+mpcQoKYpXJliupFACBJa+PWvYkCcpwp8E+b9a8sWb5JYk1B+YV2zbQl/ijh+w/8cqwcPHxSu6PRLZ01S0Y5cfKcHD9x9u7NdB4/Izk3BmMicggoQTn4k+6SK2dW2bv/qK7X+9X3E2TG7CXy6ZBfpM2L7+mYsa3x0to1H5euxsIJnw4ZLS92HqAVqlBXusOrH8ro32bZHOtFNvIPX74rZUsX0YzsVs/3lfc+GS5/TF+kZS7f6PWFvPXO19rN3axxLXmhfTPjmY/OmXNB8vf8/3QfrXKsZ1ysYpskt1JVntO1j8m5MRgTkcOoWa28zJz0rVSsUFLX8v30izHyYpcB8tUPv4u3l5eM+P4DXbHImmf69PLFZz3l0/5vaLCeu2ClKZAO0brSi5eslw5tmkiXTq2MRyeGaVj4nk81qS1XrlyVYb/8Ka92/1wXgJhiCso3TYH4xfbNZcLogTp+/KjtP3jsvgtbkOt57LatQRJyO2PHjpWdO+PH6Xr37i0VKsQXOXBny5cvl1mz4hcmaNm0irRqHt+F6q6w+AHm2aL1iVKR94OxVSQ9Qf58uXRajhlKQyIrGvNzsfqQNVTvWrlmm2YTo+gGqme1bdlQcuQIkK3b92sLuW6tiqbWdHw2tBkC5+HAk7JoyTo5dvysZMzkJw3rVdNiIMHBYdqli2SoCuVKGM+4A69nx65DugDD8RPnTJ+O8Qle9WpX0gpb1t8LMKaLCmBYd9h6/jOgRW2ey4y1iDP637+L2/L3llyY0+1j4/W5i34DJ5v+H2IkY8aM0r9/f8mZM2Vz3O0Bc9Atwy+DMSkG47sxGBM5B1cIxuymJiIisjMGYyIiIjtjMCZKhqSKUBCRfSGnACuJAbp+LeuBOxMGY6Ik+Pjcqf17/mKYLl9HRI7lzNlQib0RXwjGz89Px42dEYMxURKKFSuWsChA4PELcik0fmUgInIMSIDauD1Q4m7e0uOiRYuKt7e37jsbBmOiJAQEBOibG6Ku35Dpc9frlBoisj8E4h17T8im7Uf0GBfOlSpV0n1nxGBMlAQvLy9p1apVwpX2oSPnZeK0lRIckvp1aYnoweGieP3mQ/LXPxskLi5++KhixYpStmxZ3XdGnGdMivOMbcM48cKFC+V///tfwpzAgCx+UqFMQSleOLcubEBEaedCcLgcOHxGjp0I0iIvkCVLFvn444/11lmw6AfZxGCctOjoaJk/f76sXLlSKyoRkePInTu3vPLKK1K8eHHjjHNg0Q+iFEI3dfv27eXtt9+WHDlycD1ZIjtDIMMwUsOGDeWDDz7QZEtnx5YxKbaMkyc2NlZOnDghhw4dkqCgxEv8kXO4ePGi/t/hb5wXVs4nQ4YMUqpUKSlZsqRkzZrVOOt82E1NNjEYkzvAMMO4ceNk9+7dMmDAAClQoIBxD1HaYjc1EbmtkJAQOXbsmH4Ibt261ThLZH8MxkTkNvbv3y9Xr17V/W3btklcHOeNk2NgMCYitxATEyM7duwwjkQuX74shw8fNo6I7IvBmIjcQnBwsJw8edI4ih8/3rJli3FEZF8MxkTk8jBGjATFGzduGGfiISs+PDzcOCKyHwZjInJ5mJJm2UWNOapw5coVDchE9sZgTEQuD93TmF8M2bJlkyJFiug+WsxI5GJlNbI3BmMicnkYGzbP6UTgRbEP8/KYx48f1/FkIntiMCYil3b9+vWEgjYIwOiiPnDgQMLymJGRkbJ3717dJ7IXBmMicmmYW3zt2jXdz549u05xAvO4MSBYm88T2QODMRG5NMtKWwEBATq/GDCO7Ofnl7BvHlMmsgcGYyJyWZcuXZKjR4/qPlbfMlffAnRfo6UMWLca2daWtYKJ0hKDMRG5LIwFm7uoMV6MFX8KFy6sGxaJMLeMAS1olscke2EwJiKXZD23GIlaWP4SXdLmDYlcZqGhoQmtaKK0xmBMRC7p/PnzcubMGeMoeTZt2mTsEaUtBmMickkoc4lF6CtWrJiwFStWzLhXxN/fP9F92DD/mAVAyB4eu82MBTIZO3ZswlzM3r17S4UKFXSfyJUEBgbKjz/+qPtlypSRd999V/eJ0tpjjz2WKGGQLWMiIiI7YzAmIiKyMwZjIiIiO2MwJiIisjMGYyIiIjtjMCYiIrIzBmMiIiI7YzAmIiKyMwZjIiIiO2MwJiIisjMGYyIiIjtjMCYiIrIzBmMiIiI7YzAmIiKyMwZjIiIiO+N6xqS4nnHKYAF6vnWcz5EjR2TYsGG6X7p0aenTp4/uk/PAOsDmzZlZr2fMYEyKwfj+8FYJDQ2VgwcPyoEDByQkJMS4h5xFdHS0BAcH6763t7fkzJlT98l5+Pr6Sv78+aVixYpSqFAh8fLyMu5xLgzGZBOD8b3FxMTIhg0bZMGCBRIZGWmcJSJ7QTArUaKEtGjRQkqWLCnp0jnXqKt1MOaYMdF9REREyMSJE2XmzJkMxEQOAoEsMDBQGxLr1q0zzjovtoxJsWVsW1xcnEyfPj3Rmz1/3mxSt3pJKVYkt3hl8DTOElFaCAm7KnsPnpLN24/K9egbes7T01N69uwpZcuW1WNnwG5qsonB+G54a+zatUvGjx8vN2/eNL15RJ5uVEmaNawkPt4ZjEcRUVrDe/N80GX5ffpKOXMuVM/lyZNHPvzwQ/Hx8dFjR8duaqJkQrLP/PnzNRBDraolpPXTNRiIiewMgSxf7qzS7ZUmEpDZT89dvHhRduzYofvOiMGYKAmXLl3SNzhkzuQrrZ+qrvtE5BhyZs8s9WuV0X20Mvfs2aP7zojBmCgJJ0+e1PnEUKpYHg3IRORYKpQtKJ7pPXT//Pnzcv36dd13NgzGREm4cSM+OQRyZMvkdFMniNxBzhyZxdMzPhhjaOnatWu672z46UKUDBijIiLHgxkN5vcnuqotk6KcCYMxERGRnTEYExER2RmDMRERkZ0xGBNRsvw8epo0b9NLOrz6oXHm3tZt3KWPxxZ49LRxNl63Pl9J6art5KsfJhhnkuf0mYsJXzP4UphxNnmG/RL/+r8c+ptxxvlcvRopDZ/pLh1f/9g4Q66CwZiIkuVg4ElZumKzrFq73Thzb5dCLuvjsSGIWDp3PlgOHzllCqiXjTPJExl1PeFrRhulEJPrwKHj+rzd+44YZ5wLptkN+GKMrFq3XS9KyLUwGBNRmsuRPUAK5s8tAVkyGmfoXqKjY2TAkNHyy7iZxhlyNQzGRJTmJo0dJKcOLJRBH3c3zpAtmKZz8vR56dr7C/n+5ylyy0mn7dD9MRgTETkgdMP/8utf0vTZXvLnX/+Kn6+PVK/iPKsSUcowGBNRmvtj+iL57IsxsmiJ7XVol6/cIq91+1wq1n5RKtV5UTr3GCw7dh8y7k1a2OUr8uPIqaYA1lMTxBq1fEuGDp8i1yKjjEckLSg4TH4aNVVaPv+ulK3eQarUe1le7TZQFvy7NmGxEGt//vU//Tl27jkkN27EyvRZS+SlLgPkyae7SftX+svXP0xMcaKZ2clT5+WdD3+Qo8fPSOWKpWTunz/I083qGveSq2EwJqI0N+PvpfLl9xNk6YpNxpl4sbFx0n/gz9Ki/TsyZcYi2XfwmOw/dFwmT1soTVr1kJmm5yVlv+mxyDT+4NPhsnzVFjl2/KysWb9DPvp8pLTs0FcTymxBV/DipeulRsNX5f0Bw2XxkvVy5Nhp2bvvqPwxY7G0e/kDebX75xrorc2au1x/DmSOP2d63CvdPtPXuNZ0/Pe8FTrOW7fpG7Jhc8oXMEDCVoF8ueTnoe/LioVjpWGDapKOleBcFoMxEaUIgkR4+NX7bsh8TgkExZ/HTJcfR/wpcTfj5LlnG8l/C8bIwa2zZfK4wZI5k78M/na88ejE0Pp85c3PZM/+o5I9W4D8+PW7snvjDNm8crJ0e72tbNyyR+YtWm08OjEEypff+FQzlCtXLC1//vaF7Nv8l+xY96d82v8N8fLKINP++lfefv97bf3aMvib8bJyzVZ57eWW8s+MH2XJ3JHyXp9O4uPtpS3bD00XGMlpnVsqWiSfHNw2W/q89YJkyZyRJVldHIMxEaXIlSvXpHaTzvfdPhn0i/GM5Dl/IUR+GT9Lg/0L7ZvLtAlfypP1qkrxYgXklReekWXzR0vhgnmMRyc2c84y2bU3ULxNwW/G719Lv96dpGzpIlK1UhkZ+/Mn8sVnPYxHJhZlumD44rvfJPzKValWpaz8O2ekvNjhKSlVopBUKFdcvvi0hyk4fymenullzvwVsnLtNuOZiYVdjpCRP3woE0YNlFZP15cmDWvKD1++Kx+/97rev+/gcTlzNkj3kws/i4+Pt3FEro7BmIhS5KYpWB4KPHnf7cy5lAWfDZt3m1qnFyS9h4cGQbRILRUrkl/eNLVyrV2PjpF/Fqw0taxFWj9TXxqYAri17p3bSuFCeY2jO44cO6PzdmHIJ911ypW1p5rUlidqV5LomBvyx/TFNsePS5csLC+aLiCstW/dRG8jIiLlimkjSgqDMRGlSKaMftqCvN826ONuxjOSZ7UpKN66dVtKFi8oBfPnMs7egW7axg2qG0d3hIRclr0Hjul+I9P9HjaWuvT395Oa1cobR3es37RLYkxBNk+ubFKlUmnjbGJoFT/5RHyA37bzgM2gila0l5encXRH3rw59Bat/cgUdlOTe2EwJqIUyZDBU5qbWov32x4vX8J4RvIgexjQgk1qfDR37uySLWtm4yheZFR0QnJW0cL59dYaAnTZUoWNozuOHDurt0GXLkupKu0koEDDu7asBRvJt8Mm6eMuh0ckWufaDK/J1nrX6dLd+TmCk0ggIwIGYyJyCOZyFgj2SQVjJEQhmSkpeG5S8uSOb6VaMidVIVijBZzU5u/nq13YCLpMpKJHgcGYiByCOZAiYzmpBeLj4uIkOibGOIqH1ifGmSHGRqvVzFY2s/l7litTVDavmCR7Ns6457Zq8XjJni2LPofoYWIwJiKHUKJYQb3FCk8YO7Yl7PJVCQkJN47iodWaO1c23T8ceEpvraGM5BGrlaMASWFwIShEfP189OvY2tKnTyexsbGSOZOfeBiBn+hhYjAmIofwRJ1K4unpIcdPnpVdew4bZ+9Aa/l/y9abWr+J5/qi67hihfjx6bkLVkqcjWzna9eiZOnKzcbRHSgvieCK6ltLlm202SJHa/ydD3+UGg1f06ULIyNTNn+aKDkYjInIITSqX90UHMtpq7jfxz/dtewiWsw/j55uHN2BKVA9unYQrwyeOg942sx/E7WskcmMYiKoyGWtYoWSUqNqfL3nT4b8ogU6LAMy9lGaE5W2LgaFSp2aj4uvL+f+0sPHYExEDsHfz0eGfPqWVtpat2m3NHi6m0z6c4Gs3bBTxvw2Wxq26C4hYVckffq7u4mbNqwpzz/XTPff7POl9Or3rSz9b5MsWb5Rur79pXz+1ThdaMEapmkN+/Y9yZ8vl5y/cEm/50+j/pQVa7aagvBm+fjzUVrmEmU6a9eoIF1eeZYJXPRIMBgTkcNo3KCGTBw9UKc37dxzWBeIqP/Um9LTFFxjYmLl64E9bWZTIxFr+Hf9pOtrbUzBUmTsxL+ledve8tRzb8ukqQt0fvIbrz5rPDoxzD+eOelrqVq5jAQFh2p96sYte0jT1r3ku+GTtVu88ZM1ZMqvQ7TUJtGj8NhtW4MkFvbu3SuRkZGSOXNmKVOmjHGW8Gs7e/asXLt2TY+LFStm+kBIXDHImYwdO1Z27typ+71795YKFSrovjtbvny5zJo1S/dbNq0irZpX0313FXj0lJasxFSfurUqGmeThrm/+w8e1/2qlUpLRlMr1GzP/iMSFhYh+fLmlBLFChhn7zh1+oLMXbgqviiHKRiWKl5IXuzQXAuCLPx3nRbqaNe6kSZvWYqLuylrNuyQeQtXy7ETZ7Xl26h+NXnO9FhUBNu1J1AKFcijZTatYQ7xshWbZcOWPXL8xDnTp2N8gleDOlWk0ZPV9WtZ23fgqISEXpG8eXLoa7OG8et1G3bpfpnSRSRXjqy6n1qYi33S9LvBa0mqSIk76jdwskRGxZj+xjJK//79JWfOnMY9jgs9LJbh977BuEGDBrJt2zZp1KiRLFiwwDjrvJCMsXLlSqlTp474+d395kquqKgoadiwoRw9elSPJ0yYIG3atNF9Z8RgfDcGYyLn4ArB+L7d1NHR0Rp4Yqzm9jmjc+fOSdu2baVLly76Mz2IzZs3y5YtW+Ty5cumK/wwGT9+vAZ6IiKilHKrMeM9e/bIwoULdb7gg/rtt9/0tlWrVuLj4yNLliyRY8fi6+MSERGlBBO4UiE0NFS77DFG3LlzZx0vxkou5gBNRERpA1PX7jPa6hQeKBhfuXJFLl26lNCFjS7tffv2ybp162T37t16f3IhwKHlevjwYU0YS8r58+fl4MGD2uWclEOHDulj0IX8KPz1119y9epVyZ07tzzxxBPSvHn80mnTp0+X8PDE1YHIeVkW/g+PiHKJNzyRqwkOiZC4m7d0Hw0kb2/nnAf+QMG4V69eUrVqVVm2bJmsWLFCmjRpIvXr15emTZtq4lf16tW1tWi9yklgYKBmZrds2VKDat++ffWxeE7dunWlRo0aMmjQIJuB7euvv9YAiPuTgq+Dx8yZM8c4I5qQ1K1b/JJuISEh+n3wGmbPnq3nkgtd3H/88Yfu4/tkz55dXnzxRU0cuHDhgsybN0/vI+dXsOCd7NgjJy6YLjYffHiDiB6uQ0fOmWJMfL4OGkj4LHZGDxSM0So+c+aMzJ07V5599lnZunWr/jJKly6tCVJHjhyRHj16yPDhwxO1KtCSRusV23PPPSc///yzXL9+XYNwrly59PyQIUPk1VdfvSsgo9WMhCnzlCJbcD82y6Sz48ePy8WLF3UfXconT57UMV60cFMCmeVo9adPn15eeOEFPVe5cmWpUqWKdpf8/vvvTORyEfhbzpIlflGAoOArsmrDfraOiRxISFiELF212zgSKVu2rNMWZXkoY8YTJ07UYLR9+3Y5cOCATpFBQEVQRmD69ddfJTg42Hj0HQiGCOCYSoPAjSQodFUjMxlXNxiXHTBggAbPB4UgPn/+fN1HwEeLHC12jPkmFz6IZ86cqRcahQsXlsaNG+t5dGfi6+CPAD/Ppk2b9Dw5N0x9w/+x+c29cNkO2bn3hM5lJSL7wWdx6OWrMnXWWtNtfMMsR44c2sPqrB5KMPb395cZM2ZI+fLljTMiRYsWla+++kr3g4KCtKVqC8Zbf/rpJ/0agKLtmHpk7oZG8EOr1hHg50A2NqBr2svLS/ehdevWEhAQoIF66tSp2kom54YgXK9ePSlVqpQeIwiPn/qfTJ29Vo6dvCgRV6/L9egb3LhxS6Mt6nqMBIdckbWbD8mPoxfIwSPxuUOIGx07dtTiVM7qvkU/atasqfNpMQ68dOlS42w8BFKca9asmfz77793dQ8giCLTGHbs2KHduYCqXo8//rju43nmBChLSNQqWbKktmgnT56sXdaAFuikSZO0ixgJU7Z4enpqi/yXX36Rnj17GmdF/ve//8kzzzyjLWO8BlxJpQQStxCE0RJGopr5Q9qsa9euWvwjf/78sn79+kRjjo6ORT+Sdvr0ac19wMWYmYdHOskW4C8ZPNMbZ4joUUO0unI1Sq5FRhtnRIcM0YOFIU9ngnhpGX4fSjB+++23ddzXOhijaxqBDxAIn3rqKd03B2O0JJH1bH6MJbQwkYSFANGnTx/9+mCvYIwu7Q4dOmhXN1pLSFjDH4Gl1atXy5NPPqm/hxEjRmhQcxYMxveGnh0Mm2zcaHuZPSJKe0igbdeunVSsWNHp1pm2DsYPpZsawc86EFvDtCdrCIpJpaEj0JmDNBLF7A3j26tWrdJ9VN9CubWsWbMm2tBVDfgFo9v+XlO0yLng/7dTp07yzjvvSK1atfRCEm9+bs63mT+rbN3HzfE3xBv0mqL88AcffKA9rjjv9NAyvpcaNWogdN82tYyNM3c0a9ZM7+vXr59xJrGgoCC9H9vcuXONs7dv79mzR8+VLVv29pUrV4yzicXExNw2taT1cS+++KJx9vbt119/Xc+ZWsbGmbuZArk+xtQyNs7EW7x4sZ43Bfnbpla7cfb+bt26dXvIkCH6XNN/+m1/f//bGTNmtLn5+fnp49KlS3fbFLyNr+D4xowZc7tbt2664f+HyFXNnDnzdq9evW5HREQYZ4jSHuKEpYfSMk4ttByTmgaE8+akL1vd2ElBoQ/Tz2UcPRzoMjfPR0Y3N7qj165da3PD45BEgAQujDMykYvIcWBKJPI9UC8AeSxEjsKuwRjTi8xzf60hqKIaF5gTvyyhupd1MRHAWPDDDoAYM8fXReLWyy+/rNO4MEZha8NKThg3BowvWyb9EJF9IanUPOyF9zWRo7BrMEbrd+TIkTaDJ4pnIOCi8AKqZZlh7A4wn9m6IAiKfIwZM+aht4ynTJmiX7NQoUJaYexeMN0JyWUI3BEREQnVuojIvvA5g1oI5s+bU6dO6awNIkdg12AMKPCBgIxAijcJAjSmB6HsJbzxxhtSpEgR3QdkMgPeSLhv//79mhyGYhuY/oT1Z5Nap9g8LxhZ3mvWrNHvicpf94Ka2agwBij3iYuD+0HrGEVBAJnfD2OVKCJ6MLi4x+wNM3ZVkyOxezDGPOR33303oVY1unrffPNNDZRIWcdC0WhlmmFO8yuvvKLnUIADhUawhCFKaaIWdb9+/ZKcloPvhelMaOW2b99eM7mHDh1q3GsbgjvexHgsJpXfL2scMMaN8qCAymKYS01E9oXeNLyXLaG8LT5riOztvsEYJS2xGESJEiWMM3fgHO5DkQtbMD0J92Mz1/i1hrnCKKSBxAosOIFxZATm77//XluVmTJlMh4Zz9fXV+cPo9415kDnyZNHi2ug+xgJU19++aWO2WICOKqAWcLr/Oabb/Q8vi42LO5wL7t27dLXjznVKSm1hmkwtWvX1osLtNqJyH7Q44bAaw3jxydOnDCOiOznvkU/HgVz0Q9A8EVLEvN48cbImDGjBtekgrclJHDh+eh+xuRvLJ+VHBgnMidxYL4wArq7Y9EPcmV4vw8ePFi7ptHLhWEk1M8HXLyjQUDxkItjTp5Fw8i6QeQuECfMn4nW0EOKGIUN8cpczjklrIt+OEQwzps3r+6T/TAYkyvDIjTmJVVx8Z0tWzbNN8HHHy7IUTzCXYOOteXLl2tBDcAQGyohOjMsNIRpp+ilRLGe5MK6COaV+WzB8Cg2/C1hCPP9999PUW1s62Bs9zFjIqJHCR94liup4cMTiVzm4IuETvTMUTwk0qIGBDZbM12cCdauR6/HSy+9ZHN9/ORChS8Mu1pu6JnF10ReEBZFQo4Tvl9qMRgTkUvD2uXm3BC0jLAGO1pLlkNhKHH7MJZqJccSGBioSbi2alKkBLLuMcRhuaEbG3PVu3XrpgnF//33n84MSm1ns12CMV44xoaxJSc7mYgotZBAaf6ARADGBykg18Rc0xhjpA/SciL3g+ENJPdiESNMewWs7Hf16lXdTym7BGNkM2OeLzYkXhERPQqoI7B7927dR+BFIR6UtwVkUZtrGOAcxpDp3vD7Q1eseeEftDgxZWzdunX6e0blxOTCxQ9KkyKRDrNpkoKLJzwGW1LT0FAPAvdjuCGteziQEIhVDQFd1ua/r5SySzDGoHelSpV0wwocRESPAj4czTXurTNeMd3JsoYBWtDsqr63jz76SFuDKISEcXhM+UQhJgSjBg0a6H2YemodNNGlW7ZsWV1GFwWbPv30U50qiimpSBBDnQh8bfP/lSVkwuMx2JKahjZ16lS9HwlXuGAAVG5E+WJA4EeNCrwGTJl9VNDbi/Hk1OCYMRG5JHRNI8Cak5DwIY0WlBnut0zcsgzcZBtaxadPn5bFixdLixYttEWM3k1MgUIvBIIllhlFvQfLsVNc+CBpDq1o1GBAhUUESARvZLdjmOC7777TNeMt/48A/384hy2piyV8b9yPr2n+vqhDjvwAwPPw2vB//DCHI/C90BJetGiRHjdq1ChFGdWWGIyJyCXhw9k8XxYsg4OZ5Yc77rfMuqakoSWKudpIYMLvGAlOSJZCcMXvFGsLmAOhJZxbv369li7GhRAKPaGwEmr4I8t9xYoVWpHxYfRQIGkPQ6GAr41AjC5vfP2UwoI/Z8+eTbTh66EKJC4gVq5cKfny5ZPPP/881b29yZ5njCsbZCWauwDItWAeJv5/4emnn9aJ7OSesBgL/v8tu3CdERaFmDFjhnEUD59j5jE9jCFb17FHUg7mi7pzYunSpUu1+xmwXKzl4jjPP/+8ZiejEiJ+v6jQaAkZxUhmwlAknmuuWojWNBbaAXw9tKytf/cTJ07U9QYw5QzPxTAmIPAVKFBA9zHGXK5cOd239O2338rHH38sxYsX18x484JCGzZs0O5qBGP0kliuc3A/95tnbAlfF2sqYF2C5EpV0Q90E/zzzz/6i8YfMxG5LnzQ9uzZ02YJXGeCpCDrhWDQVYkPTUCt+s6dO+u+GT4g8UHu7BciDyI5wRjjxGjVmhffMUNvhDkpF61F83KylsEYF0gokmENWchoXeJ2xIgR8vbbb+t5ZwjGuPhA+WOUW8YFSHIu5qyDcbL+4vBHjepMDMRErg8tR1dYzQgJW1gYxnKznFuM8rnW9yOQuHMgTi4ENVvlhy2T5Gyt5Y5Wb7Vq1YyjxHARaA60aHU7ElRvw4WA9TZv3jxNPMNFCbrY27Ztm+oCMslqGV+8eFErjCCNHb8wXBW5czcOkSvCikYbN27UfSTk9OrVy+VmO2Bc88cff9R9/IypGT90dclpGaM7GQvzWEMWNab6ALKWX3vtNd03t4wRxPE3hvUIrKEHFl//77//lmeeeSYhKcoRWsaYtmUu4WwNrxu/M7x2tOp79Ogho0ePNu5NmnXLGAf3dfTo0dvdunXTzdQMv2365sY9ROQqLl26lPA+HzRo0G1TC9m4x3UcPnw44WccNmyYcZYsLVmyBBFCN1MwNs7G69Chg543BWPjTGLR0dEJzzUFY+Ps7dunTp3Sc8WKFbttajEbZxO7efPm7fbt2+vjTMHYOHv79pkzZxK+pikYG2cTQ1zC/aZgfDs0NNQ4e/v2+vXr9bwpGN8+fvy4cTZ5ZsyYkfB9TcHYOGtbbGzs7RYtWuhjTRcaxtl7w2MtJas/xjJpKyWFtonIeaBFYx4DxFgrrviJHib8XZkroFkzxaOE1fRstZzvJSXFRh4FzC3GMAeg1nlqpDgYc2UTIteEsVJzFyOmluDDkehhwjgyuqxtwcIUe/bs0f3KlSvrrbWkFmIwP89e8NoxBAKpnYmSrGBseYWMMSSOFxO5HgRjZIUCkjaZsEkPGy7yxowZY7N1PHnyZG3hYrzXcooQWp2obAUYb7a+SMTccMxdthe8TzB+jjFpQDGU1Eh2NrUZEriIyPXgItucScwuanpUpk2bJkOHDtVkL/ydIUBPnz5dBg4cqPdjuUOUrTRDbyzWIgZU6UJBESQboksbc5NRdCOpXhzzsAsqq6EACJKQU1M7GtPhkCRm3syvA0U+kNCM9bBxgZE7d27p37+/8ayUSXHL2NyNRURElFJYKAi1qUuWLKmtyCpVqmiJTARY1I/+4osvEk0vQwMQ5xDoUMoSmdyYooYCLdhHby3Kb9qCDG48D8H69ddf1+CM751SmPeMbG3zhulMXbp0kSFDhmjLHBcUKIDy119/JcynTqlkBWMicn1oGVvOHcU4GLkfdAkjQGKzXlwDU4NwPqlxUfwNmZ9rnl5kDV26CGRY+QmFpFCIBQVmBg8eLLNnz040F9wMxUMWLFigZTQRzBFgS5UqpWsJo5oXurUxxxcLVlhOx8NrwFQ2THlCCxs/m3lt6/tBsrL5Z0lqQ6EPFI5BeVC0vNFKTq1kzTPGxGb8wIDKKSiGTUSuBWNfo0aN0oL+gO42VKlyJZxnbB+WFbiwWARakUePHtWELgR8BPekgrc1dE9jPi8ebytw24JaGfhe6OVF1nP+/PmNe+zHep5xslrGuJrImzevbtb1RInIdaAClfm9bqvCEtHDgECE1jCWPUQN6uQGYkAwRes4uYEY0JLGuDOytB0hENuSrJYxrpjRJw7IbEOBdSJyLfgowHvdnCOC7j5XKw3JlrF9WLeM8bt3d6lqGSMAY+AbGwMxkWvChwMCsPm97mqBmMiR8d1GRESPFC70MNyJjRd5tvG3QkREjxTKW2LRCWypnfrj6pI1ZkxkDygKgKU7zfVqiR4U1ts1r0yFZLVatWrpPtHDgOlWSExDT8D9WI8ZMxiTQ8KfJZZgw3Jo/BMlImeAXAsUBMFshPtJVQIXUVpDCdZDhw4xEBOR00BvHrLFU4MtY3JImArxww8/6B83igI0adLEuIeIyLGcOXNGtm/frvt16tSR1157Tffvhd3U5BTQKkY9WMxvxzhMv379jHuIiBwLWsM///yz7qO6WN++fXX/XthNTU4BLWJzoRlWfSMiR4YpW2aouZ0aDMbkkLCCixmX7SQiR4bSseYVDVE3OzUYjMkhmVvFwGU7iciRoZCJuTplakd+GYzJIWERcDOWYCUiR4bPKPPSjZgJkhoMxuSQLLt6UrI6CxFRWrNsGVs2JFKCwZiIiMjOGIyJiIjsjPOMySFh3VkU/oCSJUtKwYIFdZ+IyNFgOtO2bdsSpjUlp0gRi34QERHZGYt+EBERORgGYyIiIjt74G5q9nITEZE7Qldzaj2UMWM8JTg4WA4ePCj79+9PVLqQiIjIlaH2QYkSJaRChQqSM2dOnWecUg8cjJEttnr1alm8eHGqC2ITERE5O9Skrl+/vjRv3lwyZcpknE2eBwrG4eHhMmXKFG0NExERkUiRIkV0DeM8efIYZ+4v1cE4NjZWJk2apHOpzPLkzyXlqpQx3eaWdB7MBSMiItd269YtuXQhRA7tDZRTR88kBFR0W/fo0SPZS76mKhjjIZs2bZI//vhDV9NB4K31ZHWpUruieHl7GY8iIiJyDzfjbsqBXYdk2fyVcvtWfBht3769FvxAoL0f62CcrOYsivYvWrQoYVm7ijUqSO2GNRiIiYjILXmk95DyVctKvWZ1jDMiK1eulKioKOMoZZIVjIOCgiQkJET3M2b2lzqNauo+ERGRu0LrtoIpIAdkj19ZLjQ0VE6dOqX7KZWsYHzs2LGE5nSh4gVNLeIMuk9EROTO0ENcpEQh40jk7Nmzxl7KJCsYx8TEGHsi2XNmTVZ/OBERkatDPMyc9c60poiICGMvZVKcAp2ayc1ERESu6mE0UBlZiYiI7IzBmIiIyM4YjImIiOyMwZjoIZo07g/p/nIv+aDXx8YZ54fiBrE3Yo0jInoUGIyJHqLAA4Gyatka2bhms3HGeWE6Y+DBI/LOm+/LYdMtET06DMZEZFPIpVBp3+wFWbdyvdbjJaJHh8GYiGxC1/QNdk8TpQkGYyIiIjtjMCZKA+jm/eXHsTJxzGQJvxwuoSGh8vvYKdL79XfkjY7d5dN+g2ThnMUSGxtnPOMOjN3OnTlfFv/zr1y7ek2CLgTLnxNnSP/en8hHb38qE0dPliOHjtrsSl67Yr0MeHegjPjuF+PM3UZ+P1ofM3/2QuOMyIolq2TJgmX6vePi4nQcHK9h7859xiOI6GFK1hKK8+bNk8WLF+t+oxb1pXLtirpPRIl98s5n8vf0fyRHrhyybu8K42x8MC6Tu6JkzRYgA78dICOG/iInjp5MtISah4eH1G/yhPw0dqj4+vkaZ+ODcelcj0tA1izy7id95NcRE+T8mfOJnuuf0V/e6vumdO3VOVE1oImjJ8l3g36UkmVKyILVc4yzibVp1F4O7jssL3XuKJ9/96lxroPp3CHdt/R691fk4y/6G0dEBDs37ZYVC9foftOmTXUpxfvB+9TyPcyWMVEaumpq2Q7oO1CuRVyTnv26y8RZv8qP476Teg3rasBeuWS1TP51qvHoxK5di5RvPhsqEeER0vO9t2TWkmky8a9x0rB5A1227Ychw+SP36YZj34wr7/1qvTo203303uml1ff7CQfDnpfGj3VUM8R0cPFYEyUhpAUlTFTRvl1+mjp82EvqdugtrRs+4wM/+0HqVy9kj5m9fK1emsNz0XreegvX8vbH/SQxytXkLpP1pFh476Xl17vqFfao38cK+dMreYH1eb5VvL8K+31a3p6ekqr9i2kS8/XpGbd6sYjiOhhYjAmSmNPt24upcuVMo7ioZu5fuMndP/i+Yt6a0vzVk2lQeN6ibqifXx9pOvbXSRT5oxy+XJ4ksGciBwXgzFRGitXsVyiYGqWK3dOvcWSpTdibui+JayY1vSZxuKR3sM4c0eevLmlRJkSIrdFtm/eYZwlImfBYEyUxrJlCzD2EjMvT4ryk1fCr+i+JT9/X8lXIK9xdLdixYvo7dHDx/SWiJwHgzGRk8DYrZe3l3F0Nz9/P71ltSwi58NgTOQkMA3iXoE2Kuq63np6ptfb5Lh58yaDN5EDYDAmchKRkVFyOfSycXS3E0dP6G3x0sX11hKCLop3WLscFi7XrkYaR0RkLwzGRE4CSV2oqGWrJXvy+Ck5uPeQJoZVq1XVOHtnHDryWqSEhYTpvqXDBwIlJDjEOCIie2EwJnIiM6fM0pKUlpV7oq9Hy6jvx0hExFXJky+PNHm6kXHPnVZy8MVLWm4TgRzPxYYKYN8MHCoxNjK3LcXGxkrElYiE5xHRw8dgTOREroRHSLeXesron8bJxrWb5X/zl0jnDt1kwd+LtFLWh5/305KbZtVrV5MixYtoEB46+Cd5pU0XDcC9Xn9H2jZ+XsIuhUn5SuWMRyeWPUc2/ZpxsXFa+Qsb6lMzIBM9fAzGRE7kte6dxNffTxd+eL1dV3m36/uyY8tOyZ03twwe+pk0f7aZ8ch4Xl4ZZPSUn6VKjcqajb1t03aZPG6qlt3MVzCvfD/mW6lUtaLWws6QIYPxrHgZTM/t0KmdPJbuMZ0uhTKdf0+bw2BM9AhwoQiihwirJ6FLOIOXp7ZKzfA227B6k+6Xe7yMZMmaRfctBV8MNj3/mKk16iGVq1XSYAh4LhaKANSiLlC4gLaEAw8EahAtX6m8NGxWX7uobRUTAWRa792xV3Zt36PHxUoWlao1K0tA1gAtnxkeFi4BphZ13vx59H6zqMgoWfTPv7J/936d/1ykRBHp/NarSX4fInf0MBaKYDAmcnDWwRj1qInIcXDVJiIiIhfAYExERGRnDMZERER2lqxgjDVUza5cjp9vSERpB1nN2HLliV/ZiYgcA+Ih4qIZZi2kRrKCceHChY09kRNHTrOWLVEaQqLHlz8N0q14qbtLXRKR/cTeiJUzx88aRyLFihUz9lImWcE4X758EhAQX0gARQL2bElcAYiIiMjdIA7uNsXD4AvxJWX9/PykUKFCup9SyQrGWbJkkTp16iTMLVy3bKMcP3xSbt1kC5mIiNwPFl9BHNzw32Y9Rnxs1KiR+Pv763FKJWueMURFRcnIkSPl+PHjeoxx5LKVS0v5qmUkU+aMeCV6noiIyFUhZEZcvir7th+Q/TsPJvQSFy9eXHr16iW+vr56fD+pKvphdurUKRk/frxcunTJOGNqWqd7TPwy+jMWExGRy0PIjLwaKbdu3QmduXLlki5duiTKr7qfBwrGEBoaKnPnzpVt27Yl+kJERETupmbNmvLss89K9uzZjTPJ88DBGLBI+aFDh2Tr1q0SGBgoYWF3r5NKRETkipDQXLp0aalevbqUKlVK0qdPb9yTfA8lGBMREVHqWQdjVuAiIiKyMwZjIiIiO2MwJiIisjMGYyIiIjtjMCYiIrIrkf8DtIK9q3uZXr4AAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBKy2GzK76sc"
      },
      "source": [
        "# Wide and Deep Network--Ist version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JouACnATIIUF"
      },
      "source": [
        "# 3.1 Design model\r\n",
        "\r\n",
        "# 3.1.1 Inputs to model. Note that inputs is NOT\r\n",
        "#       a part of layers object\r\n",
        "inputs = tf.keras.Input(shape = X.shape[1:])\r\n",
        "# 3.1.2 Add layers\r\n",
        "x = layers.Dense(100, activation = 'relu')(inputs)\r\n",
        "x = layers.Dense(100, activation = 'relu')(x)\r\n",
        "x = tf.keras.layers.concatenate([x,inputs])\r\n",
        "out = layers.Dense(1,activation = 'sigmoid')(x)\r\n",
        "# 3.1.3 Create model now\r\n",
        "model = Model(inputs = [inputs], outputs = [out])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZmSNOwJJ2cf",
        "outputId": "c2bc469a-2d6c-4879-8a5e-53d7cc4768ec"
      },
      "source": [
        "# 3.2 Print model summary\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_14 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 100)          900         input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 100)          10100       dense_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 108)          0           dense_26[0][0]                   \n",
            "                                                                 input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 1)            109         concatenate_8[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 11,109\n",
            "Trainable params: 11,109\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBZwG9ht70_A",
        "outputId": "6c600044-b10c-48f2-862f-048ac0a284ad"
      },
      "source": [
        "# 3.3 `Model` groups layers into an object \r\n",
        "#       with training and inference features.\r\n",
        "help(Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Model in module tensorflow.python.keras.engine.training:\n",
            "\n",
            "class Model(tensorflow.python.keras.engine.base_layer.Layer, tensorflow.python.keras.utils.version_utils.ModelVersionSelector)\n",
            " |  `Model` groups layers into an object with training and inference features.\n",
            " |  \n",
            " |  Arguments:\n",
            " |      inputs: The input(s) of the model: a `keras.Input` object or list of\n",
            " |          `keras.Input` objects.\n",
            " |      outputs: The output(s) of the model. See Functional API example below.\n",
            " |      name: String, the name of the model.\n",
            " |  \n",
            " |  There are two ways to instantiate a `Model`:\n",
            " |  \n",
            " |  1 - With the \"Functional API\", where you start from `Input`,\n",
            " |  you chain layer calls to specify the model's forward pass,\n",
            " |  and finally you create your model from inputs and outputs:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  inputs = tf.keras.Input(shape=(3,))\n",
            " |  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n",
            " |  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
            " |  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
            " |  ```\n",
            " |  \n",
            " |  2 - By subclassing the `Model` class: in that case, you should define your\n",
            " |  layers in `__init__` and you should implement the model's forward pass\n",
            " |  in `call`.\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super(MyModel, self).__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |  \n",
            " |    def call(self, inputs):\n",
            " |      x = self.dense1(inputs)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  If you subclass `Model`, you can optionally have\n",
            " |  a `training` argument (boolean) in `call`, which you can use to specify\n",
            " |  a different behavior in training and inference:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super(MyModel, self).__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |      self.dropout = tf.keras.layers.Dropout(0.5)\n",
            " |  \n",
            " |    def call(self, inputs, training=False):\n",
            " |      x = self.dense1(inputs)\n",
            " |      if training:\n",
            " |        x = self.dropout(x, training=training)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  Once the model is created, you can config the model with losses and metrics\n",
            " |  with `model.compile()`, train the model with `model.fit()`, or use the model\n",
            " |  to do prediction with `model.predict()`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Model\n",
            " |      tensorflow.python.keras.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
            " |      tensorflow.python.keras.utils.version_utils.ModelVersionSelector\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *args, **kwargs)\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Builds the model based on input shapes received.\n",
            " |      \n",
            " |      This is to be used for subclassed models, which do not know at instantiation\n",
            " |      time what their inputs look like.\n",
            " |      \n",
            " |      This method only exists for users who want to call `model.build()` in a\n",
            " |      standalone way (as a substitute for calling the model on real data to\n",
            " |      build it). It will never be called by the framework (and thus it will\n",
            " |      never throw unexpected errors in an unrelated workflow).\n",
            " |      \n",
            " |      Args:\n",
            " |       input_shape: Single tuple, TensorShape, or list/dict of shapes, where\n",
            " |           shapes are tuples, integers, or TensorShapes.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError:\n",
            " |          1. In case of invalid user-provided data (not of type tuple,\n",
            " |             list, TensorShape, or dict).\n",
            " |          2. If the model requires call arguments that are agnostic\n",
            " |             to the input shapes (positional or kwarg in call signature).\n",
            " |          3. If not all layers were properly built.\n",
            " |          4. If float type inputs are not supported within the layers.\n",
            " |      \n",
            " |        In each of these cases, the user should build their model by calling it\n",
            " |        on real tensor data.\n",
            " |  \n",
            " |  call(self, inputs, training=None, mask=None)\n",
            " |      Calls the model on new inputs.\n",
            " |      \n",
            " |      In this case `call` just reapplies\n",
            " |      all ops in the graph to the new inputs\n",
            " |      (e.g. build a new computational graph from the provided inputs).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: A tensor or list of tensors.\n",
            " |          training: Boolean or boolean scalar tensor, indicating whether to run\n",
            " |            the `Network` in training mode or inference mode.\n",
            " |          mask: A mask or list of masks. A mask can be\n",
            " |              either a tensor or None (no mask).\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor if there is a single output, or\n",
            " |          a list of tensors if there are more than one outputs.\n",
            " |  \n",
            " |  compile(self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, **kwargs)\n",
            " |      Configures the model for training.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          optimizer: String (name of optimizer) or optimizer instance. See\n",
            " |            `tf.keras.optimizers`.\n",
            " |          loss: String (name of objective function), objective function or\n",
            " |            `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n",
            " |            function is any callable with the signature `loss = fn(y_true,\n",
            " |            y_pred)`, where y_true = ground truth values with shape =\n",
            " |            `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse\n",
            " |            categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.\n",
            " |            y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It\n",
            " |            returns a weighted loss float tensor. If a custom `Loss` instance is\n",
            " |            used and reduction is set to NONE, return value has the shape\n",
            " |            [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values;\n",
            " |            otherwise, it is a scalar. If the model has multiple outputs, you can\n",
            " |            use a different loss on each output by passing a dictionary or a list\n",
            " |            of losses. The loss value that will be minimized by the model will\n",
            " |            then be the sum of all individual losses.\n",
            " |          metrics: List of metrics to be evaluated by the model during training\n",
            " |            and testing. Each of this can be a string (name of a built-in\n",
            " |            function), function or a `tf.keras.metrics.Metric` instance. See\n",
            " |            `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n",
            " |            function is any callable with the signature `result = fn(y_true,\n",
            " |            y_pred)`. To specify different metrics for different outputs of a\n",
            " |            multi-output model, you could also pass a dictionary, such as\n",
            " |              `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n",
            " |                You can also pass a list (len = len(outputs)) of lists of metrics\n",
            " |                such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n",
            " |                `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
            " |                strings 'accuracy' or 'acc', we convert this to one of\n",
            " |                `tf.keras.metrics.BinaryAccuracy`,\n",
            " |                `tf.keras.metrics.CategoricalAccuracy`,\n",
            " |                `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n",
            " |                function used and the model output shape. We do a similar\n",
            " |                conversion for the strings 'crossentropy' and 'ce' as well.\n",
            " |          loss_weights: Optional list or dictionary specifying scalar coefficients\n",
            " |            (Python floats) to weight the loss contributions of different model\n",
            " |            outputs. The loss value that will be minimized by the model will then\n",
            " |            be the *weighted sum* of all individual losses, weighted by the\n",
            " |            `loss_weights` coefficients.\n",
            " |              If a list, it is expected to have a 1:1 mapping to the model's\n",
            " |                outputs. If a dict, it is expected to map output names (strings)\n",
            " |                to scalar coefficients.\n",
            " |          weighted_metrics: List of metrics to be evaluated and weighted by\n",
            " |            sample_weight or class_weight during training and testing.\n",
            " |          run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
            " |            logic will not be wrapped in a `tf.function`. Recommended to leave\n",
            " |            this as `None` unless your `Model` cannot be run inside a\n",
            " |            `tf.function`.\n",
            " |          steps_per_execution: Int. Defaults to 1. The number of batches to\n",
            " |            run during each `tf.function` call. Running multiple batches\n",
            " |            inside a single `tf.function` call can greatly improve performance\n",
            " |            on TPUs or small models with a large Python overhead.\n",
            " |            At most, one full epoch will be run each\n",
            " |            execution. If a number larger than the size of the epoch is passed,\n",
            " |            the execution will be truncated to the size of the epoch.\n",
            " |            Note that if `steps_per_execution` is set to `N`,\n",
            " |            `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n",
            " |            will only be called every `N` batches\n",
            " |            (i.e. before/after each `tf.function` execution).\n",
            " |          **kwargs: Arguments supported for backwards compatibility only.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: In case of invalid arguments for\n",
            " |              `optimizer`, `loss` or `metrics`.\n",
            " |  \n",
            " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False)\n",
            " |      Returns the loss value & metrics values for the model in test mode.\n",
            " |      \n",
            " |      Computation is done in batches (see the `batch_size` arg.)\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely). If\n",
            " |            `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\n",
            " |            should not be specified (since targets will be obtained from the\n",
            " |            iterator/dataset).\n",
            " |          batch_size: Integer or `None`. Number of samples per batch of\n",
            " |            computation. If unspecified, `batch_size` will default to 32. Do not\n",
            " |            specify the `batch_size` if your data is in the form of a dataset,\n",
            " |            generators, or `keras.utils.Sequence` instances (since they generate\n",
            " |            batches).\n",
            " |          verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n",
            " |          sample_weight: Optional Numpy array of weights for the test samples,\n",
            " |            used for weighting the loss function. You can either pass a flat (1D)\n",
            " |            Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples), or in the case of\n",
            " |                temporal data, you can pass a 2D array with shape `(samples,\n",
            " |                sequence_length)`, to apply a different weight to every timestep\n",
            " |                of every sample. This argument is not supported when `x` is a\n",
            " |                dataset, instead pass sample weights as the third element of `x`.\n",
            " |          steps: Integer or `None`. Total number of steps (batches of samples)\n",
            " |            before declaring the evaluation round finished. Ignored with the\n",
            " |            default value of `None`. If x is a `tf.data` dataset and `steps` is\n",
            " |            None, 'evaluate' will run until the dataset is exhausted. This\n",
            " |            argument is not supported with array inputs.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances. List of\n",
            " |            callbacks to apply during evaluation. See\n",
            " |            [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |            input only. Maximum size for the generator queue. If unspecified,\n",
            " |            `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |            only. Maximum number of processes to spin up when using process-based\n",
            " |            threading. If unspecified, `workers` will default to 1. If 0, will\n",
            " |            execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |            threading. If unspecified, `use_multiprocessing` will default to\n",
            " |            `False`. Note that because this implementation relies on\n",
            " |            multiprocessing, you should not pass non-picklable arguments to the\n",
            " |            generator as they can't be passed easily to children processes.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.evaluate` is wrapped in `tf.function`.\n",
            " |          ValueError: in case of invalid arguments.\n",
            " |  \n",
            " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Evaluates the model on a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.evaluate` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given below.\n",
            " |          y: Target data. Like the input data `x`,\n",
            " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
            " |            tensor targets, or inversely). If `x` is a dataset, generator,\n",
            " |            or `keras.utils.Sequence` instance, `y` should\n",
            " |            not be specified (since targets will be obtained from `x`).\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per gradient update.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          epochs: Integer. Number of epochs to train the model.\n",
            " |              An epoch is an iteration over the entire `x` and `y`\n",
            " |              data provided.\n",
            " |              Note that in conjunction with `initial_epoch`,\n",
            " |              `epochs` is to be understood as \"final epoch\".\n",
            " |              The model is not trained for a number of iterations\n",
            " |              given by `epochs`, but merely until the epoch\n",
            " |              of index `epochs` is reached.\n",
            " |          verbose: 0, 1, or 2. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            " |              Note that the progress bar is not particularly useful when\n",
            " |              logged to a file, so verbose=2 is recommended when not running\n",
            " |              interactively (eg, in a production environment).\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training.\n",
            " |              See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
            " |              and `tf.keras.callbacks.History` callbacks are created automatically\n",
            " |              and need not be passed into `model.fit`.\n",
            " |              `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
            " |              `verbose` argument to `model.fit`.\n",
            " |          validation_split: Float between 0 and 1.\n",
            " |              Fraction of the training data to be used as validation data.\n",
            " |              The model will set apart this fraction of the training data,\n",
            " |              will not train on it, and will evaluate\n",
            " |              the loss and any model metrics\n",
            " |              on this data at the end of each epoch.\n",
            " |              The validation data is selected from the last samples\n",
            " |              in the `x` and `y` data provided, before shuffling. This argument is\n",
            " |              not supported when `x` is a dataset, generator or\n",
            " |             `keras.utils.Sequence` instance.\n",
            " |          validation_data: Data on which to evaluate\n",
            " |              the loss and any model metrics at the end of each epoch.\n",
            " |              The model will not be trained on this data. Thus, note the fact\n",
            " |              that the validation loss of data provided using `validation_split`\n",
            " |              or `validation_data` is not affected by regularization layers like\n",
            " |              noise and dropout.\n",
            " |              `validation_data` will override `validation_split`.\n",
            " |              `validation_data` could be:\n",
            " |                - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
            " |                - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
            " |                - dataset\n",
            " |              For the first two cases, `batch_size` must be provided.\n",
            " |              For the last case, `validation_steps` could be provided.\n",
            " |              Note that `validation_data` does not support all the data types that\n",
            " |              are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
            " |          shuffle: Boolean (whether to shuffle the training data\n",
            " |              before each epoch) or str (for 'batch'). This argument is ignored\n",
            " |              when `x` is a generator. 'batch' is a special option for dealing\n",
            " |              with the limitations of HDF5 data; it shuffles in batch-sized\n",
            " |              chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers)\n",
            " |              to a weight (float) value, used for weighting the loss function\n",
            " |              (during training only).\n",
            " |              This can be useful to tell the model to\n",
            " |              \"pay more attention\" to samples from\n",
            " |              an under-represented class.\n",
            " |          sample_weight: Optional Numpy array of weights for\n",
            " |              the training samples, used for weighting the loss function\n",
            " |              (during training only). You can either pass a flat (1D)\n",
            " |              Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples),\n",
            " |              or in the case of temporal data,\n",
            " |              you can pass a 2D array with shape\n",
            " |              `(samples, sequence_length)`,\n",
            " |              to apply a different weight to every timestep of every sample. This\n",
            " |              argument is not supported when `x` is a dataset, generator, or\n",
            " |             `keras.utils.Sequence` instance, instead provide the sample_weights\n",
            " |              as the third element of `x`.\n",
            " |          initial_epoch: Integer.\n",
            " |              Epoch at which to start training\n",
            " |              (useful for resuming a previous training run).\n",
            " |          steps_per_epoch: Integer or `None`.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              before declaring one epoch finished and starting the\n",
            " |              next epoch. When training with input tensors such as\n",
            " |              TensorFlow data tensors, the default `None` is equal to\n",
            " |              the number of samples in your dataset divided by\n",
            " |              the batch size, or 1 if that cannot be determined. If x is a\n",
            " |              `tf.data` dataset, and 'steps_per_epoch'\n",
            " |              is None, the epoch will run until the input dataset is exhausted.\n",
            " |              When passing an infinitely repeating dataset, you must specify the\n",
            " |              `steps_per_epoch` argument. This argument is not supported with\n",
            " |              array inputs.\n",
            " |          validation_steps: Only relevant if `validation_data` is provided and\n",
            " |              is a `tf.data` dataset. Total number of steps (batches of\n",
            " |              samples) to draw before stopping when performing validation\n",
            " |              at the end of every epoch. If 'validation_steps' is None, validation\n",
            " |              will run until the `validation_data` dataset is exhausted. In the\n",
            " |              case of an infinitely repeated dataset, it will run into an\n",
            " |              infinite loop. If 'validation_steps' is specified and only part of\n",
            " |              the dataset will be consumed, the evaluation will start from the\n",
            " |              beginning of the dataset at each epoch. This ensures that the same\n",
            " |              validation samples are used every time.\n",
            " |          validation_batch_size: Integer or `None`.\n",
            " |              Number of samples per validation batch.\n",
            " |              If unspecified, will default to `batch_size`.\n",
            " |              Do not specify the `validation_batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          validation_freq: Only relevant if validation data is provided. Integer\n",
            " |              or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
            " |              If an integer, specifies how many training epochs to run before a\n",
            " |              new validation run is performed, e.g. `validation_freq=2` runs\n",
            " |              validation every 2 epochs. If a Container, specifies the epochs on\n",
            " |              which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            " |              validation at the end of the 1st, 2nd, and 10th epochs.\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up\n",
            " |              when using process-based threading. If unspecified, `workers`\n",
            " |              will default to 1. If 0, will execute the generator on the main\n",
            " |              thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      Unpacking behavior for iterator-like inputs:\n",
            " |          A common pattern is to pass a tf.data.Dataset, generator, or\n",
            " |        tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            " |        yield not only features (x) but optionally targets (y) and sample weights.\n",
            " |        Keras requires that the output of such iterator-likes be unambiguous. The\n",
            " |        iterator should return a tuple of length 1, 2, or 3, where the optional\n",
            " |        second and third elements will be used for y and sample_weight\n",
            " |        respectively. Any other type provided will be wrapped in a length one\n",
            " |        tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
            " |        should still adhere to the top-level tuple structure.\n",
            " |        e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            " |        features, targets, and weights from the keys of a single dict.\n",
            " |          A notable unsupported data type is the namedtuple. The reason is that\n",
            " |        it behaves like both an ordered datatype (tuple) and a mapping\n",
            " |        datatype (dict). So given a namedtuple of the form:\n",
            " |            `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            " |        it is ambiguous whether to reverse the order of the elements when\n",
            " |        interpreting the value. Even worse is a tuple of the form:\n",
            " |            `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            " |        where it is unclear if the tuple was intended to be unpacked into x, y,\n",
            " |        and sample_weight or passed through as a single element to `x`. As a\n",
            " |        result the data processing code will simply raise a ValueError if it\n",
            " |        encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          A `History` object. Its `History.history` attribute is\n",
            " |          a record of training loss values and metrics values\n",
            " |          at successive epochs, as well as validation loss values\n",
            " |          and validation metrics values (if applicable).\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: 1. If the model was never compiled or,\n",
            " |          2. If `model.fit` is  wrapped in `tf.function`.\n",
            " |      \n",
            " |          ValueError: In case of mismatch between the provided input data\n",
            " |              and what the model expects or when the input data is empty.\n",
            " |  \n",
            " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
            " |      Fits the model on data yielded batch-by-batch by a Python generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.fit` now supports generators, so there is no longer any need to use\n",
            " |        this endpoint.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  get_layer(self, name=None, index=None)\n",
            " |      Retrieves a layer based on either its name (unique) or index.\n",
            " |      \n",
            " |      If `name` and `index` are both provided, `index` will take precedence.\n",
            " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          name: String, name of layer.\n",
            " |          index: Integer, index of layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: In case of invalid layer name or index.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Retrieves the weights of the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A flat list of Numpy arrays.\n",
            " |  \n",
            " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None)\n",
            " |      Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n",
            " |      \n",
            " |      If `by_name` is False weights are loaded based on the network's\n",
            " |      topology. This means the architecture should be the same as when the weights\n",
            " |      were saved.  Note that layers that don't have weights are not taken into\n",
            " |      account in the topological ordering, so adding or removing layers is fine as\n",
            " |      long as they don't have weights.\n",
            " |      \n",
            " |      If `by_name` is True, weights are loaded into layers only if they share the\n",
            " |      same name. This is useful for fine-tuning or transfer-learning models where\n",
            " |      some of the layers have changed.\n",
            " |      \n",
            " |      Only topological loading (`by_name=False`) is supported when loading weights\n",
            " |      from the TensorFlow format. Note that topological loading differs slightly\n",
            " |      between TensorFlow and HDF5 formats for user-defined classes inheriting from\n",
            " |      `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n",
            " |      TensorFlow format loads based on the object-local names of attributes to\n",
            " |      which layers are assigned in the `Model`'s constructor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String, path to the weights file to load. For weight files in\n",
            " |              TensorFlow format, this is the file prefix (the same as was passed\n",
            " |              to `save_weights`).\n",
            " |          by_name: Boolean, whether to load weights by name or by topological\n",
            " |              order. Only topological loading is supported for weight files in\n",
            " |              TensorFlow format.\n",
            " |          skip_mismatch: Boolean, whether to skip loading of layers where there is\n",
            " |              a mismatch in the number of weights, or a mismatch in the shape of\n",
            " |              the weight (only valid when `by_name=True`).\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for loading weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          When loading a weight file in TensorFlow format, returns the same status\n",
            " |          object as `tf.train.Checkpoint.restore`. When graph building, restore\n",
            " |          ops are run automatically as soon as the network is built (on first call\n",
            " |          for user-defined classes inheriting from `Model`, immediately if it is\n",
            " |          already built).\n",
            " |      \n",
            " |          When loading weights in HDF5 format, returns `None`.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If h5py is not available and the weight file is in HDF5\n",
            " |              format.\n",
            " |          ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n",
            " |            `False`.\n",
            " |  \n",
            " |  make_predict_function(self)\n",
            " |      Creates a function that executes one step of inference.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.predict` and `Model.predict_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.predict_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.predict` or\n",
            " |      `Model.predict_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return the outputs of the `Model`.\n",
            " |  \n",
            " |  make_test_function(self)\n",
            " |      Creates a function that executes one step of evaluation.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.evaluate` and `Model.test_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.test_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.evaluate` or\n",
            " |      `Model.test_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_test_batch_end`.\n",
            " |  \n",
            " |  make_train_function(self)\n",
            " |      Creates a function that executes one step of training.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.fit` and `Model.train_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual training\n",
            " |      logic to `Model.train_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.fit` or\n",
            " |      `Model.train_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Generates output predictions for the input samples.\n",
            " |      \n",
            " |      Computation is done in batches. This method is designed for performance in\n",
            " |      large scale inputs. For small amount of inputs that fit in one batch,\n",
            " |      directly using `__call__` is recommended for faster execution, e.g.,\n",
            " |      `model(x)`, or `model(x, training=False)` if you have layers such as\n",
            " |      `tf.keras.layers.BatchNormalization` that behaves differently during\n",
            " |      inference. Also, note the fact that test loss is not affected by\n",
            " |      regularization layers like noise and dropout.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input samples. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A `tf.data` dataset.\n",
            " |            - A generator or `keras.utils.Sequence` instance.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per batch.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of dataset, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          verbose: Verbosity mode, 0 or 1.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              before declaring the prediction round finished.\n",
            " |              Ignored with the default value of `None`. If x is a `tf.data`\n",
            " |              dataset and `steps` is None, `predict` will\n",
            " |              run until the input dataset is exhausted.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during prediction.\n",
            " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up when using\n",
            " |              process-based threading. If unspecified, `workers` will default\n",
            " |              to 1. If 0, will execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`. Note that Model.predict uses the same interpretation rules as\n",
            " |      `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n",
            " |      three methods.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of mismatch between the provided\n",
            " |              input data and the model's expectations,\n",
            " |              or in case a stateful model receives a number of samples\n",
            " |              that is not a multiple of the batch size.\n",
            " |  \n",
            " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Generates predictions for the input samples from a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.predict` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  predict_on_batch(self, x)\n",
            " |      Returns predictions for a single batch of samples.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
            " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
            " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict_on_batch` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of mismatch between given number of inputs and\n",
            " |            expectations of the model.\n",
            " |  \n",
            " |  predict_step(self, data)\n",
            " |      The logic for one inference step.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.make_predict_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of inference.\n",
            " |      This typically includes the forward pass.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_predict_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The result of one inference step, typically the output of calling the\n",
            " |        `Model` on data.\n",
            " |  \n",
            " |  reset_metrics(self)\n",
            " |      Resets the state of all the metrics in the model.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> _ = model.fit(x, y, verbose=0)\n",
            " |      >>> assert all(float(m.result()) for m in model.metrics)\n",
            " |      \n",
            " |      >>> model.reset_metrics()\n",
            " |      >>> assert all(float(m.result()) == 0 for m in model.metrics)\n",
            " |  \n",
            " |  reset_states(self)\n",
            " |  \n",
            " |  save(self, filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None, save_traces=True)\n",
            " |      Saves the model to Tensorflow SavedModel or a single HDF5 file.\n",
            " |      \n",
            " |      Please see `tf.keras.models.save_model` or the\n",
            " |      [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)\n",
            " |      for details.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String, PathLike, path to SavedModel or H5 file to save the\n",
            " |              model.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          include_optimizer: If True, save optimizer's state together.\n",
            " |          save_format: Either `'tf'` or `'h5'`, indicating whether to save the\n",
            " |              model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,\n",
            " |              and 'h5' in TF 1.X.\n",
            " |          signatures: Signatures to save with the SavedModel. Applicable to the\n",
            " |              'tf' format only. Please see the `signatures` argument in\n",
            " |              `tf.saved_model.save` for details.\n",
            " |          options: (only applies to SavedModel format)\n",
            " |              `tf.saved_model.SaveOptions` object that specifies options for\n",
            " |              saving to SavedModel.\n",
            " |          save_traces: (only applies to SavedModel format) When enabled, the\n",
            " |              SavedModel will store the function traces for each layer. This\n",
            " |              can be disabled, so that only the configs of each layer are stored.\n",
            " |              Defaults to `True`. Disabling this will decrease serialization time\n",
            " |              and reduce file size, but it requires that all custom layers/models\n",
            " |              implement a `get_config()` method.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      from keras.models import load_model\n",
            " |      \n",
            " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
            " |      del model  # deletes the existing model\n",
            " |      \n",
            " |      # returns a compiled model\n",
            " |      # identical to the previous one\n",
            " |      model = load_model('my_model.h5')\n",
            " |      ```\n",
            " |  \n",
            " |  save_weights(self, filepath, overwrite=True, save_format=None, options=None)\n",
            " |      Saves all layer weights.\n",
            " |      \n",
            " |      Either saves in HDF5 or in TensorFlow format based on the `save_format`\n",
            " |      argument.\n",
            " |      \n",
            " |      When saving in HDF5 format, the weight file has:\n",
            " |        - `layer_names` (attribute), a list of strings\n",
            " |            (ordered names of model layers).\n",
            " |        - For every layer, a `group` named `layer.name`\n",
            " |            - For every such layer group, a group attribute `weight_names`,\n",
            " |                a list of strings\n",
            " |                (ordered names of weights tensor of the layer).\n",
            " |            - For every weight in the layer, a dataset\n",
            " |                storing the weight value, named after the weight tensor.\n",
            " |      \n",
            " |      When saving in TensorFlow format, all objects referenced by the network are\n",
            " |      saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n",
            " |      instances or `Optimizer` instances assigned to object attributes. For\n",
            " |      networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n",
            " |      outputs)`, `Layer` instances used by the network are tracked/saved\n",
            " |      automatically. For user-defined classes which inherit from `tf.keras.Model`,\n",
            " |      `Layer` instances must be assigned to object attributes, typically in the\n",
            " |      constructor. See the documentation of `tf.train.Checkpoint` and\n",
            " |      `tf.keras.Model` for details.\n",
            " |      \n",
            " |      While the formats are the same, do not mix `save_weights` and\n",
            " |      `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n",
            " |      loaded using `Model.load_weights`. Checkpoints saved using\n",
            " |      `tf.train.Checkpoint.save` should be restored using the corresponding\n",
            " |      `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n",
            " |      `save_weights` for training checkpoints.\n",
            " |      \n",
            " |      The TensorFlow format matches objects and variables by starting at a root\n",
            " |      object, `self` for `save_weights`, and greedily matching attribute\n",
            " |      names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n",
            " |      is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n",
            " |      means saving a `tf.keras.Model` using `save_weights` and loading into a\n",
            " |      `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n",
            " |      the `Model`'s variables. See the [guide to training\n",
            " |      checkpoints](https://www.tensorflow.org/guide/checkpoint) for details\n",
            " |      on the TensorFlow format.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String or PathLike, path to the file to save the weights to.\n",
            " |              When saving in TensorFlow format, this is the prefix used for\n",
            " |              checkpoint files (multiple files are generated). Note that the '.h5'\n",
            " |              suffix causes weights to be saved in HDF5 format.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n",
            " |              '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n",
            " |              `None` defaults to 'tf'.\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for saving weights.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If h5py is not available when attempting to save in HDF5\n",
            " |              format.\n",
            " |          ValueError: For invalid/unknown format arguments.\n",
            " |  \n",
            " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
            " |      Prints a string summary of the network.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          line_length: Total length of printed lines\n",
            " |              (e.g. set this to adapt the display to different\n",
            " |              terminal window sizes).\n",
            " |          positions: Relative or absolute positions of log elements\n",
            " |              in each line. If not provided,\n",
            " |              defaults to `[.33, .55, .67, 1.]`.\n",
            " |          print_fn: Print function to use. Defaults to `print`.\n",
            " |              It will be called on each line of the summary.\n",
            " |              You can set it to a custom function\n",
            " |              in order to capture the string summary.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if `summary()` is called before the model is built.\n",
            " |  \n",
            " |  test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Test the model on a single batch of samples.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
            " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
            " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors, if\n",
            " |            the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of invalid user-provided arguments.\n",
            " |  \n",
            " |  test_step(self, data)\n",
            " |      The logic for one evaluation step.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.make_test_function`.\n",
            " |      \n",
            " |      This function should contain the mathematical logic for one step of\n",
            " |      evaluation.\n",
            " |      This typically includes the forward pass, loss calculation, and metrics\n",
            " |      updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_test_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned.\n",
            " |  \n",
            " |  to_json(self, **kwargs)\n",
            " |      Returns a JSON string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a JSON save file, use\n",
            " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `json.dumps()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A JSON string.\n",
            " |  \n",
            " |  to_yaml(self, **kwargs)\n",
            " |      Returns a yaml string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a yaml save file, use\n",
            " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
            " |      \n",
            " |      `custom_objects` should be a dictionary mapping\n",
            " |      the names of custom losses / layers / etc to the corresponding\n",
            " |      functions / classes.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `yaml.dump()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A YAML string.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: if yaml module is not found.\n",
            " |  \n",
            " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Runs a single gradient update on a single batch of data.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |                if the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers) to a\n",
            " |            weight (float) to apply to the model's loss for the samples from this\n",
            " |            class during training. This can be useful to tell the model to \"pay\n",
            " |            more attention\" to samples from an under-represented class.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar training loss\n",
            " |          (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If `model.train_on_batch` is wrapped in `tf.function`.\n",
            " |        ValueError: In case of invalid user-provided arguments.\n",
            " |  \n",
            " |  train_step(self, data)\n",
            " |      The logic for one training step.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.make_train_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of training.\n",
            " |      This typically includes the forward pass, loss calculation, backpropagation,\n",
            " |      and metric updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_train_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned. Example:\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_config(config, custom_objects=None) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  distribute_strategy\n",
            " |      The `tf.distribute.Strategy` this model was created under.\n",
            " |  \n",
            " |  layers\n",
            " |  \n",
            " |  metrics\n",
            " |      Returns the model's metrics added using `compile`, `add_metric` APIs.\n",
            " |      \n",
            " |      Note: Metrics passed to `compile()` are available only after a `keras.Model`\n",
            " |      has been trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.add_metric(\n",
            " |      ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc', 'mean']\n",
            " |  \n",
            " |  metrics_names\n",
            " |      Returns the model's display labels for all outputs.\n",
            " |      \n",
            " |      Note: `metrics_names` are available only after a `keras.Model` has been\n",
            " |      trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> model.metrics_names\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc']\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Non-trainable weights are *not* updated during training. They are expected\n",
            " |      to be updated manually in `call()`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of non-trainable variables.\n",
            " |  \n",
            " |  run_eagerly\n",
            " |      Settable attribute indicating whether the model should run eagerly.\n",
            " |      \n",
            " |      Running eagerly means that your model will be run step by step,\n",
            " |      like Python code. Your model might run slower, but it should become easier\n",
            " |      for you to debug it by stepping into individual layer calls.\n",
            " |      \n",
            " |      By default, we will attempt to compile your model to a static graph to\n",
            " |      deliver the best execution performance.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Boolean, whether the model should run eagerly.\n",
            " |  \n",
            " |  state_updates\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Returns the `updates` from all layers that are stateful.\n",
            " |      \n",
            " |      This is useful for separating training updates and\n",
            " |      state updates, e.g. when we need to update a layer's internal state\n",
            " |      during prediction.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of update ops.\n",
            " |  \n",
            " |  trainable_weights\n",
            " |      List of all trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Trainable weights are updated via gradient descent during training.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of trainable variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        *args: Positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
            " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_loss(self, losses, **kwargs)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be dependent\n",
            " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
            " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
            " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(self, inputs):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any loss Tensors passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      losses become part of the model's topology and are tracked in `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Activity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss references\n",
            " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
            " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
            " |      topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      d = tf.keras.layers.Dense(10)\n",
            " |      x = d(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      Arguments:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
            " |          may also be zero-argument callables which create a loss tensor.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |            inputs - Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_metric(self, value, name=None, **kwargs)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      This method can be used inside the `call()` method of a subclassed layer\n",
            " |      or model.\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
            " |        def __init__(self):\n",
            " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
            " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
            " |      \n",
            " |        def call(self, inputs):\n",
            " |          self.add_metric(self.mean(x))\n",
            " |          self.add_metric(tf.reduce_sum(x), name='metric_2')\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any tensor passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      metrics become part of the model's topology and are tracked when you\n",
            " |      save the model via `save()`.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
            " |      Functional Model, as shown in the example below, is not supported. This is\n",
            " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        name: String metric name.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |          `aggregation` - When the `value` tensor provided is not the result of\n",
            " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
            " |          using a `keras.Metric.Mean`.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Add update op(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and variance\n",
            " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
            " |      when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case, variable\n",
            " |      updates are run on the fly and thus do not need to be tracked for later\n",
            " |      execution).\n",
            " |      \n",
            " |      Arguments:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |        inputs: Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        use_resource: Whether to use `ResourceVariable`.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            " |          `AUTO` and the current `DistributionStrategy` chooses\n",
            " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
            " |          `trainable` must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The variable created.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
            " |  \n",
            " |  apply(self, inputs, *args, **kwargs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      This is an alias of `self.__call__`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      If the layer has not been built, this method will call `build` on the\n",
            " |      layer. This assumes that the layer will later be used with inputs that\n",
            " |      match the input shape provided here.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
            " |          how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves losses relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of loss tensors of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves updates relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of update ops of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from Numpy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      sets the weight values from numpy arrays. The weight values should be\n",
            " |      passed in the order they are created by the layer. Note that the layer's\n",
            " |      weights must be instantiated before calling this function by calling\n",
            " |      the layer.\n",
            " |      \n",
            " |      For example, a Dense layer returns a list of two values-- per-output\n",
            " |      weights and the bias value. These can be used to set the weights of another\n",
            " |      Dense layer:\n",
            " |      \n",
            " |      >>> a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b.set_weights(a.get_weights())\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Arguments:\n",
            " |          weights: a list of Numpy arrays. The number\n",
            " |              of arrays and their shape must match\n",
            " |              number of the dimensions of the weights\n",
            " |              of the layer (i.e. it should match the\n",
            " |              output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the provided weights list does not match the\n",
            " |              layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  compute_dtype\n",
            " |      The dtype of the layer's computations.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
            " |      the weights.\n",
            " |      \n",
            " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
            " |      computations and the output to be in the compute dtype as well. This is done\n",
            " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
            " |      these casts if implementing your own layer.\n",
            " |      \n",
            " |      Layers often perform certain internal computations in higher precision when\n",
            " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
            " |      will still typically be float16 or bfloat16 in such cases.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The layer's compute dtype.\n",
            " |  \n",
            " |  dtype\n",
            " |      The dtype of the layer weights.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
            " |      dtype of the layer's computations.\n",
            " |  \n",
            " |  dtype_policy\n",
            " |      The dtype policy associated with this layer.\n",
            " |      \n",
            " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
            " |  \n",
            " |  dynamic\n",
            " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  input_spec\n",
            " |      `InputSpec` instance(s) describing the input format for this layer.\n",
            " |      \n",
            " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
            " |      the layer to run input compatibility checks when it is called.\n",
            " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
            " |      of rank 4. As such, you can set, in `__init__()`:\n",
            " |      \n",
            " |      ```python\n",
            " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
            " |      ```\n",
            " |      \n",
            " |      Now, if you try to call the layer on an input that isn't rank 4\n",
            " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
            " |      error:\n",
            " |      \n",
            " |      ```\n",
            " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
            " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
            " |      ```\n",
            " |      \n",
            " |      Input checks that can be specified via `input_spec` include:\n",
            " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
            " |      - Shape\n",
            " |      - Rank (ndim)\n",
            " |      - Dtype\n",
            " |      \n",
            " |      For more information, see `tf.keras.layers.InputSpec`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
            " |  \n",
            " |  losses\n",
            " |      List of losses added using the `add_loss()` API.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is accessed,\n",
            " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
            " |      propagate gradients back to the corresponding variables.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
            " |      ...   def call(self, inputs):\n",
            " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |      ...     return inputs\n",
            " |      >>> l = MyLayer()\n",
            " |      >>> l(np.ones((10, 1)))\n",
            " |      >>> l.losses\n",
            " |      [1.0]\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Activity regularization.\n",
            " |      >>> len(model.losses)\n",
            " |      0\n",
            " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      >>> len(model.losses)\n",
            " |      1\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
            " |      >>> x = d(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Weight regularization.\n",
            " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      >>> model.losses\n",
            " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  name\n",
            " |      Name of the layer (string), set in the constructor.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  supports_masking\n",
            " |      Whether this layer supports computing a mask using `compute_mask`.\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variable_dtype\n",
            " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      >>> class MyModule(tf.Module):\n",
            " |      ...   @tf.Module.with_name_scope\n",
            " |      ...   def __call__(self, x):\n",
            " |      ...     if not hasattr(self, 'w'):\n",
            " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
            " |      ...     return tf.matmul(x, self.w)\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      >>> mod = MyModule()\n",
            " |      >>> mod(tf.ones([1, 2]))\n",
            " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
            " |      >>> mod.w\n",
            " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
            " |      numpy=..., dtype=float32)>\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      >>> a = tf.Module()\n",
            " |      >>> b = tf.Module()\n",
            " |      >>> c = tf.Module()\n",
            " |      >>> a.b = b\n",
            " |      >>> b.c = c\n",
            " |      >>> list(a.submodules) == [b, c]\n",
            " |      True\n",
            " |      >>> list(b.submodules) == [c]\n",
            " |      True\n",
            " |      >>> list(c.submodules) == []\n",
            " |      True\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "2ex5iFJiHkDR",
        "outputId": "0ef6bc23-e7db-47f1-e27c-bca93fa986dc"
      },
      "source": [
        "# 3.4 Display model now\r\n",
        "# Ref: https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model\r\n",
        "plot_model(model, show_shapes= True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAAIECAIAAACiyHIsAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzde0ATR/4A8ElISAgkEOQpCvJUERWtWglaamlR4QSRolS0VduKqEV8FRFBBHwVDzks1PoovZNWAaFAFbRVDy2K/GqVgvhCBBQRARESILzC/v7Y3l4OIYQQsoF8P381M8PsdwfLN7s7O0PBMAwBAAAAQ49KdgAAAABUBaQcAAAACgIpBwAAgIJAygEAAKAgNLIDUHb5+fkxMTFkRwEAGJlSU1PJDkGh4CqnH8+ePTt79izZUYBh4+bNmzdv3iQ7iiFXVVUF/18MkmqOIVzlSEXVvokAmXl7eyMV+AeTkpKybNmyEX+aQwofQ7KjUDS4ygEAAKAgkHIAAAAoCKQcAAAACgIpBwAAgIJAygEAAKAgkHIAIF92dra2tvbPP/9MdiBytm7dOsp/rFixQrzq0qVLwcHBaWlpFhYWeIOVK1eKN3BxcWGz2WpqapMmTbp9+7ZiA/+vH3/8cebMmWw228zMbPXq1TU1NXh5VlbWwYMHRSIR0TIjI4M4WT09PZLiVXaQcgAg3whe0F1XVzcnJ+fhw4cnT54kCnfv3h0XF7dz504vL68nT55YWlqOGjUqKSnp/PnzRJtffvklNTV10aJFJSUl06dPJyN2lJyc7Ovr6+3tXVVVlZmZee3atYULF3Z1dSGE3N3dmUyms7NzY2Mj3tjDw6OqquratWuurq6kRDssQMoBgHxubm5NTU2LFi0a6gMJhUIejzfURxGnoaGxYMECGxsbBoOBlxw4cODMmTMpKSlsNptoFhcXR6VS/fz8mpqaFBmeZN9+++3o0aO3b9+ura1tb2+/ZcuWwsLCgoICvHbTpk1Tp051dXXFkxCFQjExMZk7d661tTWpUSs1SDkAqJCTJ0/W1taSGMDjx49DQ0P37NnDZDLFy3k8XmBg4PPnz7dt20ZWbG969uyZsbExhULBP44dOxYhVFlZSTQIDw8vLCyMjY0lJ75hCFIOACTLy8szNTWlUChff/01QighIUFTU5PFYmVmZi5cuJDD4YwZM+b06dN447i4OCaTaWBgsG7dOmNjYyaTyePxiO/dAQEB6urqRkZG+McNGzZoampSKJT6+nqEUGBg4NatW8vKyigUipWVFULowoULHA5n7969CjvZuLg4DMPc3d3frIqKirKxsTlx4sSlS5d6/VkMw2JiYiZOnMhgMLhc7uLFix88eIBXSR40hJBIJAoLCzM1NdXQ0JgyZUpycrI00VpYWIhnaPxBjoWFBVHC5XKdnJxiY2NH8K1ROcOARPg/TbKjAMPGhx9++OGHHw70p549e4YQOnLkCP4xJCQEIXT58uWmpqba2tq5c+dqamp2dHTgtX5+fpqamvfu3WtrayspKcEfbj99+hSv9fX1NTQ0JHqOjo5GCNXV1eEfvby8LC0tidpz586x2eyIiIiBBizl/xd+fn4mJibiJRYWFra2tj2aWVpalpeXYxh248YNKpU6bty45uZmDMNycnI8PDyIZmFhYerq6qdOnWpsbCwqKpo+fbqenl5NTQ1eK3nQtm3bxmAwzp49+/r16507d1Kp1N9//73f+HNzc+l0elxcHJ/Pv3v37sSJE+fPn9+jTXBwMELozp07RMmmTZtGjRrVb+eq+bcFrnIAUFI8Ho/D4ejr6/v4+LS0tDx9+pSootFo+Jd9W1vbhIQEgUCQmJgowyHc3Nz4fH5oaKj8opakpaWlvLzc0tKyrwYODg6bN2+uqKjYsWNHjyqhUBgTE7NkyZIVK1Zoa2tPnjz56NGj9fX1x44dE2/W66C1tbUlJCR4enp6eXnp6Ojs2rWLTqdLM2JOTk5BQUEBAQEcDsfOzk4gEJw4caJHG/zJTXFxsZSDoOIg5QCg7NTV1RFCnZ2dvdbOmDGDxWIRt5iUWW1tLYZhLBZLQpuoqKjx48fHx8fn5eWJl5eUlDQ3N8+YMYMomTlzprq6OnFTsQfxQXv48GFra6udnR1epaGhYWRkJM2IhYSEHDt27PLly83NzU+ePOHxeA4ODvglKQE/nZcvX/bbG0CQcgAYARgMRl1dHdlR9K+trQ0hRExd6xWTyUxMTKRQKGvWrBEKhUQ5PhdZS0tLvLGOjo5AIOj3uC0tLQihXbt2Ee/NVFZWtra2Sv6pFy9eHDx4cO3ate+9956mpqa5ufnx48erq6vxe5UEDQ0N4tRAvyDlADC8dXZ2NjY2jhkzhuxA+of/dRZ/fbJXDg4OW7ZsKS0tjYyMJAp1dHQQQj0SjJQnrq+vjxA6fPiw+EOF/Px8yT9VWloqEolGjx5NlHA4HF1d3ZKSEvFmHR0dxKmBfkHKAWB4y83NxTBs9uzZ+EcajdbXLTjSGRgYUCgUad68iYyMnDBhwp07d4gSOzs7LS2tW7duESUFBQUdHR1vvfVWv72NHTuWyWQWFhYOKFo8mb148YIoEQgEDQ0N+FRpAn46hoaGA+pcZUHKAWD46e7ufv36dVdXV1FRUWBgoKmp6apVq/AqKyurhoaGjIyMzs7Ouro68ZdIEEK6urrV1dUVFRUCgaCzszMnJ0eRk6RZLJaFhUVVVVW/LfHba2pqauIlW7duTU9PT0pK4vP5xcXF/v7+xsbGfn5+0vS2evXq06dPJyQk8Pl8kUhUVVWF5xIfHx9DQ8NeF9QxNzefN2/e8ePHr127JhQKnz17hh/r008/FW+Gn87kyZP7DQMgpHpT9AZKNScyApnJMEn6yJEj+Js0LBbL3d09Pj4efyJtbW1dVlZ27NgxDoeDEDIzM3v06BGGYX5+fnQ63cTEhEajcTicxYsXl5WVEb29evVq3rx5TCbT3Nz8iy++2L59O0LIysoKn0V9+/ZtMzMzDQ2NOXPm1NTUZGdns9nsqKiogZ6mzJOkAwIC6HR6a2sr/jE9PR2fwKanp7dx48YeP759+3bxSdLd3d3R0dHW1tZ0Op3L5Xp6ej58+BCv6nfQ2tvbg4KCTE1NaTSavr6+l5dXSUkJhmGenp4IobCwsF7jr6+vDwwMtLKyYjAYWlpajo6OP/30U482bm5uJiYm3d3dRAlMkpZA5U54oFTznwWQmWzv5QyIn5+frq7ukB6iXzKnnNLSUhqNdurUqSELbWBEItHcuXNPnjwp24/X19czmcxDhw6JF0LKkQBurAEw/PT7BF55CIXCixcvlpaW4o/ZraysIiIiIiIimpubyQ4NiUSijIwMgUDg4+MjWw/h4eH29vYBAQEIIQzDqqur8/LyHj9+LNcwRxRIOQCAIdTQ0IAv67lmzRq8JDg42Nvb28fHh/QVPHNzc9PS0nJyciS/KtSXmJiYwsLC7OxsOp2OEMrMzMSX9RRfDxv0AClHPpR5v5Pu7u7Dhw9LXj+4ra1twoQJu3btkqbDmzdvTpw4kUqlUigUQ0PDqKgoOUXaP/HtVYyMjHpswaIKdu7cmZiY2NTUZG5ufvbsWbLD6cfRo0eJOypJSUlE+d69ewMCAvbv309ibAghZ2fnH374gViSbkAyMzPb29tzc3O5XC5esnjxYuJk8UXtwJtoZAcwQmDKuqhfaWnp6tWrr1+/PnXqVAnNQkJCHj58KGWfs2fPvn///oIFCy5evPjw4UP8hQnF8PLy8vLysrKyqq+vJzbLUin79u3bt28f2VHIgYuLi4uLC9lRyM7Dw8PDw4PsKIYfuMqRD+Xc7+TPP//csWOHv7+/vb29hGY3bty4e/euPKIbEorf4gUAMEQg5QwzA9rvZOrUqWlpab6+vhKWGBEKhdu3b1fmDT9I3+IFACAvkHLkgMT9TgYvJCRkw4YN+Iog4ga0k4qynfJvv/1ma2urra3NZDInT5588eJFhNBnn32GPwSytLTEX2tfvXo1i8XS1tbOyspCfWyp8tVXX7FYLDabXVtbu3XrVhMTE+nvQAIAelL8vOzhRcq582TtdyKlt99+e+rUqW+W5+Xlubu7YxiGLwoZEhJCVPW7k8r8+fMRQq9fv1b8KVtaWmpra0s439TU1PDw8IaGhlevXs2ePZt4ScLLy0tNTe358+dEy+XLl2dlZeH/3deWKvipbdq06ciRI0uWLLl//76EQyvgvRxloJrvlMiXao4hXOUMIQXsdzIYQqEwMDAwISGh11rZdlJRklP+8MMPd+/ezeVydXV13d3dX716hedUf39/kUhEHJfP5//++++urq5Iii1VDhw4sHHjxrS0tAkTJgxR2ACMeDBjTRGUc7+TnTt3rl271sTEZCg6V55Txt+ZwN+dfO+992xsbL777rudO3dSKJQzZ874+PjgC3nJvKXKm86ePUuhUOR3BspLRU4TyBGkHKWg+P1O8vLyiouLY2JiFHlQcUN6yufPn4+Oji4pKeHz+eJpj0KhrFu3bsuWLZcvX37//ff/9a9//fDDD3gVsaWK+MtJxsbGMhx99uzZmzdvHtwZKLv8/PzY2Fj81hCQDT6GZEehaJByyEfKficnT568fPkylfo/d1b37t27d+/e33//XXzvxaEwFKd87dq1P/74Y/PmzU+fPvX09FyyZMl33303evToI0eOfPnll0SzVatW7dy588SJE2PHjuVwOGZmZng5saVKYGDgICMZM2bM0qVLB9mJ8ouNjVWF0xxSKphy4FkO+UjZ7yQxMVH8mZ749IGhzjdoaE75jz/+0NTURAgVFxd3dnauX7/ewsKCyWT2uPnD5XKXLVuWkZFx6NChzz//nCiXbUsVAMCAQMohh7z2Oxm6COW+k8rQnXJnZ+fLly9zc3PxlGNqaooQunTpUltbW2lpKTEbm+Dv79/e3n7u3DnxV3clbKkCAJAbMqbJDSfSTGQkcb8TyYHl5+c7OjoSDySMjIx4PN7Vq1ffbPnmJGkJO6ncvHlz0qRJ+E05IyOjvXv3KuyUv/nmG3x7lV6lp6fjHQYFBenq6uro6Hh7e+MvS1laWhJzsjEMmzZtWnBwcI/z6nVLlYMHD+J7DI8dO1aaJfdhkjSQkmqOocqd8EANxT8LZdjvRMGU7ZRdXV2fPHkyFD1DygFSUs0xhBtr5BhG+53IC+mnTNyUKyoqwq+oyI0HABUEKWe4evDgAaVvMm85NYIFBQWVlpY+evRo9erVkZGRZIejEtatW0f8m+yx08SlS5eCg4PFd6NYuXKleAMXFxc2m62mpjZp0qTbt28rNvD/+vHHH/HFMszMzFavXk2sX56VlXXw4EHxL1IZGRnEyerp6ZEUr9Ij+zJL2cn94jc4OBh/TXLcuHGpqaly7FlpKckph4SEUKnUsWPHEivcDAW4sSYOv6Gak5Pz8OHDtrY2ojwsLGzRokV8Ph//aGlpOWrUKITQuXPnxH88JyfHw8NDvpEPyJkzZxBCBw8ebGxsvHPnjoWFhb29fWdnJ14bGxvr5ORErPnU3d1dVVV17do1V1dX2Ii6Lyp3wgOlmv8sgMwUkHJaW1sdHBzI7Ur6lGNiYtKjcP/+/TY2NkKhkCixtLT84YcfqFSqiYlJY2MjUU56ypk3b97o0aO7u7vxj/hUlLy8PKJBQECAg4MDkYRwmzZtgpTTF7ixBsAwI8fdHBS/McTjx49DQ0P37NnDZDLFy3k8XmBg4PPnz7dt26bIeCR79uyZsbEx8WrX2LFjEULik/jDw8MLCwtV8I1OmUHKAYAEGIbFxMTg65xyudzFixcT67kNaDcH+W4MMaAdK2QTFxeHYZi7u/ubVVFRUTY2NidOnLh06VKvPyth0CRvn4H62JmiXxYWFuIpGX+QY2FhQZRwuVwnJ6fY2FhMWfcFVjrkXmQpP9W8+AUyk/LGWlhYmLq6+qlTpxobG4uKiqZPn66np0e8aDWg3RzkuDFEvztWEGS+sWZhYWFra9ujmaWlZXl5OYZhN27coFKp48aNa25uxt64sSZ50CRvn9HXzhSS5ebm0un0uLg4Pp9/9+7diRMnzp8/v0eb4OBghNCdO3eIErixJgFc5QCgaEKhMCYmZsmSJStWrNDW1p48efLRo0fr6+uPHTsmW4fy2hhCth0rpNfS0lJeXi7hZV4HB4fNmzdXVFTs2LGjR5WUg9br9hn97kzRFycnp6CgoICAAA6HY2dnJxAITpw40aONtbU1Qqi4uFjKQVBxkHIAULSSkpLm5mbxtexmzpyprq7+5to8MiBrLwxp1NbWYhiGL1TRl6ioqPHjx8fHx+fl5YmXD3TQxLfPkHlnipCQkGPHjl2+fLm5ufnJkyc8Hs/BwQHfj5GAn87Lly/77Q0gSDkAKF5jYyNCSEtLS7xQR0dHIBDIpX/F74Uhpba2NoQQg8GQ0IbJZCYmJlIolDVr1giFQqJ8MING7ExBvDdTWVnZ2toq+adevHhx8ODBtWvXvvfee5qamubm5sePH6+ursZvThLw9ZDwUwP9gpQDgKLp6OgghHr8rZTXbg6k7IUhJfyvc7/rUDg4OGzZsqW0tFT8jd3BDBqxM4X4Q4X8/HzJP1VaWioSiUaPHk2UcDgcXV3dkpIS8WYdHR3EqYF+QcoBQNHs7Oy0tLRu3bpFlBQUFHR0dLz11lv4x8Hs5kDKXhhSMjAwoFAoTU1N/baMjIycMGHCnTt3iJJ+B00C2XamwJOZ+GriAoGgoaEBnypNwE/H0NBwQJ2rLEg5ACgak8ncunVrenp6UlISn88vLi729/c3Njb28/PDGwx0Nwd5bQwh9x0remCxWBYWFlVVVf22xG+v4XuEEyWSB01yb33tTOHj42NoaNjrgjrm5ubz5s07fvz4tWvXhELhs2fP8GN9+umn4s3w05k8eXK/YQCEVG+K3kCp5kRGIDMpJ0l3d3dHR0dbW1vT6XQul+vp6fnw4UOidkAbWMhxLwwJO1b0IPMk6YCAADqd3train9MT0/HJ7Dp6elt3Lixx49v375dfJK0hEHrd/uMXnemwDDM09MTIRQWFtZr/PX19YGBgVZWVgwGQ0tLy9HR8aeffurRxs3NzcTEhFihAINJ0hKp3AkPlGr+swAyU/waa6RsDCFzyiktLaXRaNLsPKQYIpFo7ty5J0+elO3H6+vrmUzmoUOHxAsh5UgAN9YAGPZI3xhCAqFQePHixdLSUvwxu5WVVURERERERHNzM9mhIZFIlJGRIRAIZF55PTw83N7ePiAgACGEYVh1dXVeXt7jx4/lGuaIAikHADCEGhoaFixYYGNjs2bNGrwkODjY29vbx8dHmnkEQyo3NzctLS0nJ0fyq0J9iYmJKSwszM7OptPpCKHMzEwTE5O5c+eeP39e3pGOHJByABjGdu7cmZiY2NTUZG5ufvbsWbLD6eno0aPEHZWkpCSifO/evQEBAfv37ycxNoSQs7PzDz/8QKxBNyCZmZnt7e25ublcLhcvWbx4MXGy+Cp24E00sgMAAMhu3759+/btIzsKWbi4uLi4uJAdhew8PDw8PDzIjmL4gascAAAACgIpBwAAgIJAygEAAKAgkHIAAAAoCEwfkEpKSgrZIYDhAV/+ZMT/g8HXxBzxpzmk+l1XdESiYLB/qkQpKSnLli0jOwoAwMikan+BIeUA1dLY2Pjee+8JBIKrV6+Kr0s/MrS3t/v6+ubk5KSlpS1YsIDscADoCZ7lABXC5/Pnz59fV1f366+/jrx8gxBiMBjJyckfffSRh4cHvoQXAEoFnuUAVSEUCj08PCoqKq5evTpu3Diywxkqampqx48f53K5H330UU1NzaZNm8iOCID/gpQDVEJHR4e3t3dhYeGVK1cmTJhAdjhDi0KhREdH6+npBQYGvnjx4sCBA2RHBMBfIOWAkU8kEq1cufK33367dOnStGnTyA5HQYKCgjgczsaNG6lU6jBdFAeMPJBywAiHYdjatWvPnTuXnZ09c+ZMssNRKH9/f01NzTVr1mAYRvoamgAgSDlgZMMwbP369UlJSRkZGU5OTmSHQ4KPP/6YwWCsWLGiq6srOjqa7HCAqoOUA0ayHTt2HD9+/PTp0wsXLiQ7FtLgL5atWLECw7BDhw6RHQ5QaZBywIi1e/fuQ4cOnTp1ytvbm+xYSEZkHYQQZB1AIkg5YGT6xz/+ERkZ+c033yxfvpzsWJTCsmXLurq6PvnkEzqdDs91AFkg5YAR6Lvvvtu8efPBgwf9/PzIjkWJ+Pr6dnd3r1q1isvlfvnll2SHA1QRpBww0iQlJX3++ecRERHbt28nOxals3LlSoFAsHHjRi6X+/nnn5MdDlA5kHLAiJKRkbF69eqAgIBdu3aRHYuSWr9+fW1trb+/v46ODjzlAgoGKQeMHL/++quPj8/atWsPHz5MdixKLTw8vKmpacWKFRwOZ/78+WSHA1QIrCQNRojr16/Pnz/fy8srMTGRSoX1avuBYdiaNWtSUlJ+/fVXHo9HdjhAVUDKASNBQUHBBx988P7776ekpNBocO0ulc7OTg8Pj1u3bt28edPCwoLscIBKgJQDhr2ioqJ58+bNmDEjKyuLwWCQHc5w0tLS8s4777S1td24cUNbW5vscMDIBykHDG+lpaXvvPOOra3tuXPnNDQ0yA5n+Kmurn777bcnTZp07tw5uEAEQw1ueYNh7OnTpx988IGZmVlGRgbkG9mMHj06MzPzt99+27p1K9mxgJEPUg4Yrp4/fz5v3jxtbe3s7Gw2m012OMPY9OnT//Wvfx05cuSbb74hOxYwwqmFh4eTHQMAA1ZXV+fs7IwQunLlir6+PtnhDHu2trYIoeDgYB6PB1MJwNCBZzlg+GlqanJ2dq6rq/vtt99MTU3JDmeEwDDM19f3woULt2/fHsEbdQNyQcoBw0xra+v8+fPLy8t/++03c3NzssMZUYRC4dtvv62hofHbb7+pq6uTHQ4YgeBZDhhOhELh3/72t4cPH/7666+Qb+ROQ0MjLS3twYMHO3bsIDsWMDJBygHDRmdn59KlS2/fvn3hwoWJEyeSHc7IZG1tfezYsdjY2PT0dLJjASMQ3FgDw4NIJPL19T137tyFCxfmzJlDdjgj3Nq1a1NTU2/fvg2XkkC+IOWAYQDDsM8///zHH3/Mzs5+9913yQ5n5Gtra+PxeDQaLS8vDx7qADmCG2tA2WEYtnHjxlOnTqWmpkK+UQwmk3n69OkHDx4EBweTHQsYUSDlAGUXHBz87bffnjp1ys3NjexYVMj48ePj4uJiY2OvX79Odixg5IAba0CpRUREhIeHHz9+/NNPPyU7FlW0ePHi+/fvFxYWwnpCQC7gKgcor7i4uPDw8Pj4eMg3ZElISKirq9u9ezfZgYARAlIOUFLff/99YGDggQMH/P39yY5FdY0ePfqrr776+9//DrfXgFzAjTWgjM6ePevj4xMaGgrfr0mHYZirq2tFRcWdO3eYTCbZ4YDhDa5ygNLJzMz86KOPNm7cCPlGGVAolG+//fb58+eRkZFkxwKGPUg5gDStra2nT5/uUXjp0iUfH5+PP/748OHDpEQF3mRqanrgwIHo6OiHDx+SHQsY3iDlANL861//8vX1PXLkCFFy48YNT0/PDz/88Pjx4xQKhcTYQA/r1q2bMmXKtm3byA4EDG/wLAeQo7u728rKqry8HCG0b9++4ODgwsLCefPmvfvuu6mpqbAjshL697///d577128eNHFxYXsWMBwBSkHkOOnn35asmQJ/t8UCuWzzz5LS0ubNWtWZmYmrLCitDw8PB4/fvznn3/CdwIgG0g5gByzZ8++deuWSCTCP1Kp1Lfeeuvf//63pqYmuYEBCcrKyiZNmhQXF7d27VqyYwHDEjzLAST4/fffCwoKiHyDEOru7v7jjz/Wrl3b1dVFYmBAMktLS39//9DQ0KamJrJjAcMSpBxAggMHDtDp9B6F3d3dycnJH330UWdnJylRAWmEhYWJRKL9+/eTHQgYluDGGlC08vJyKyur7u7uvhosWrQoNTWVwWAoMiogvcOHD+/atau8vNzAwIDsWMAwA1c5QNFiY2PV1NR6raJSqYaGhm5ublQq/MtUXv7+/tra2jExMWQHAoYfuMoBCvX69WsTExOhUNijnEajsdnsoKCgTZs2waoqyi86OnrPnj3l5eX6+vpkxwKGE/guCRTqm2++6fGoRk1NTVtbOyoqqqqqKigoCPLNsLB+/XoWiyX+Gi8A0oCrHKA4HR0dJiYm9fX1+EcajaahobFly5YtW7ZwOBxyYwMDtXv37m+++aayshK20gHSg6scoDg//vgjnm9oNBqLxQoJCXn27Fl4eDjkm+Fow4YNAoHg1KlTZAcChhO4ygEKgmGYra3tgwcPmEzm5s2bt23bpqurS3ZQYFDWrFlTUFBw9+5dWBAPSAsTk5ycTHY4AABlkZycjEl0584dhNCVK1ckNwOA0MtCSZB4hrv8/PzY2Fhl+z0WFBTY2NhwuVw59rls2bLAwEAHBwc59glwy5Yt67eNvb3922+//e23386bN08BIYERoJeUs3TpUsXHAeQrNjZW2X6PQxHPsmXLHBwclO1MRwZpUg5CyM/Pb926dbW1tfBaKJAGTB8AAMjOx8dHU1MzMTGR7EDA8AApBwAgOw0NjRUrVhw7dkzCCkYAECDlAAAGxd/fv7y8/NKlS2QHAoYBSDkAgEGZOHGio6Pjt99+S3YgYBiAlAMAGCw/P7+srKzq6mqyAwHKDlIO+Et2dra2tvbPP/9MdiDkuHTpUnBwcFpamoWFBYVCoVAoK1euFG/g4uLCZrPV1NQmTZp0+/ZtsuL88ccfZ86cyWazzczMVq9eXVNTg5dnZWUdPHhQfNc7RfL29uZyud999x0pRwfDCKQc8BdMhdeh2L17d1xc3M6dO728vJ48eWJpaTlq1KikpKTz588TbX755ZfU1NRFixaVlJRMnz6dlDiTk5N9fX29vb2rqqoyMzOvXbu2cOFCfB9Vd3d3JpPp7Ozc2Nio+MAYDMbHH3987NgxsnIeGC4g5YC/uLm5NTU1LVq0aKgPJBQKeTzeUB9FegcOHDhz5kxKSgqbzSYK4+LiqFSqn5+fUu24/O233xWlrMwAACAASURBVI4ePXr79u3a2tr29vZbtmwpLCwsKCjAazdt2jR16lRXV1dSNvP29/evqqq6cOGC4g8NhhFIOUDRTp48WVtbS3YUf3n8+HFoaOiePXt6bJrA4/ECAwOfP3++bds2smJ707Nnz4yNjYkFzcaOHYsQqqysJBqEh4cXFhbGxsYqPjZLS8t58+bBJAIgGaQcgBBCeXl5pqamFArl66+/RgglJCRoamqyWKzMzMyFCxdyOJwxY8acPn0abxwXF8dkMg0MDNatW2dsbMxkMnk8HvFdOyAgQF1d3cjICP+4YcMGTU1NCoWCryEdGBi4devWsrIyCoViZWWFELpw4QKHw9m7dy8Jp41QXFwchmHu7u5vVkVFRdnY2Jw4caKv6b8YhsXExEycOJHBYHC53MWLFz948ACvkjyACCGRSBQWFmZqaqqhoTFlyhQpVyeysLAQz9b4gxwLCwuihMvlOjk5xcbGknKbdN26ddnZ2c+ePVP8ocGwIb7gGv7vnoyl3oA8yfZ7xP9SHDlyBP8YEhKCELp8+XJTU1Ntbe3cuXM1NTU7OjrwWj8/P01NzXv37rW1tZWUlOAPtJ8+fYrX+vr6GhoaEj1HR0cjhOrq6vCPXl5elpaWRO25c+fYbHZERIQMZ4qkWHpSMgsLC1tb2x6FlpaW5eXlGIbduHGDSqWOGzeuubkZw7CcnBwPDw+iWVhYmLq6+qlTpxobG4uKiqZPn66np1dTU4PXSh7Abdu2MRiMs2fPvn79eufOnVQq9ffff+832tzcXDqdHhcXx+fz7969O3HixPnz5/doExwcjBC6c+eOjCPyHzKMbXt7u56eXlRU1CAPDUYwuMoBkvB4PA6Ho6+v7+Pj09LS8vTpU6KKRqPhX/BtbW0TEhIEAoFsq564ubnx+fzQ0FD5RS2tlpaW8vJyS0vLvho4ODhs3ry5oqJix44dPaqEQmFMTMySJUtWrFihra09efLko0eP1tfXHzt2TLxZrwPY1taWkJDg6enp5eWlo6Oza9cuOp0uzeg5OTkFBQUFBARwOBw7OzuBQHDixIkebaytrRFCxcXFUg6CHKmrqy9fvvyf//wnpsJTUYBkkHKAVNTV1RFCPfaQJsyYMYPFYhG3lYaL2tpaDMNYLJaENlFRUePHj4+Pj8/LyxMvLykpaW5unjFjBlEyc+ZMdXV14gZjD+ID+PDhw9bWVjs7O7xKQ0PDyMhImtELCQk5duzY5cuXm5ubnzx5wuPxHBwcetzIwk/n5cuX/fY2FD755JPS0tL8/HxSjg6UH6QcIB8MBqOuro7sKAamra0NIcRgMCS0YTKZiYmJFAplzZo1QqGQKMfnImtpaYk31tHREQgE/R63paUFIbRr1y7Kf1RWVra2tkr+qRcvXhw8eHDt2rXvvfeepqamubn58ePHq6ur8fuWBHxbaPzUFG/69OlTpkz55z//ScrRgfKDlAPkoLOzs7GxccyYMWQHMjD4X+d+XyVxcHDYsmVLaWlpZGQkUaijo4MQ6pFgpBwEfX19hNDhw4fF73H3e2VQWloqEolGjx5NlHA4HF1d3ZKSEvFmHR0dxKmR4uOPPz5z5ky/GRSoJkg5QA5yc3MxDJs9ezb+kUaj9XULTqkYGBhQKBRp3ryJjIycMGECvgkmzs7OTktL69atW0RJQUFBR0fHW2+91W9vY8eOZTKZhYWFA4oWT2YvXrwgSgQCQUNDAz5VmoCfjqGh4YA6l6OVK1cKhcKMjAyyAgDKDFIOkFF3d/fr16+7urqKiooCAwNNTU1XrVqFV1lZWTU0NGRkZHR2dtbV1Ym/OIIQ0tXVra6urqioEAgEnZ2dOTk5ZE2SZrFYFhYWVVVV/bbEb6+pqamJl2zdujU9PT0pKYnP5xcXF/v7+xsbG/v5+UnT2+rVq0+fPp2QkMDn80UiUVVVFZ5LfHx8DA0Ne11Qx9zcfN68ecePH7927ZpQKHz27Bl+rE8//VS8GX46kydP7jeMIWJgYLBgwQK4twZ6J35pD5OkRwYZfo9HjhzB36RhsVju7u7x8fH4U2hra+uysrJjx45xOByEkJmZ2aNHjzAM8/Pzo9PpJiYmNBqNw+EsXry4rKyM6O3Vq1fz5s1jMpnm5uZffPHF9u3bEUJWVlb4LOrbt2+bmZlpaGjMmTOnpqYmOzubzWbLNrMWDXqSdEBAAJ1Ob21txT+mp6fjE9j09PQ2btzYo/H27dvFJ0l3d3dHR0dbW1vT6XQul+vp6fnw4UO8qt8BbG9vDwoKMjU1pdFo+vr6Xl5eJSUlGIZ5enoihMLCwnqNtr6+PjAw0MrKisFgaGlpOTo6/vTTTz3auLm5mZiYdHd3D2ZYsMGN7dmzZ6lUKjFpHgACpJwRSAG/Rz8/P11d3SE9hDQGn3JKS0tpNNqpU6fkFdIgiUSiuXPnnjx5UrYfr6+vZzKZhw4dGnwkgxlb/AWdffv2DT4MMMLAjTUgo5GxgKOVlVVERERERERzczPZsSCRSJSRkSEQCHx8fGTrITw83N7ePiAgQL6BDZS6urqPj8/333+PwQs64H8NNuV89tlnbDabQqEM9Fno0ImIiLC1teVwOAwGw8rK6ssvvxT/axIVFUX5X8TrEZKJL2uPU1dXNzAwePfdd6Ojo1+/fj1kJwSGVnBwsLe3t4+PD+kreObm5qalpeXk5Eh+VagvMTExhYWF2dnZdDpd7rEN1CeffPLo0aObN2+SHQhQLoNNOSdOnDh+/LhcQpGXK1eubNy4saKior6+ft++fbGxsd7e3oPvlljWXltbG8Ow7u7u2tralJQUc3PzoKCgSZMmiU9eGtl27tyZmJjY1NRkbm5+9uxZssORg7179wYEBOzfv5/cMJydnX/44QdieboByczMbG9vz83N5XK5cg9MBjNmzLCzs4NJBKAn8btssj0DwBcrHPyaTvLi5ubW1dVFfFy6dClCiHiSGRkZOZgb90TKEZeamkqlUg0MDBobG2XuWY5U55kcGvSzHNCXwY/tgQMHRo0aRSwrBwAml2c5xFLqSuLcuXPik1n19PQQQkP6YtqHH364atWq2trao0ePDt1RABheli5d2tDQcOXKFbIDAUpElpSDYVh0dPT48eMZDIa2tjY+BZbQ66rs/a7lfvXq1VmzZrFYLA6HM3nyZD6f31dXA/X8+XMNDQ1zc3NpGsu8kD7+SkpOTg7+UdkGAQDFMzc3nzFjRmpqKtmBAGUifskj5Q2ZkJAQCoXy97///fXr162trfHx8Ujsxlpfq7JLWMu9ubmZw+EcPHhQKBTW1NQsWbIEX+hetgXexbW0tLDZ7ICAAKIkMjJyzJgxOjo6dDp93LhxHh4e//d//0fU9ruQfq831jAMw9PD2LFjlWEQ4MYaGDy5jO1XX33F5XLb29vlEhIYAQacclpbW1ks1gcffECUiD/LEQqFLBbLx8eHaMxgMNavX4/956+tUCjEq/BE9fjxYwzD7t69ixA6d+6c+IEkdCW9kJAQGxsbPp9PlDx9+vT27dsCgaC9vT0/P3/atGkaGhp3796VssO+Ug6GYRQKRUdHR3LkihkESDlg8OQytk+fPqVQKNnZ2XIJCYwAtIFeFT1+/Li1tdXZ2bnXWulXZRdfy93CwsLAwGDFihWbNm1atWrVuHHjBtRVX9LT01NSUn755RfxPe3Hjh1LrEk1e/bsxMREe3v7+Pj4hIQE6Xt+U0tLC4Zh+BvmSjIIKSkpgzmj4QLWyVdmY8eOnTVrVmpq6sKFC8mOBSgH8fwjzbfj7OxshJD429HiVznXr19/8xCzZ8/G3viCj0+tvn//Pv7x7t27f/vb32g0GoVCWbZsWWtrq4SupHH69OmZM2c+f/5ccjORSKSmpubs7Cxlt31d5eCLYrm4uCjDIMDzHiAXcrmCPHTokI6ODtxbA7gBTx9gMpkIofb29l5rZVuVHSE0adKkn3/+ubq6OigoKDk5+dChQzJ3hRA6cuRIUlLSlStXxFd671V3d3d3d7fkHVOkceHCBYQQ/lVOSQZh6P7RKA8EN9aGzGD+dxC3dOnSpqamX3/9VV4dgmFtwCnHzs6OSqVevXq111rZVmWvrq6+d+8eQkhfX3///v3Tp0+/d++ebF1hGBYUFFRcXJyRkdFj+yzc/PnzxT/ij+IdHBwGdJQeampqDh8+PGbMmDVr1iAlGAQAlMfYsWNnz54N89YAbsApB1/19uzZsydPnuTz+UVFReKbvUtYlV2C6urqdevWPXjwoKOj486dO5WVlbNnz5atq3v37n311VfHjx+n0+nii9McOnQIb/D8+fMzZ840NjZ2dnbm5+d/9tlnpqam/v7+eK00C+ljGNbc3Iyv1FtXV5ecnOzo6KimppaRkYE/yyF9EABQKt7e3j/99BNZG5UC5SJ+KS3lTCeBQPDZZ5+NGjVKS0trzpw5YWFhCKExY8b8+eefWB+rsktey72iooLH43G5XDU1tdGjR4eEhODLB/S1wLsExcXFvZ5mdHQ03mDr1q2Wlpaampo0Gm3MmDGff/55dXU18eMSFtLPysqaMmUKi8VSV1enUqkIIXyK2qxZsyIiIl69eiXemNxBgBlrYPDkOLbPnj2jUqlZWVly6Q0MaxRM7KZtSkrKsmXLMFj8dZhTnd8jhUJJTk7G1zQC8iXfseXxeBMmTPjuu+/k0hsYvmDzAgDAkHNzc8vJyVGFr0FAsmGWch48eEDpm8y7jAAAhpSrq2tNTc2dO3fIDgSQbJilnAkTJki4S3jmzBmyAwTK69KlS8HBweL7Hq1cuVK8gYuLC5vNVlNTmzRpEv6iFVm6u7sPHz7M4/HerMrLy3N0dGSxWMbGxkFBQT1eV+irNisr6+DBgyTuqmdvb29iYnL+/HmyAgBKYpilHABks3v37ri4uJ07dxL7Ho0aNSopKUn8j+Avv/ySmpq6aNGikpKS6dOnkxVqaWnpO++8s2XLljeXPy8pKXFxcXF2dq6rq0tPT//uu++IyZaSa93d3ZlMprOzc2Njo+LORAyFQlmwYAGx7i1QWZBygCyEQmGv38HJ7aovBw4cOHPmTEpKivjSR3FxcVQq1c/Pj/TNQMX9+eefO3bs8Pf3t7e3f7M2MjLSyMhoz549mpqaDg4OQUFB33//PbEAkuTaTZs2TZ061dXVtaurS3HnI8bV1bWgoKC2tpaUowMlASkHyOLkyZPy+tshx6569fjx49DQ0D179uALZxB4PF5gYODz58+3bds2dEcfqKlTp6alpfn6+r65IkZXV9f58+ednJyIHaoWLlyIYVhmZma/tbjw8PDCwsLY2FiFnEpPLi4udDr9l19+IeXoQElAylFdGIbFxMRMnDiRwWBwudzFixcT34gDAgLU1dWJHZE3bNigqalJoVDq6+sRQoGBgVu3bi0rK6NQKFZWVnFxcUwm08DAYN26dcbGxkwmk8fjFRQUyNAVGsSWRX2Ji4vDMMzd3f3NqqioKBsbmxMnTly6dGmgQ9Tv7kdy3+joyZMnzc3NpqamRImlpSVCqKioqN9aHJfLdXJyio2NJWXmmJaWlqOjI9xbU3Xij99V5xXCkU3K32NYWJi6uvqpU6caGxuLioqmT5+up6dXU1OD1/r6+hoaGhKNo6OjEUL4Fj4Yhnl5eVlaWhK1fn5+mpqa9+7da2trKykpmTlzJpvNJjb/HlBX/W5ZJA5J8bqihYWFra1tj0JLS8vy8nIMw27cuEGlUseNG9fc3IxhWE5OjoeHB9FM8hBJ2P0IG/RuT2+//fbUqVPFS/BVpoiXmnEaGhr4orSSawnBwcFIum3jpRnbgTp06JCurm5nZ6d8uwXDCFzlqCihUBgTE7NkyZIVK1Zoa2tPnjz56NGj9fX14ssXDQiNRsOvBmxtbRMSEgQCQWJiogz9uLm58fn80NBQ2cLooaWlpby8HP++3ysHB4fNmzdXVFTs2LGjR5WUQ8Tj8Tgcjr6+vo+PT0tLy9OnTxFCbW1tCQkJnp6eXl5eOjo6u3btotPpsg0IAZ9+Jr7JOkKITqcLhcJ+awnW1tYIob4W6Rhqrq6uDQ0NxBUwUEGQclRUSUlJc3PzjBkziJKZM2eqq6vL5c/BjBkzWCzWgDY3GiK1tbUYhuHrDPUlKipq/Pjx8fHxeXl54uUDHSLx3Y8Gv9vTm/BnUT0e/nd0dGhoaPRbS8CH4uXLl4OJRGYTJ060sLDAN0ABqglSjorCJ8v2WGxbR0dHIBDIpX8Gg1FXVyeXrgYDX0pS8uYUTCYzMTGRQqGsWbNG/JpgMEPU0tKCENq1axfxnnJlZeWbk54HBH8ehm95jmttbW1razM2Nu63loBnIBJX2HRxcbly5QpZRwekg5SjonR0dBBCPf56NjY2jhkzZvCdd3Z2yqurQcL/wvb7CqSDg8OWLVtKS0sjIyOJwsEM0WA2OuqLubk5m82urKwkSh4/fowQmjJlSr+1hI6ODvSfYSHFnDlz/vjjj0FmXzB8QcpRUXZ2dlpaWrdu3SJKCgoKOjo63nrrLfwjjUbD7xHJIDc3F8Ow2bNnD76rQTIwMKBQKNK8eRMZGTlhwgTxFVn6HSIJhmKjIxqN5urqeu3ate7ubrwkJyeHQqHgk/Ek1xLwoTA0NJRjYAMyZ86czs7O//u//yMrAEAuSDkqislkbt26NT09PSkpic/nFxcX+/v7Gxsb+/n54Q2srKwaGhoyMjI6Ozvr6urEvz4jhHR1daurqysqKgQCAZ5Ouru7X79+3dXVVVRUFBgYaGpqumrVKhm6kmbLIumxWCwLC4uqqippBiQxMVH88Xu/QyS5t742OvLx8TE0NJRtQZ3Q0NCXL1/u3r27paUlPz8/Ojp61apV48ePl6YWhw/F5MmTZTi6XJiZmZmamvZ4bAZUiPiFP0ySHhmk/D12d3dHR0dbW1vT6XQul+vp6fnw4UOi9tWrV/PmzWMymebm5l988cX27dsRQlZWVvjU59u3b5uZmWloaMyZM6empsbPz49Op5uYmNBoNA6Hs3jx4rKyMtm6krBl0ZuQFBN5AwIC6HR6a2sr/jE9PR2fwKanp7dx48Yejbdv3y4+SVrCEEne/Qjre6MjT09PhFBYWFiv0ebn5zs6OhIPYIyMjHg83tWrV4kGV69enTVrFoPBMDY23r59e1tbm/iPS67FMMzNzc3ExATfYFAyacZWNsuXL58/f/5Q9AyUH6ScEUjxv0c/Pz9dXV1FHhEnzZ/F0tJSGo126tQpxYTUL5FINHfu3JMnTyr+0PX19Uwm89ChQ9I0HrqUk5CQwOFw8B0IgaqBG2tAPkhcpVgyKyuriIiIiIiI5uZmsmNBIpEoIyNDIBCQstFGeHi4vb19QECA4g8tbs6cOfge9uSGAUgBKQeMfMHBwd7e3j4+PqSv4Jmbm5uWlpaTkyP5VaGhEBMTU1hYmJ2dTafTFXzoHuzs7HR1dX/77TdywwCkgJQDBmvnzp2JiYlNTU3m5uZnz54lO5ze7d27NyAgYP/+/eSG4ezs/MMPPxArzilMZmZme3t7bm4ul8tV8KHfRKFQeDze9evXyQ4EkIBGdgBg2Nu3b9++ffvIjqJ/Li4uLi4uZEdBDg8PDw8PD7Kj+C9HR8d//OMfZEcBSABXOQAARcNnJ1ZUVJAdCFA0SDkAAEWbNm0alUoVf/EWqAhIOQAARdPU1LSyspLv6gxgWICUAwAgwbRp0+AqRwX1Mn3A29tb8XEAOcIXNVGR3+Phw4dTU1PJjgIMmL29fXx8PNlRAEWjYGJb0ubn58fExJAYDVAG+HfPadOmkR0IINmWLVscHByGqPMLFy4sXLjw1atXurq6Q3QIoIT+J+UAgBBaunQpQiglJYXsQMBIVllZOW7cuOvXr/N4PLJjAYoDz3IAACQwNTXV0tK6f/8+2YEAhYKUAwAgAYVCsbGxUYbdyoEiQcoBAJBjwoQJcJWjaiDlAADIMWHChEePHpEdBVAoSDkAAHKYmZk9ffqU2DkbqAJIOQAAcowbN669vb2mpobsQIDiQMoBAJDDzMwMIVRZWUl2IEBxIOUAAMgxZswYOp0O60mrFEg5AAByqKmpGRsbP3v2jOxAgOJAygEAkMbIyOjly5dkRwEUB1IOAIA0hoaGkHJUCqQcAABpIOWoGkg5AADSwI01VQMpBwBAmlGjRjU0NJAdBVAcSDkAANKw2WyBQEB2FEBxIOUAAEjD4XAEAgGseaM6IOUAAEjDZrMxDIMLHdUBKQcAQBptbW2EEJ/PJzsQoCCQcgAApGEymQih9vZ2sgMBCgIpBwBAGjqdjhDq7OwkOxCgIJByAACkodFoCFKOKoGUAwAgDVzlqBpIOQAA0kDKUTWQcgAApBGJROg/t9eAKoCUAwAgDX59g1/rAFUAKQcAQBo85cBVjuqAlAMAIA1c5agaSDkAANJ0dHQgSDmqBFIOAIA0+OpqHA6H7ECAgkDKAQCQBl9djc1mkx0IUBBIOQAA0vD5fA0NDXV1dbIDAQoCKQcAQBo+nw931VQKzE0EqLW1VXwpX/yJ7uvXr4kSBoPBYrFIiAyMdE1NTTo6OmRHARQHUg5A33///YYNG3oU6urqEv8dHx+/fv16xQYFVEJNTY2hoSHZUQDFgRtrAHl7e6upqfVVq6am5u3trch4gOqAlKNqIOUApK+v7+zs3GvWUVNTe//99/X19RUfFVAFL1++hJSjUiDlAIQQWrFiBYZhb5ZjGLZixQrFxwNUBKQcVQMpByCE0OLFi3t9A5xGo7m7uys+HqAiqqurjY2NyY4CKA6kHIAQQmw2e9GiRT2yDo1G8/DwgDmsYIjU1dW1tLSMGzeO7ECA4kDKAX/x9fXt6uoSLxGJRL6+vmTFA0a8yspKhJCZmRnZgQDFgZQD/uLq6qqlpSVeoqmpuWDBArLiASNeRUUFlUodO3Ys2YEAxYGUA/6irq7u7e1NLD1Cp9OXLVvGYDDIjQqMYE+fPjU2NoZ/YyoFUg74r+XLl+NLDyCEOjs7ly9fTm48YGQrLS21tLQkOwqgUJBywH/NmzePeAVHT0/PycmJ3HjAyHb//v2JEyeSHQVQKEg54L+oVOry5cvV1dXpdLqvr6+EJQkAGDxIOSoIUg74Hx999FFHRwfcVQNDraGhoba2dsKECWQHAhTqf5b1rKqqunHjBlmhAGWAYdioUaMQQuXl5RUVFWSHA8jE4/HGjBkzRJ3fv38fIQQpR+VgYpKTk8kOBwCgLJKTk7Ehk5CQwOFwuru7h+4QQAn1snkB1ttaW2AYSUlJWbZsmcy/x3v37iGEbG1t5RrUkKBQKMnJyUuXLiU7kBGIQqEMaf937tyZNm3aUB8FKBvYLwf0NCySDRjuCgsLHR0dyY4CKBpMHwAAKJpIJCopKbG3tyc7EKBokHIAAIr24MGD1tZWSDkqCFIOAEDR8vPzNTU14RauCoKUAwBQtLy8vNmzZ/e6RRMY2SDlAAAULS8vb86cOWRHAUgAKQf8JTs7W1tb++effyY7kKFy6dKl4ODgtLQ0CwsLCoVCoVBWrlwp3sDFxYXNZqupqU2aNOn27dtkxYkQ6u7uPnz4MI/He7MqLy/P0dGRxWIZGxsHBQW1t7dLU5uVlXXw4EGRSKSI6PtTU1NTVlYG09VUE6Qc8JeR/T7W7t274+Lidu7c6eXl9eTJE0tLy1GjRiUlJZ0/f55o88svv6Smpi5atKikpGT69OlkhVpaWvrOO+9s2bKltbW1R1VJSYmLi4uzs3NdXV16evp3333n7+8vTa27uzuTyXR2dm5sbFTcmfTh2rVrNBpt9uzZZAcCyCD+Xii++gApr6QCOVLy32Nra6uDg4NcukLSvSG/f/9+GxsboVBIlFhaWv7www9UKtXExKSxsZEoz8nJ8fDwkEtssiksLFyyZElSUpK9vf3UqVN71C5btszc3Jx4Yz86OppCody/f1+aWgzDAgICHBwcOjs7pYlEyrGVwRdffDFjxoyh6BkoP7jKAYp28uTJ2tpahR3u8ePHoaGhe/bsYTKZ4uU8Hi8wMPD58+fbtm1TWDD9mjp1alpamq+v75sbl3V1dZ0/f97JyYl4Y3/hwoUYhmVmZvZbiwsPDy8sLIyNjVXIqfQJHuSoMkg5ACGE8vLyTE1NKRTK119/jRBKSEjQ1NRksViZmZkLFy7kcDhjxow5ffo03jguLo7JZBoYGKxbt87Y2JjJZPJ4vIKCArw2ICBAXV3dyMgI/7hhwwZNTU0KhVJfX48QCgwM3Lp1a1lZGYVCsbKyQghduHCBw+Hs3bt3iE4tLi4OwzB3d/c3q6KiomxsbE6cOHHp0qVefxbDsJiYmIkTJzIYDC6Xu3jx4gcPHuBVkocIISQSicLCwkxNTTU0NKZMmTL4BQyfPHnS3NxsampKlOD7mxUVFfVbi+NyuU5OTrGxsRh5N1H5fH5RURGkHJUFKQcghNCcOXPEFxFfv3795s2bhUIhm81OTk4uKyuzsLD4/PPPOzs7EUIBAQGrVq1qbW3dtGlTRUXF7du3u7q6Pvjgg2fPniGE4uLixBc9i4+P37NnD/ExNjZ20aJFlpaWGIY9fvwYIYQ/0+7u7h6iUzt//vz48eNZLNabVRoaGt9//z2VSv38889bWlrebBAeHh4cHBwSElJbW3vt2rVnz57NnTv35cuXqL8hQgjt2LHjq6++Onz48IsXLxYtWrR8+fJbt24N5kRqamoQQmw2myhhMpkaGhp4PJJrCdOmTXv+/Pmff/45mEgGIz8/XyQS9TozAqgCSDlAEh6Px+Fw9PX1fXx8Wlpanj59SlTRaDT867+trW1CQoJAIEhMTJThEG5ubnw+PzQ0VH5R/1dLS0t5ebmE3Y4dHBw2b95cUVGxY8eOHlVCoTAmJmbJkiUrVqzQ1taePHny0aNH6+vrjx07Jt6s1yFqa2tLSEjw9PT08vLS0dHZtWsXnU6XbXwI+PSzHvvm0el0oVDYby3BcwSWrwAAIABJREFU2toaIVRcXDyYSAYjLy/P2tra2NiYrAAAuSDlAKmoq6sjhIiv8D3MmDGDxWIRN52UR21tLYZhvV7iEKKiosaPHx8fH5+XlydeXlJS0tzcPGPGDKJk5syZ6urqxC3EHsSH6OHDh62trXZ2dniVhoaGkZHRIMcHfxbV1dUlXtjR0aGhodFvLQEfih6XPop05cqVd955h6yjA9JBygHywWAw6urqyI6ip7a2NoTQm4/ixTGZzMTERAqFsmbNGvFrAnw+sZaWlnhjHR0dgUDQ73Hx23S7du2i/EdlZeWbk54HBH88xufziZLW1ta2tjb8ikFyLQHPQPiwKF5DQ0NBQcHChQtJOTpQBpBygBx0dnY2NjYO3Q6SMsP/wvb7CqSDg8OWLVtKS0sjIyOJQh0dHYRQjwQj5Wnq6+sjhA4fPiw+PTQ/P1+GUyCYm5uz2ezKykqiBH8YNmXKlH5rCR0dHeg/w6J4OTk5VCr1/fffJ+XoQBlAygFykJubi2EY8XIfjUbr6xacghkYGFAolKampn5bRkZGTpgw4c6dO0SJnZ2dlpaW+DP/goKCjo6Ot956q9/exo4dy2QyCwsLZQu7VzQazdXV9dq1a8RUi5ycHAqFgk/Gk1xLwIfC0NBQjoFJLycnZ+7cudra2qQcHSgDSDlARt3d3a9fv+7q6ioqKgoMDDQ1NV21ahVeZWVl1dDQkJGR0dnZWVdXJ/7VGyGkq6tbXV1dUVEhEAg6OztzcnKGbpI0i8WysLCoqqrqtyV+e0388TuTydy6dWt6enpSUhKfzy8uLvb39zc2Nvbz85Omt9WrV58+fTohIYHP54tEoqqqqhcvXiCEfHx8DA0NZVtQJzQ09OXLl7t3725pacnPz4+Ojl61atX48eOlqcXhQzF58mQZjj5IIpHo4sWLcFdN1Ylf+Cv5W+tASjL8Ho8cOYI/DGCxWO7u7vHx8fhzZmtr67KysmPHjnE4HISQmZnZo0ePMAzz8/Oj0+kmJiY0Go3D4SxevLisrIzo7dWrV/PmzWMymebm5l988cX27dsRQlZWVk+fPsUw7Pbt22ZmZhoaGnPmzKmpqcnOzmaz2VFRUTKcKZLiDfmAgAA6nd7a2op/TE9Pxyew6enpbdy4sUfj7du3i68+0N3dHR0dbW1tTafTuVyup6fnw4cP8ap+h6i9vT0oKMjU1JRGo+nr63t5eZWUlGAY5unpiRAKCwvrNdr8/HxHR0fiAYyRkRGPx7t69SrR4OrVq7NmzWIwGMbGxtu3b29raxP/ccm1GIa5ubmZmJgQKxRIIM3YDsj169cRQvfu3ZNjn2DYgZQzAing9+jn56erqzukh5CGNH8WS0tLaTTaqVOnFBNSv0Qi0dy5c0+ePKn4Q9fX1zOZzEOHDknTWO4pJyQkxNzcXI4dguEIbqwBGSnJssT9srKyioiIiIiIaG5uJjsWJBKJMjIyBAKBj4+P4o8eHh5ub28fEBCg+EMjhLKzs11dXUk5NFAeg005n332GZvNplAo8n1SOhgRERG2trYcDofBYFhZWX355Zc9/tZ0dnbu27fPyspKXV1dR0fHzs6uoqKi327FF73HqaurGxgYvPvuu9HR0a9fvx6q8wGDFhwc7O3t7ePjI808giGVm5ublpaWk5Mj+VWhoRATE1NYWJidnU3KxmgvXrwoLCyElAPkcGMNX1fqzp078rv2GhQnJ6f4+PhXr17x+fzk5GQ6nb5gwQLxBp6enuPHj79582ZnZ2d1dbW7u3txcbGUnVtaWmpra2MYhj88//e//71q1SoKhWJsbPz777/L/2RkMtQ31oKDg/HXHseNG5eamjp0B+oXGsjNn4sXLwYFBQ1pPEorIyNj3759XV1d0v/IgMa2XydOnNDQ0GhpaZFXh2CYGoEpx83NTfx/LXy9L/zBNYZhp0+fplAoRUVFsnVOpBxxqampVCrVwMBAfBl8EqnOMzn5/lkE4uQ7tkuWLHF1dZVXb2D4ksOzHGKxdCVx7tw58amuenp6CCHixe9vvvlm+vTp8p0k+uGHH65ataq2tvbo0aNy7BaAkaGlpeXixYuLFi0iOxBAPllSDoZh0dHR48ePZzAY2tra+BRYQq9rtve70js+uZPFYnE4nMmTJ+Prdshl+ffnz59raGiYm5sjhDo6Om7evGlvb99XY5kX0sdfScnJycE/KtsgAECic+fOtbW1LVmyhOxAgBIQv+SR8oZMSEgIhUL5+9///vr169bW1vj4eCR2Y23btm0MBuPs2bOvX7/euXMnlUrFH3KEhIQghC5fvtzU1FRbWzt37lxNTc2Ojg4Mw5qbmzkczsGDB4VCYU1NzZIlS+rq6iR0Jb2WlhY2mx0QEIB/LC8vRwjZ29u/++67RkZGDAZjwoQJX3/9NfGawrlz59hsdkRERF8d9npjDcMwPD2MHTtWGQYBbqyBwZPj2Hp5eb3//vty6QoMdwNOOa2trSwW64MPPiBKxJ/lCIVCFovl4+NDNGYwGOvXr8f+89eW2AwYT1SPHz/GMOzu3bsIoXPnzokfSEJX0gsJCbGxseHz+fhHfM32Dz744Pr1669evWpsbMSXrE9KSpKyw75SDoZhFApFR0dHGQYBUg4YPHmNbXNzs6am5rfffjv4rsAIQBvoVdHjx49bW1udnZ17rZV+zXbxld4tLCwMDAxWrFixadOmVatWjRs3bkBd9SU9PT0lJeWXX34h9q3CVxSeNGkSsUPUnj17vvnmm2PHjvn6+krf85vwqTj4++dKMgje3t6DOaPh4vDhw6mpqWRHAfqUlZXV3t6+ePFisgMBSmHAz3LwNZrwhXLfJNua7RoaGleuXJkzZ87evXstLCx8fHyEQuEgl38/c+bMgQMHcnNz8b/dOHwdEXxHZJy6urqZmVlZWZmU3fbl0aNHCKEJEyYgZRoEAEiXmpr63nvvGRgYkB0IUAoDvsrBd4LCtyB8E7Fme2Bg4IC6nTRp0s8//1xXVxcTE3PgwIFJkybhr2fL0BVC6MiRIxcvXrxy5UqPzU60tLSsra3v3bsnXtjV1TX4pW0vXLiAEMKXLFSSQVCF7/4UCmXz5s3i+14DeZHLTFSBQHDhwoW4uLjBdwVGhgFf5djZ2VGp1KtXr/ZaK9ua7dXV1Xga0NfX379///Tp0+/duydbVxiGBQUFFRcXZ2Rk9Mg3uGXLlt25c+fJkyf4x9bW1srKykHOma6pqTl8+PCYMWPWrFmDlGAQAFASWVlZXV1d+EqmACAZUg6+Ju7Zs2dPnjzJ5/OLiorEt4KXsGa7BNXV1evWrXvw4EFHR8edO3cqKytnz54tW1f37t376quvjh8/TqfTxRenOXToEN5gy5YtZmZmq1atevr06atXr4KCgoRCIbHvvTQL6WMY1tzcjE9yq6urS05OdnR0VFNTy8jIwJ/lkD4IACiJ1NRUZ2fnUaNGkR0IUBricwmknOkkEAg+++yzUaNGaWlpzZkzJywsDCE0ZsyYP//8E+tjzXbJK71XVFTweDwul6umpjZ69OiQkBB8+YC+ln+XAJ+T9qbo6GiizbNnzz766CMul8tgMGbNmpWTk0NUSVhIPysra8qUKSwWS11dnUqlIoTwKWqzZs2KiIh49eqVeGNyBwFmrIHBG/zY8vl8JpNJyprZQGlRMAwj/i6npKQsW7ZMvAQMR6rze6RQKMnJyfAsZygMfmxPnTr16aef1tTU6OrqyjEwMKzB5gUAgCGRnJz8wQcfQL4B4oZZynnw4AGlb6TsQQKGi0uXLgUHB4tvQrFy5UrxBi4uLmw2W01NbdKkSbJtFC0v3d3dhw8fJt4eE5eXl+fo6MhisYyNjYOCgnrMHe2rNisr6+DBg4rc4ujly5cXL178+OOPFXZEMDyI32VTnWcAI5vq/B6R1M8bwsLCFi1aRKxDYWlpiT/T7rHcQ05OjvhG1KR49OiRo6MjQmjq1Kk9qu7evauhoREaGtrc3Hzjxg09Pb3Vq1dLWRsbG+vk5PT69Wspw5B+bHv11VdfaWtrE/t/A4AbZlc5QEkIhcJev4OT21VfDhw4cObMmZSUFGIdCoRQXFwclUr18/Mjfd82cX/++eeOHTv8/f17XXw2MjLSyMhoz549mpqaDg4OQUFB33//PbEaheTaTZs2TZ061dXVtaurSwEncurUqY8++khDQ0MBxwLDCKQcIIuTJ0/W1tYqW1e9evz4cWho6J49e/C3mAk8Hi8wMPD58+fbtm0buqMP1NSpU9PS0nx9ffHFmcR1dXWdP3/eycmJeElz4cKFGIZlZmb2W4sLDw8vLCyMjY0d6rO4detWcXHxJ598MtQHAsMOpBzVhWFYTEzMxIkTGQwGl8tdvHgx8Y04ICBAXV3dyMgI/7hhwwZNTU0KhYKvFRQYGLh169aysjIKhWJlZRUXF8dkMg0MDNatW2dsbMxkMnk8XkFBgQxdoUHsH9GXuLg4DMPc3d3frIqKirKxsTlx4sSlS5cGOkT9bkUh910nnjx50tzcbGpqSpRYWloihIqKivqtxXG5XCcnp9jYWGyIZzP+85//tLGxefvtt4f0KGBYEr/LpjrPAEY2KX+PYWFh6urqp06damxsLCoqmj59up6eXk1NDV7r6+traGhINI6OjkYI4fspYBjm5eVlaWlJ1Pr5+Wlqat67d6+tra2kpGTmzJlsNpvYiXVAXfW7f4Q4JMXzBgsLC1tb2x6FlpaW5eXlGIbduHGDSqWOGzeuubkZe+NZjuQhkrAVBTborTfefvvtHs9y8CU/xN8wwzBMQ0PD2dm531pCcHAwkm4PX2nGtlft7e16enr79++X4WfBiAdXOSpKKBTGxMQsWbJkxYoV2trakydPPnr0aH19vfhaEgNCo9HwqwFbW9uEhASBQJCYmChDP25ubnw+PzQ0VLYwemhpaSkvL8e/7/fKwcFh8+bNFRUVxAoUBCmHiMfjcTgcfX19Hx+flpaWp0+fIoTa2toSEhI8PT29vLx0dHR27dpFp9NlGxACPv1MfMdbhBCdThcKhf3WEqytrRFCfb0xLRdZWVkNDQ2DXJodjFSQclRUSUlJc3PzjBkziJKZM2eqq6sTN8QGY8b/t3fnAU1c7cLATyAhIZCwiCCiKJsoglJXCFJUWqxSRUQKt1pFfd+iVSOiFBFBBNfiRS4qrwuW1h1ZClZFrfaCtaJv+ykiWBVQEEQEBCQsMZDM98fczs0NEGK2IfD8/jJzJmeeOcE8me08U6YwmcwPqjShInV1dRiG4ZM+9Gbnzp329vaHDx++ffu2+PIPHSLxUhSKl97oDr8WJXHxXyAQ4JfopbcS8KF48+aNIpFI9+OPP3766acjR45U3SaA5oKUM0g1NzcjhCRmPjU0NOTxeErpn06n19fXK6UrRfD5fPR3naTeMBiM1NRUCoWycuVK8WMCRYZIFVUn8OtheP1ZXHt7O5/Px0tySG8l4BkIHxZVqKuru3btGtw4AHoDKWeQMjQ0RAhJfHs2NzePGDFC8c47OzuV1ZWC8G/YPh+BdHV1DQ0NLS0tjYuLIxYqMkREAQvxs9gFBQVy7ALBysqKxWJVVlYSS8rKyhBCEyZM6LOVIBAI0N/DogqnTp1iMpk+Pj4q6h9oOkg5g5Sjo6O+vv6ff/5JLLl3755AIJg8eTL+kkql4ueI5JCXl4dhmIuLi+JdKcjU1JRCocjy5E1cXNzYsWMfPHhALOlziKRQRdUJKpU6b968W7duiUQifElubi6FQsFvxpPeSsCHwszMTImBiTt58mRgYKD0M5lgMIOUM0gxGIxNmzZlZWWdPn26paXl0aNHa9asMTc3Dw4OxlewtbVtbGzMzs7u7Oysr68X//mMEDI2Nq6pqamoqODxeHg6EYlETU1NXV1dRUVFISEhlpaWQUFBcnQlS/0I2TGZTGtra7yUbZ8DkpqaKn75vc8hkt5bb1UnAgMDzczM5JtQJyoq6s2bN9u3b29raysoKIiPjw8KCrK3t5elFYcPhYIFonrzxx9/FBUVwVk1II34gT/cJD0wyPg5ikSi+Ph4Ozs7Go1mZGTk6+v79OlTovXt27ezZs1iMBhWVlbr168PCwtDCNna2uK3Pt+/f3/UqFG6urozZsyora0NDg6m0WgWFhZUKpXNZi9cuLC8vFy+rqTUj+gOyXAjL5fLpdFoxMwrWVlZ+A1sJiYm69atk1g5LCxM/CZpKUMkvRQF1nvVCbxeWXR0dI/RFhQUuLm5ERdghg0bxuFw8vPziRXy8/OnTZtGp9PNzc3DwsL4fL7426W3Yhjm7e1tYWGBV3uSTpaxlbBq1apx48Z90FvAYAMpZwBS/+cYHBxsbGyszi3iZPlaLC0tpVKpp06dUk9IfRIKhe7u7qRUkWloaGAwGPv375dl5Q9NOc3NzXp6egcPHpQ3OjAowIk1oBzqnKX4g9ja2sbGxsbGxra2tpIdCxIKhdnZ2Twej5RZz2NiYpydnblcrio6P336tEgkgsdxgHSQcsDAFxER4e/vHxgYSPoMnnl5eZmZmbm5ueq/wJ6QkFBYWHjlyhUajaaK/lNSUvBiu6roHAwYkHKAorZu3Zqamvru3TsrK6uMjAyyw+nZrl27uFzunj17yA3D09PzzJkzxIxzapOTk/P+/fu8vDwVpYQ7d+4UFhbKcmMFGOSoZAcANN7u3bt3795NdhR98/Ly8vLyIjsKcvj4+Kj0WZmjR49OnDhx2rRpqtsEGBjgKAcAoJDm5uaMjIw1a9aQHQjQAJByAAAK+eGHH7S0tP7jP/6D7ECABoCUAwBQSEpKypIlS/DHkgCQDq7lAADkl5+fX1JScvLkSbIDAZoBjnIAAPI7evTo1KlTJ02aRHYgQDP0cJRD1E4HGm2QfI4BAQEBAQFkRzFINTQ0/PTTT4cOHSI7EKAx/k/K4XA4ildoB6BHQqEwLCxMS0tr+/btLBaL7HBA3zgcjvQVjh07xmAwSJlJAWgoCoZhZMcABovq6uqPP/7YwMDg119/hcfUNd379++trKyWL19O+gO2QIPAtRygPiNGjPjll1/q6uq8vb37w4xnQBEnT55sbGxcv3492YEATQJHOUDdnj596uHh4ejoeOnSJQaDQXY4QB4Yho0fP57D4aSkpJAdC9AkcJQD1M3e3v7atWv3798PCAggq1ooUFBOTs6TJ09CQ0PJDgRoGDjKAeS4e/fup59+OmfOnLS0NPFanEAjcDicIUOG/Pzzz2QHAjQMHOUAcri4uOTk5Fy+fHnVqlXwu0ez5OTk3L17NzIykuxAgOaBoxxApmvXrvn4+Hz99ddJSUlkxwJkIhKJJk2aZGdnl56eTnYsQPPAhDeATHPmzDl79mxAQICBgUFcXBzZ4YC+nTx5sri4+Ny5c2QHAjQSHOUA8v34448rV67ctWvXli1byI4FSCMQCMaNG+fp6Xns2DGyYwEaCY5yAPmWL1/e2tq6bt06HR0duAmqP0tOTn79+nV0dDTZgQBNBSkH9Atr164VCASbNm1isVj//Oc/yQ4H9KC2tjYmJmbjxo0jRowgOxagqSDlgP5i48aNDQ0Na9asYbFYMG1XP7R582YDA4OtW7eSHQjQYJByQD+ya9cugUCwbNkyPT29+fPnkx0O+F+3bt06e/bsTz/9pKenR3YsQIPB7QOgf8EwbPXq1T/88ENOTs5nn31GdjgAIYQEAoGzs/OoUaNyc3PJjgVoNjjKAf0LhUL517/+xePx/Pz8rl696u7uTnZEACUkJFRUVFy+fJnsQIDGg6Mc0B8JhcLAwMBffvnl5s2bkydPJjucQe2vv/6aNGlSdHR0REQE2bEAjQcpB/RTAoFg4cKFf/zxR15e3vjx48kOZ5Dq6upyc3Pr6uq6e/cujUYjOxyg8WCONdBP6ejoZGRkODg4eHp6Pn36lOxwBqndu3c/fPjw5MmTkG+AUsBRDujXWlpaPD0937x589tvv40aNYrscAaXwsLC6dOn79mzB57PBcoCKQf0dw0NDTNnzhQIBPn5+ebm5mSHM1i8f/9+2rRpBgYGeXl5WlpwOgQoB/wlgf7OxMTk119/1dbWnjNnztu3b8kOZ7AIDQ2tqKhITU2FfAOUCP6YgAYwNTW9fv06j8f75JNPmpubyQ5n4EtPT09OTk5OTraxsSE7FjCgwIk1oDHKyso+/vhjKyur69evwzPwqlNeXj558uSlS5ceOnSI7FjAQAMpB2iS4uLimTNnfvTRRz///DODwSA7nAHo/fv3HA4HvytaV1eX7HDAQAMn1oAmcXR0vHHjxv/7f/8vMDCwq6uL7HAGoA0bNpSXl2dlZUG+AaoAKQdoGGdn58uXL9+8eXPFihUikYjscAaUo0ePHjt2LDU1FS7hABWBOdaA5nF1df3pp5/mz59Po9FOnDhBoVDIjmgg+P3337lcbkxMjK+vL9mxgAELruUATXXx4sXFixevWbPmv/7rv8iOReNVVlZOmzaNw+FkZWVBCgeqox0TE0N2DADIw97e3sHB4dtvvxWJRDNnziQ7HA3W0dHx2Wef6ejoXLp0CW7KACoFJ9aABvPz80tJSVm5cqWurm54eDjZ4WgkoVD45ZdfVlZW/vvf/2az2WSHAwY4SDlAswUFBfF4vA0bNrDZ7DVr1pAdjuYJDQ29evXqL7/8YmVlRXYsYOCDlAM03vr169+9e7d27VodHZ1Vq1aRHY4m2bt376FDh9LS0mbMmEF2LGBQgJQDBoJt27a1tbUFBwfr6+sHBASQHY5mOH/+fGRkZEJCwuLFi8mOBQwWkHLAALFnzx6BQPDVV1/p6+t7e3uLN3V2djY2NpqZmZEVWz/066+/Ll++fOPGjRs2bCA7FjCIwKOgYODYv3//8uXL/f39//u//5tYyOfzFyxYEB0dTWJg5AoPD29raxNfcvfuXR8fHz8/v++++46sqMDgBM/lgAFFKBQuXbr04sWL165dmzFjRmtr6+eff37r1i0ajVZdXT106FCyA1S369evz5kzZ9asWVeuXMFvgH706NHMmTOnTp168eJFHR0dsgMEgwsc5YABRVtb++TJk7Nnz/7888/z8/M9PT3v3LmDYRiGYcnJyWRHR4LIyEhtbe3ffvvNz8+vs7OzrKzMy8vL2dk5Ozsb8g1QPzjKAQMQn8+fO3fu8+fPX79+3dnZiS80NDSsqakZVLNVXrp0af78+fi/tbW1P/nkk8ePH1tYWPzyyy/6+vrkxgYGJzjKAQNQc3NzTU2NeL5BCPF4vFOnTpEYlZphGBYVFaWtrY2/FAqFv/zyC4PBuHTpEuQbQBZIOWCgqaysdHFxefHihXi+QQiJRKK9e/cOnsmns7OzCwsLhUIhsUQkEpWXlw/mOykA6SDlgAHlyZMn06dPr6mpkcg3CCEMw168eHHp0iVSAlMzDMO2bdtGHOIQRCLRkSNHQkNDSYkKAEg5YEDJzMxsamrqrVVbW3vfvn3qjIcs6enpf/31l/ghDkEkEh04cGDXrl3qjwoASDlgQImMjKyqqtq4cSONRqPRaBKtQqHwzp079+7dIyU2tREKhZGRkb3VINDW1tbT0+PxeAKBQM2BAQApBww0pqam+/btKysrCwoK0tbWlkg8NBpt//79ZMWmHufOnSsvL5e4aqWlpUWhUCwsLHbt2lVdXb137164SRqoH9wkDQayioqKXbt2ff/991QqlfhRr6WlVVpaam1tTW5sKiIUCseMGVNRUUGkHCqV2tXV5ejoGBYW9uWXX1KpMM0VIA0c5YCBbPTo0cePHy8uLvb399fS0sK/bbW1tQ8ePEh2aKry448/EvmGRqNRKBR3d/eLFy8+evRo2bJlkG8AueAoBwwWRUVF27Ztu3TpEoZhurq6NTU1hoaGZAelZJ2dndbW1tXV1XhqWbZs2ebNm8eNG0d2XAD8D0VTTkJCQkFBgbKiAUDVmpqaiouL37x54+TkZG9vT3Y4Svb8+fP79+/TaDRbW1sbGxuoKg1IFxoa6urqSrxU9MRaQUHB3bt3FewEALUxMjJyd3efPXt2XV1dRkYG2eEok0gkqqqq+uijjz7//PPx48cT+SYjI6O6uprc2MDglJGRUVVVJb5ECSd2XVxc0tPTFe8HAHW6cOFCQEDAQPrT7ejooNPpWlqSvyMpFMrGjRu/+OILUqICg1n3O/XhWiIAA8SgmrEUaCi4Yw0AAICaQMoBAACgJpByAAAAqAmkHAAAAGoCKQeAD3DlyhUDA4Off/6Z7ECUbPXq1ZS/LV26VLzpxo0bERERmZmZ1tbW+ApfffWV+ApeXl4sFktbW3v8+PH3799Xb+D/Bz5JNofD6d50+/ZtNzc3JpNpbm4eHh7+/v17WVovXry4b9++Hifk7pOmjNvZs2enTp3KYrFGjRq1YsWK2tpafHn3fc/Ozib+SExMTOTcHqaYxYsXL168WMFOAFC/tLQ0Of7+L126xGazL168qIqQVAQhlJaWJn2d4OBgY2Pj3Nzcp0+f8vl8Ynl0dPT8+fNbWlrwlzY2NkOGDEEI4ZM4EHJzc318fJQe+Qd59uyZm5sbQmjixIkSTcXFxbq6ulFRUa2trXfu3DExMVmxYoWMrYmJiR4eHk1NTR8UjKaM2/nz5xFC+/bta25ufvDggbW1tbOzc2dnJ94qse8ikai6uvrWrVvz5s0bMmSILP13/9uDlAMGKflSjtq0t7e7uroqpSsZU46FhYXEwj179owZM6ajo4NYYmNjc+bMGS0tLQsLi+bmZmI56V+dhYWFixYtOn36tLOzc/eUExAQYGVlJRKJ8Jfx8fEUCuWvv/6SpRXDMC6X6+rqSnwR90mDxm3WrFnDhw8n9v3QoUMIodu3bxMr9LjvGzZskDvlwIk1APqjEydO1NXVkRhAWVlZVFTUjh07JGbN4XA4ISEhr1692rx5M1mxdTdx4sTMzMwlS5bQ6XSJpq6ursuXL3t4eBCPJc6dOxfDsJycnD7jVqRIAAAgAElEQVRbcTExMYWFhYmJibJEolnjVlVVZW5uTuz7yJEjEUKVlZXECh+077KAlAOArG7fvm1paUmhUPAfg8nJyXp6ekwmMycnZ+7cuWw2e8SIEefOncNXTkpKYjAYpqamq1evNjc3ZzAYHA6HqA7H5XJ1dHSGDRuGv1y7dq2enh6FQmloaEAIhYSEbNq0qby8nEKh2NraIoSuXr3KZrPVWcozKSkJw7AFCxZ0b9q5c+eYMWNSUlJu3LjR43sxDEtISBg3bhydTjcyMlq4cOGTJ0/wJumDhhASCoXR0dGWlpa6uroTJkzAD0YV8fz589bWVktLS2KJjY0NQqioqKjPVpyRkZGHh0diYiImw4yUmjVu1tbW4r9s8As54nU9PmjfZSL3ERkOTqwBDSXfiTV8wqiDBw/iLyMjIxFCN2/efPfuXV1dnbu7u56enkAgwFuDg4P19PQeP37M5/NLSkrwi7QvX77EW5csWWJmZkb0HB8fjxCqr6/HX/r5+dnY2BCtly5dYrFYsbGxcuwpkuvEmrW1tYODg8RqNjY2L168wDDszp07Wlpao0ePbm1txbqdIIqOjtbR0Tl16lRzc3NRUdGkSZNMTExqa2vxVumDtnnzZjqdnpGR0dTUtHXrVi0trT/++EP2nZ0+fbrEibX8/HyEUHx8vPhCXV1dT0/PPlsJERERCKEHDx70GYBmjVteXh6NRktKSmppaSkuLh43btycOXMk1um+73BiDQAycTgcNps9dOjQwMDAtra2ly9fEk1UKhX/0erg4JCcnMzj8VJTU+XYhLe3d0tLS1RUlPKilqatre3Fixf47/0eubq6bty4saKiYsuWLRJNHR0dCQkJixYtWrp0qYGBgZOT05EjRxoaGo4dOya+Wo+Dxufzk5OTfX19/fz8DA0Nt23bRqPR5BsxAn77mba2tvhCGo3W0dHRZyvBzs4OIfTo0SPp29K4cfPw8AgPD+dyuWw229HRkcfjpaSkSKwj477LCFIOAEqDl3bu7OzssXXKlClMJpM4VdKf1dXVYRjGZDKlrLNz5057e/vDhw/fvn1bfHlJSUlra+uUKVOIJVOnTtXR0SFOKkoQH7SnT5+2t7c7OjriTbq6usOGDVNwxPBrKl1dXeILBQIBPiWd9FYCPhRv3ryRvi2NG7fIyMhjx47dvHmztbX1+fPnHA7H1dVVYu5nGfddRpByAFAfOp1eX19PdhR94/P5CKHul+LFMRiM1NRUCoWycuVK8WOC5uZmhJC+vr74yoaGhjwer8/ttrW1IYS2bdtGPP9RWVnZ3t4u317g8AtmLS0txJL29nY+n29ubt5nKwHPQPiwSKFZ4/b69et9+/Z9/fXXs2fP1tPTs7KyOn78eE1NDX6OlyDjvssIUg4AatLZ2dnc3DxixAiyA+kb/i3T5yOQrq6uoaGhpaWlcXFxxEK81qrEF6WMOz506FCE0IEDB8TP/itYBNLKyorFYonfhVVWVoYQmjBhQp+tBIFAgGSYq1uzxq20tFQoFA4fPpxYwmazjY2NS0pKxFeTcd9lBCkHADXJy8vDMMzFxQV/SaVSezsFRzpTU1MKhfLu3bs+14yLixs7duyDBw+IJY6Ojvr6+n/++Sex5N69ewKBYPLkyX32NnLkSAaDUVhYKF/YPaJSqfPmzbt165ZIJMKX5ObmUigU/KYy6a0EfCjMzMykb0uzxg1PZq9fvyaW8Hi8xsZG/FZpgoz7LiNIOQCokEgkampq6urqKioqCgkJsbS0DAoKwptsbW0bGxuzs7M7Ozvr6+vFf2gjhIyNjWtqaioqKng8XmdnZ25urjpvkmYymdbW1rLUEsVPE4lffmcwGJs2bcrKyjp9+nRLS8ujR4/WrFljbm4eHBwsS28rVqw4d+5ccnJyS0uLUCisrq7GvxMDAwPNzMzkmxgmKirqzZs327dvb2trKygoiI+PDwoKIsqQS2/F4UPh5OQkPRLNGjcrK6tZs2YdP3781q1bHR0dVVVV+LZWrVrV274rgSw3ukkBN0kDDSXHTdIHDx7ET/0zmcwFCxYcPnwYv7JqZ2dXXl5+7NgxNpuNEBo1atSzZ88wDAsODqbRaBYWFlQqlc1mL1y4sLy8nOjt7du3s2bNYjAYVlZW69evDwsLQwjZ2trid1Hfv39/1KhRurq6M2bMqK2tvXLlCovF2rlzpxx7iuS6SZrL5dJotPb2dvxlVlYWfiOWiYnJunXrJN4eFhYmfrOvSCSKj4+3s7Oj0WhGRka+vr5Pnz7Fm/octPfv34eHh1taWlKp1KFDh/r5+ZWUlGAY5uvrixCKjo7uMf6CggI3NzfiAsywYcM4HE5+fj6xQn5+/rRp0+h0urm5eVhYmPikPn22Yhjm7e1tYWGBP6UvPRLNGreGhoaQkBBbW1s6na6vr+/m5vbTTz9J2XecIjdJQ8oBg5QaJrzBJy5T6SZkIV/KKS0tpVKpp06dUmVoH0AoFLq7u584cUL9m25oaGAwGPv375clkgE2bhL7joPncgDop+SbhJgUHR0d165dKy0txS8X29raxsbGxsbGtra2kh0aEgqF2dnZPB4vMDBQ/VuPiYlxdnbmcrmyRDLAxk183zEMq6mpuX37Nn6HhXwg5QAAEEKosbHxs88+GzNmzMqVK/ElERER/v7+gYGBslwPV6m8vLzMzMzc3Fzpj7yoQkJCQmFh4ZUrV2g0moyRDJhxk9j3nJwcCwsLd3f3y5cvyx+TfEdbBDixplxnzpyZMmWKvr4+fp359evXH/T2J0+erFu3zsHBQV9fX1tbm81m29nZzZs3786dOyoKWHOp+sRaREQE/rDe6NGj09PTVbehPiEZTqxJce3atfDwcCXGo0Gys7N3797d1dUlx3s1fdwU2XdC9789SDn9iPTaFX1KSUmh0Wgff/zx1atXm5qa+Hx+eXn5+fPnORzO0aNHVRq5JurnxQuUSMGUA4Dcuv/tDaITax0dHT2WC+w/nR89enT48OFhYWEGBgbOzs6hoaGFhYW9TXch4e7du8HBwe7u7jdv3pwzZ46hoSGdTre2tg4ICIiOjsbPzqtZ/x9wAICaDaKUo9ICJErpvM/aFVLs3LlTKBTu2bOHSqVKNM2ZM2fdunUKxiaH/j/gAAA1U1/KOXXq1JQpUxgMhp6e3ujRo/GZHjAF6kP01udvv/3m4OBgYGDAYDCcnJyuXbuGeipA0mN5iT43qkjnfZJeu0JKxRSBQHDz5s0hQ4ZMmzZN+iZgwAEAZFLwVJ2M13IOHDiAENqzZ8/bt28bGxuPHj26ZMkSTLH6EL31mZ6eHhMT09jY+PbtWxcXF+L+cYkCJL2Vl5C+UQU7l0567QopFVOePXuGEHJxcelzEzDgBLiWA4Cqdf/bU0fKEQgEhoaGs2bNIpZ0dXUlJia2t7fr6+sHBgYSy//9738jhIhvVfzLiCghfvjwYYRQWVmZlD4lNr17927094zi4l9SHR0dTCaT2HR7ezudTv/mm2+kb1Txzvu0bds24tfAiBEjqqqqZHkXPi/TJ598In01GHBxkHIAULXuf3uS5/1VoaioqLm5ec6cOcQSbW3tDRs2/Pnnn3LXh+itT4m34LeTd38cT/byElIqoCjeuYTIyMiUlJSbN29Onz69rq5uy5Ytrq6ud+7ckZhlrzt8wvM+5ypXpCDHgBxwhBBx5WxgCwgICAgIIDsKAJA6Ug5eiwKfmlucIvUheusTIXT58uX4+PiSkpKWlpbeZuolykuIH1VIVMjokeo6x2tXREREzJ49GyGE164wMjKKj49PSkqS/t7Ro0czGAz89JoUMODdDYarPgEBASEhIa6urmQHAgad7j901JFy8HoMDQ0NEssVqQ/RW58vX7709fVdtGjR999/P3z48IMHD3777bfd306UlwgJCZF9R1TauYy1K3pEp9PnzJmTk5Pz+++/u7m5SbQ2NjZ+++23KSkpMODdffHFFx/6Fo0TEBDg6uo6GPYU9DfdU4467lgbPXq0sbHx9evXJZYrUh+itz4fPXrU2dn5zTffWFtbMxiM3k6byFdeQqWdy1i7ojcxMTF0Oj00NFSiZjtCqLi4GL9zGgYcAEAudaQcOp2+devWW7ducbncV69eiUQiHo/3+PFjRepD9NanpaUlQujGjRt8Pr+0tFT8KoV4ARJtbe3eyktIodLO+6xdIb1iirOz85kzZ4qLi93d3a9cufLu3bvOzs4XL14cP3581apV+FUQGHAAAMkUvCFB9glvDh065OTkxGAwGAzGRx99dPjwYUyx+hC99RkeHm5sbGxoaOjv73/o0CGEkI2NzcuXLyUKkPRYXqLPjSrSeZ9DJL12hSwVU16+fLl582YnJyd8jjVDQ8OPPvpo1apVv//+O74CDDgB7lgDQNW6/+1R8KVy8/f3Rwilp6cr0gkA6nfhwoWAgAAF//41AoVCSUtLg2s5QP26/+0NoglvAAAAkAtSjpo8efKE0jtSCk8B0P/duHEjIiIiMzPT2toa/8/y1Vdfia/g5eXFYrG0tbXHjx9///59suJECIlEogMHDvQ42+zt27fd3NyYTKa5uXl4ePj79+/FW8+ePTt16lQWizVq1KgVK1bgM10hhC5evLhv3z4NqvInEwVP1UHxAqCh4FpO/xcdHT1//vyWlhb8pY2NzZAhQxBCly5dEl8tNzfXx8eHjAD/17Nnz/DnEyZOnCjRVFxcrKurGxUV1draeufOHRMTkxUrVhCt0kuWJCYmenh4NDU1qW9PlKr73x4c5QCgKkqssDAIizXs3bv3/PnzFy5cYLFYxMKkpCQtLa3g4GDSC26Ke/jw4ZYtW9asWePs7Ny9NS4ubtiwYTt27NDT03N1dQ0PD//hhx+I2TGklyzZsGHDxIkT582b19XVpb79USVIOQCoihIrLAy2Yg1lZWVRUVE7duxgMBjiyzkcTkhIyKtXrzZv3kxWbN1NnDgxMzNzyZIldDpdoqmrq+vy5cseHh7EU2Vz587FMCwnJwd/2WfJkpiYmMLCwsTERJXvhlpAygFAGqz3cg9cLldHR2fYsGH4y7Vr1+rp6VEoFHyOBokKC0lJSQwGw9TUdPXq1ebm5gwGg8PhED9mP6grJLWSxcCQlJSEYdiCBQu6N+3cuXPMmDEpKSk3btzo8b1SPrI+i2UovQrG8+fPW1tb8SfMcDY2NgihoqIi/KX0kiUIISMjIw8PD3wKXQWD6RcUPFUH13KAhpLxWo70cg9LliwxMzMjVo6Pj0cI1dfX4y8lKiwEBwfr6ek9fvyYz+eXlJTgV4xfvnwpR1dSKll0hzTwWo61tbWDg4PEQhsbmxcvXmAYdufOHS0trdGjR7e2tmLdruUoUqFDvrIjhOnTp0tcy8nPz0cIxcfHiy/U1dX19PTE/y29ZAkuIiICIfTgwQPZI+knuv/twVEOAL3q6OhISEhYtGjR0qVLDQwMnJycjhw50tDQcOzYMfk6pFKp+K9vBweH5ORkHo+XmpoqRz/e3t4tLS1RUVHyhdHPtbW1vXjxAj8a6JGrq+vGjRsrKiq2bNki0STjR8bhcNhs9tChQwMDA9va2l6+fIkQ4vP5ycnJvr6+fn5+hoaG27Zto9Fo8n1ABPzmNG1tbfGFNBqNmJjKw8MjPDycy+Wy2WxHR0cej5eSkiLRiZ2dHULo0aNHikTST0DKAaBXH1ru4YNMmTKFyWTKWGRhUMFLIuHTUvRm586d9vb2hw8fvn37tvhyRSp0KFIFozf4tSiJi/8CgUBXVxf/d2Rk5LFjx27evNna2vr8+XMOh+Pq6lpVVSW+Pj4Ub968USSSfgJSDgC9UqTcgyzodHp9fb1SuhpI+Hw+Qqj7pXhxDAYjNTWVQqGsXLlSfCpbRT4yogoG8cBcZWVln2WopMOvz+G1P3Dt7e18Ph8vroGXLPn6669nz56tp6eHlyypqanBT6sS8PyED4umg5QDQK8UKffQp87OTmV1NcDg37B9PgLp6uoaGhpaWloaFxdHLFTkIyOqYIhfeygoKJBjFwhWVlYsFkv8DrSysjKE0IQJE5DMJUsEAgH6e1g0HaQcAHrVZ7kHKpXaW+G4PuXl5WEY5uLionhXA4ypqSmFQpHlyZu4uLixY8c+ePCAWKJIhQ5VVMGgUqnz5s27deuWSCTCl+Tm5lIoFPxmPBlLluBDYWZmpsTAyAIpB4Be9VnuwdbWtrGxMTs7u7Ozs76+XvzHLPq/FRbwdCISiZqamrq6uoqKikJCQiwtLYOCguToSnolC03HZDKtra2rq6v7XBM/vSZ+cV6RCh0MBqO3KhiBgYFmZmbyTagTFRX15s2b7du3t7W1FRQUxMfHBwUF2dvbIxlKluDwoXBycpJj6/2OgvfAwU3SQEPJeJO0lHIPGIa9fft21qxZDAbDyspq/fr1YWFhCCFbW1v81meJCgvBwcE0Gs3CwoJKpbLZ7IULF5aXl8vXlSyVLAhIA2+S5nK5NBqtvb0df5mVlYXfwGZiYrJu3TqJlcPCwsRvklakQkdvVTB8fX0RQtHR0T1GW1BQ4ObmRtQ+HzZsGIfDyc/PJ1bIz8+fNm0anU43NzcPCwvj8/lEk/SSJThvb28LCwuRSCTfYJKo+98epBwwSKl/jrXg4GBjY2N1bhGniSmntLSUSqWeOnWK7ED+h1AodHd3P3HihPo33dDQwGAw9u/fr/5NK6773x6cWANAfQbarMAqY2trGxsbGxsb29raSnYsSCgUZmdn83g8UmZ8j4mJcXZ25nK56t+0KkDKAQD0RxEREf7+/oGBgaTP4JmXl5eZmZmbmyv9USFVSEhIKCwsvHLlCl5LfgCAlAOAOmzdujU1NfXdu3dWVlYZGRlkh6MZdu3axeVy9+zZQ24Ynp6eZ86cIWbAU5ucnJz379/n5eUZGRmpedOqQyU7AAAGhd27d+/evZvsKDSPl5eXl5cX2VGQw8fHx8fHh+wolAyOcgAAAKgJpBwAAABqAikHAACAmkDKAQAAoCZKuH2gurr6woULivcDgDrh0zUOkj9dBeemBEBpFHy4dPHixWTvAQAAgH5KYvYBCjYw6mkD0A9cuHAhICAA/k8B0Bu4lgMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNqGQHAIAGq66uXr58uVAoxF82NTWxWKyZM2cSK9jb2x89epSc4ADofyDlACC/ESNGVFZWlpeXiy/Mz88n/v3xxx+rPSgA+i84sQaAQpYtW0aj0XprDQwMVGcwAPRzFAzDyI4BAA1WXl5uZ2fX4/+j8ePHFxcXqz8kAPotOMoBQCE2NjYTJkygUCgSy2k02vLly0kJCYB+C1IOAIpatmyZtra2xMKuri5/f39S4gGg34ITawAo6vXr1yNGjBCJRMQSLS2t6dOn37lzh8SoAOiH4CgHAEWZm5u7ublpaf3v/yYtLa1ly5aRGBIA/ROkHACU4KuvvhJ/iWHYokWLyAoGgH4LUg4ASrB48WLico62tvYnn3xiampKbkgA9EOQcgBQAiMjo08//RTPOhiGLV26lOyIAOiPIOUAoBxLly7F7yCg0WgLFy4kOxwA+iNIOQAox4IFC+h0OkJo/vz5+vr6ZIcDQH8EKQcA5dDT08MPbuCsGgC9gedylKb78+cAgMFp8eLF6enpZEfRH8FM0soUEhLi6upKdhRAIQEBAXJ/jkKhMC0t7csvv1R6VEp34MABhNDGjRvJDmQAwscW9AiOcpSGQqGkpaV98cUXZAcCFKLg58jn8xkMhnJDUgV8Mh74Ja4KMLZSwLUcAJRJI/INAGSBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAOAEly5csXAwODnn38mOxBVuXHjRkRERGZmprW1NYVCoVAoEpNne3l5sVgsbW3t8ePH379/n6w4EUIikejAgQMcDqd70+3bt93c3JhMprm5eXh4+Pv378Vbz549O3XqVBaLNWrUqBUrVtTW1uLLL168uG/fPqFQqI7oBzpIOQAowcB+2GD79u1JSUlbt2718/N7/vy5jY3NkCFDTp8+ffnyZWKd69evp6enz58/v6SkZNKkSWSFWlpa+vHHH4eGhra3t0s0lZSUeHl5eXp61tfXZ2Vlff/992vWrCFa09LSlixZ4u/vX11dnZOTc+vWrblz53Z1dSGEFixYwGAwPD09m5ub1bozAxGkHACUwNvb+927d/Pnz1f1hjo6Onr8/a46e/fuPX/+/IULF1gsFrEwKSlJS0srODj43bt36gxGuocPH27ZsmXNmjXOzs7dW+Pi4oYNG7Zjxw49PT1XV9fw8PAffvjhyZMneOvRo0eHDx8eFhZmYGDg7OwcGhpaWFh47949vHXDhg0TJ06cN28enoSA3CDlAKBJTpw4UVdXp7bNlZWVRUVF7dixQ+J5Iw6HExIS8urVq82bN6stmD5NnDgxMzNzyZIl+Pyq4rq6ui5fvuzh4UFMTDV37lwMw3JycvCXVVVV5ubmROvIkSMRQpWVlUQPMTExhYWFiYmJKt+NAQ1SDgCKun37tqWlJYVCOXToEEIoOTlZT0+PyWTm5OTMnTuXzWaPGDHi3Llz+MpJSUkMBsPU1HT16tXm5uYMBoPD4RC/prlcro6OzrBhw/CXa9eu1dPTo1AoDQ0NCKGQkJBNmzaVl5dTKBRbW1uE0NWrV9ls9q5du1S0a0lJSRiGLViwoHvTzp07x4wZk5KScuPGjR7fi2FYQkLCuHHj6HS6kZHRwoULiUMK6UOEEBIKhdHR0ZaWlrq6uhMmTEhLS1NwR54/f97a2mppaUkssbGxQQgVFRXhL62trcVzOX4hx9ramlhiZGTk4eGRmJg4sE+iqhqkHAAUNWPGjDt37hAvv/nmm40bN3Z0dLBYrLS0tPLycmtr63/+85+dnZ0IIS6XGxQU1N7evmHDhoqKivv373d1dX366adVVVUIoaSkJPG5dg4fPrxjxw7iZWJi4vz5821sbDAMKysrQwjh17TxOj2qcPnyZXt7eyaT2b1JV1f3hx9+0NLS+uc//9nW1tZ9hZiYmIiIiMjIyLq6ulu3blVVVbm7u7958wb1NUQIoS1btnz33XcHDhx4/fr1/Pnzv/zyyz///FORHcFTiPi5QQaDoauri8eDENq6dWttbe3Bgwd5PF5JSUliYuKcOXNcXFzEO/noo49evXr18OFDRSIZ5CDlAKAqHA6HzWYPHTo0MDCwra3t5cuXRBOVSsV//js4OCQnJ/N4vNTUVDk24e3t3dLSEhUVpbyo/1dbW9uLFy/wo4Eeubq6bty4saKiYsuWLRJNHR0dCQkJixYtWrp0qYGBgZOT05EjRxoaGo4dOya+Wo9DxOfzk5OTfX19/fz8DA0Nt23bRqPR5BsfAn5zGlEsHEej0To6OvB/e3h4hIeHc7lcNpvt6OjI4/FSUlIkOrGzs0MIPXr0SJFIBjlIOQConI6ODkKI+AkvYcqUKUwmkzjp1H/U1dVhGNbjIQ5h586d9vb2hw8fvn37tvjykpKS1tbWKVOmEEumTp2qo6NDnEKUID5ET58+bW9vd3R0xJt0dXWHDRum4Pjg16IkLv4LBAJdXV3835GRkceOHbt582Zra+vz5885HI6rqyt+6EnAh4I4MAJygJQDAPnodHp9fT3ZUUji8/kIoe6X4sUxGIzU1FQKhbJy5UriiAEhhN9PLFEd1dDQkMfj9bld/DTdtm3bKH+rrKzsftPzB8Evj7W0tBBL2tvb+Xy+ubk5Quj169f79u37+uuvZ8+eraenZ2Vldfz48Zqamvj4ePFO8PyEDwuQD6QcAEjW2dnZ3Nw8YsQIsgORhH/D9vkIpKura2hoaGlpaVxcHLHQ0NAQISSRYGTczaFDhyKEDhw4gIkpKCiQYxcIVlZWLBZL/A40/GLYhAkTEEKlpaVCoXD48OFEK5vNNjY2LikpEe9EIBCgv4cFyAdSDgAky8vLwzCMuFJNpVJ7OwWnZqamphQKRZYnb+Li4saOHfvgwQNiiaOjo76+vvg1/3v37gkEgsmTJ/fZ28iRIxkMRmFhoXxh94hKpc6bN+/WrVvErRa5ubkUCgW/GQ9PhK9fvybW5/F4jY2N+K3SBHwozMzMlBjYYAMpBwASiESipqamrq6uoqKikJAQS0vLoKAgvMnW1raxsTE7O7uzs7O+vl78hzlCyNjYuKampqKigsfjdXZ25ubmqu4maSaTaW1tXV1d3eea+Ok18YvzDAZj06ZNWVlZp0+fbmlpefTo0Zo1a8zNzYODg2XpbcWKFefOnUtOTm5paREKhdXV1Xg+CAwMNDMzk29CnaioqDdv3mzfvr2tra2goCA+Pj4oKMje3h4hZGVlNWvWrOPHj9+6daujo6OqqgqPc9WqVeI94EPh5OQkx9bB/8CAkiCE0tLSyI4CKEqOz/HgwYP4pQImk7lgwYLDhw/j15nt7OzKy8uPHTvGZrMRQqNGjXr27BmGYcHBwTQazcLCgkqlstnshQsXlpeXE729fft21qxZDAbDyspq/fr1YWFhCCFbW9uXL19iGHb//v1Ro0bp6urOmDGjtrb2ypUrLBZr586dH7qbixcvXrx4cZ+rcblcGo3W3t6Ov8zKysJvYDMxMVm3bp3EymFhYT4+PsRLkUgUHx9vZ2dHo9GMjIx8fX2fPn2KN/U5RO/fvw8PD7e0tKRSqUOHDvXz8yspKcEwzBq15tQAAA3ISURBVNfXFyEUHR3dY7QFBQVubm745RmE0LBhwzgcTn5+PrFCfn7+tGnT6HS6ubl5WFgYn88nmhoaGkJCQmxtbel0ur6+vpub208//STRv7e3t4WFhUgkkj5oMo7t4AQpR2kg5QwMavgcg4ODjY2NVbqJPsn4tVhaWkqlUk+dOqWGkGQhFArd3d1PnDih/k03NDQwGIz9+/f3uSakHCngxBoAJNCUaYltbW1jY2NjY2NbW1vJjgUJhcLs7GwejxcYGKj+rcfExDg7O3O5XPVveiCBlEOaf/zjHywWi0KhKPcyqSJiY2MdHBzYbDadTre1tf3222/Fv2hmzpxJ6UbiLtgeic94j9PR0TE1NZ05c2Z8fHxTU5Mq9wkoKiIiwt/fPzAwkPQZPPPy8jIzM3Nzc6U/KqQKCQkJhYWFV65codFoat70AAMphzQpKSnHjx8nO4r/49dff123bl1FRUVDQ8Pu3bsTExP9/f2lv2XGjBl9dkvMeG9gYIBhmEgkqquru3DhgpWVVXh4+Pjx4xWcy0SzbN26NTU19d27d1ZWVhkZGWSHI5Ndu3Zxudw9e/aQG4anp+eZM2eICejUJicn5/3793l5eUZGRmre9MBDJTsA0I/o6+sHBwfj9x198cUXmZmZFy5cqKqqwm8VZTAYLS0t4rNUrV69WnxCMBlRKBRDQ8OZM2fOnDnT29s7ICDA29v72bNnBgYGStyXfmv37t27d+8mO4oP5uXl5eXlRXYU5PDx8fHx8SE7igECjnLIRMyU3k9cunRJ/D5XExMThBDx1PfVq1fF801VVVVxcfHs2bMV2eLixYuDgoLq6uqOHDmiSD8AAI0AKUetMAyLj4+3t7en0+kGBgb4/a+EHids73Oad/y+TyaTyWaznZyc8Ck9lDL3+6tXr3R1da2srHps3bt374YNG4iXcs+ijz+Pkpubi7/sb4MAAFAmsm+ZGziQDDfXRkZGUiiU//zP/2xqampvbz98+DBC6MGDB3jr5s2b6XR6RkZGU1PT1q1btbS0/vjjD/xdCKGbN2++e/eurq7O3d1dT09PIBBgGNba2spms/ft29fR0VFbW7to0aL6+nopXcmura2NxWJxudweW6urqx0cHIRCIbHk0qVLLBYrNja2tw6JazkS8PQwcuTIfjIIsnyOAwDcyKs6MLZSQMpRmj6/qtrb25lM5qeffkoswX+n4ymno6ODyWQGBgYSK9Pp9G+++Qb7+9u2o6MDb8ITVVlZGYZhxcXFCKFLly6Jb0hKV7KLjIwcM2ZMS0tLj63r1q3717/+9UEd9pZyMAzDr+5g/WMQIOUABcHYSgG3D6hPWVlZe3u7p6dnj62yT9guPs27tbW1qanp0qVLN2zYEBQUNHr06A/qqjdZWVkXLly4fv26+MUbQk1NzcWLFyUm2ZVbW1sbhmH4w+f9ZBAUnEFSI+Bzt1y4cIHsQAag6urqfjhJa39Bds4bOFBfv46vXLmCEBJ/cFr8KOf333/v/um4uLhg3X7g47dW//XXX/jL4uLizz//nEqlUiiUgICA9vZ2KV3J4ty5c1OnTn316lVvK3C53Li4OBl7I/R2lIPPl+Xl5YX1j0GQ//8SAH+Do5zewO0D6oMXicKrE3Yn94Tt48eP//nnn2tqasLDw9PS0vbv36/I3O8HDx48ffr0r7/+Kj6Ru7ja2tqzZ89+8803svQmi6tXryKE5s6di/rNIMCJNaCIxYsXK/RfYkCDlKM+jo6OWlpa+fn5PbbKN2F7TU3N48ePEUJDhw7ds2fPpEmTHj9+LF9XGIaFh4c/evQoOztbypwC+/btW7p0qbGx8Qd13pva2toDBw6MGDFi5cqVqB8MAgBApSDlqA8+IW5GRsaJEydaWlqKiorE68BLmbBdipqamtWrVz958kQgEDx48KCystLFxUW+rh4/fvzdd98dP36cRqOJT06zf/9+Yp03b958//33Gzdu7P52WWbRxzCstbUVn4i3vr4+LS3Nzc1NW1s7Ozsbv5ZD+iAAAFSL5EPQAQTJcEKGx+P94x//GDJkiL6+/owZM6KjoxFCI0aMePjwIdbLhO3Sp3mvqKjgcDhGRkba2trDhw+PjIzs6urqrSvpsT169KjHv5D4+HhindDQ0KVLl/b4dimz6F+8eHHChAlMJlNHR0dLSwv9PQHBtGnTYmNj3759K74yuYOAwR1rQGEwtlJQMLheqiQUCiUtLU2OCWBAvzJIPkd89rz09HSyAxmAYGylgBNrAAAA1ARSzmDx5MmT7qUHCKQUIAEADDaQcgaLsWPHSjnBev78ebIDBBrsxo0bERER4oWRvvrqK/EVvLy8WCyWtrb2+PHj8SexyCISiQ4cOMDhcMQXXrx4cd++fZpSN0+jQcoBAChk+/btSUlJW7duJQojDRky5PTp05cvXybWuX79enp6+vz580tKSiZNmkRWqKWlpR9//HFoaCgxPzpuwYIFDAbD09OzubmZrNgGCUg5AKhVR0eHxE/s/tCV3Pbu3Xv+/PkLFy6Iz42UlJSkpaUVHBxMeiFRcQ8fPtyyZcuaNWucnZ27t27YsGHixInz5s3r6upSf2yDB6QcANTqxIkTdXV1/a0r+ZSVlUVFRe3YsQOfWYPA4XBCQkJevXq1efNmsmLrbuLEiZmZmUuWLKHT6T2uEBMTU1hYmJiYqObABhVIOQB8MAzDEhISxo0bR6fTjYyMFi5cSEwYyuVydXR0iGLJa9eu1dPTo1AoDQ0NCKGQkJBNmzaVl5dTKBRbW9ukpCQGg2Fqarp69Wpzc3MGg8HhcO7duydHV0iBkkVyS0pKwjBswYIF3Zt27tw5ZsyYlJSUGzdu9PheKWPYZ3kkFVVCMjIy8vDwSExMhEdHVEgdD/8MDmhwPEI44MnyOUZHR+vo6Jw6daq5ubmoqGjSpEkmJia1tbV465IlS8zMzIiV8Sm38RI+GIb5+fnZ2NgQrcHBwXp6eo8fP+bz+SUlJVOnTmWxWC9fvpSjqz5LFolTyuOK1tbWDg4OEgttbGxevHiBYdidO3e0tLRGjx7d2tqKYVhubq6Pjw+xmvQxlFIeCVO4HNT06dMnTpzYY1NERAQSK2ElH3gUVAo4ygHgw3R0dCQkJCxatGjp0qUGBgZOTk5HjhxpaGgQn77og1CpVPzHvoODQ3JyMo/HS01NlaMfb2/vlpaWqKgo+cL4UG1tbS9evLCxseltBVdX140bN1ZUVGzZskWiScYx5HA4bDZ76NChgYGBbW1tL1++RAjx+fzk5GRfX18/Pz9DQ8Nt27bRaDT5Rqw7Ozs7hFBvM3EAxUHKAeDDlJSUtLa2TpkyhVgydepUHR0d4oSYIqZMmcJkMj+ouBFZ6urqMAzDJyLqzc6dO+3t7Q8fPnz79m3x5R86huLlkRQvByUFvjtv3rxRSm+gO0g5AHwY/D5aicm2DQ0NeTyeUvqn0+n19fVK6Uql+Hw+Qqi3S/E4BoORmppKoVBWrlzZ0dFBLFdkDNva2hBC27ZtIx5krqyslLjpWW66urro710DqgApB4APY2hoiBCS+HJsbm5WSiHIzs5OZXWlavi3c5+PT7q6uoaGhpaWlsbFxRELFRlDRSoh9UkgEKC/dw2oAqQcAD6Mo6Ojvr7+n3/+SSy5d++eQCCYPHky/pJKpeKngOSQl5eHYZiLi4viXamaqakphUKR5cmbuLi4sWPHPnjwgFjS5xhKodJKSPjumJmZqaJzgCDlAPChGAzGpk2bsrKyTp8+3dLS8ujRozVr1pibmwcHB+Mr2NraNjY2Zmdnd3Z21tfXV1ZWir/d2Ni4pqamoqKCx+Ph6UQkEjU1NXV1dRUVFYWEhFhaWgYFBcnRlSwli5SIyWRaW1tXV1f3uSZ+ek1bW1t8ifQxlN5bb5WQAgMDzczMFJlQB98dJycnuXsAfSDnRrmBCMFN0gOCLJ+jSCSKj4+3s7Oj0WhGRka+vr5Pnz4lWt++fTtr1iwGg2FlZbV+/fqwsDCEkK2tLX7r8/3790eNGqWrqztjxoza2trg4GAajWZhYUGlUtls9sKFC8vLy+XrSkrJou6UciMvl8ul0Wjt7e34y6ysLPwGNhMTk3Xr1kmsHBYWJn6TtJQxlF4eCeu9EpKvry9CKDo6usdoCwoK3NzczM3N8a++YcOGcTic/Px88XW8vb0tLCzwKoJyg5ukpYCUozSQcgYGNX+OwcHBxsbGatscQSlfi6WlpVQq9dSpU0oJSXFCodDd3f3EiRPyvb2hoYHBYOzfv1/BMCDlSAEn1gAgmeZOYGxraxsbGxsbG9va2kp2LEgoFGZnZ/N4PLkrccTExDg7O3O5XOUGBsRBygEAyC8iIsLf3z8wMJD0GTzz8vIyMzNzc3OlPyrUm4SEhMLCwitXrtBoNKXHBgiQcgAgzdatW1NTU9+9e2dlZZWRkUF2OHLatWsXl8vds2cPuWF4enqeOXOGmJLug+Tk5Lx//z4vL8/IyEjpgQFxVLIDAGDw2r179+7du8mOQgm8vLy8vLzIjkJ+Pj4+Pj4+ZEcxKMBRDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ISCQf07JaFQKC4uLhoxISOQIiMjYzB8jnfv3kUIEZO5ASW6e/eui4tLeno62YH0R5BylMbf35/sEAAA/QI+fzbZUfRHkHIAAACoCVzLAQAAoCaQcgAAAKgJpBwAAABqAikHAACAmvx/BY2qLfciv0cAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7X1WpoaJ8vZ",
        "outputId": "5cf50d05-5574-4e91-dad9-5e0ae71c6bc8"
      },
      "source": [
        "# 3.5 Compile model\r\n",
        "model.compile(loss = \"mean_squared_error\")\r\n",
        "history = model.fit(\r\n",
        "                    X_train,\r\n",
        "                    y_train,\r\n",
        "                    epochs = 10,\r\n",
        "                    verbose = 1\r\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5214\n",
            "Epoch 2/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4380\n",
            "Epoch 3/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5411\n",
            "Epoch 4/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4837\n",
            "Epoch 5/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4958\n",
            "Epoch 6/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4681\n",
            "Epoch 7/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4758\n",
            "Epoch 8/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4888\n",
            "Epoch 9/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4339\n",
            "Epoch 10/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QQ8xAgGLTOo",
        "outputId": "e958e9a0-cc59-45e4-9cd2-0388954fbfec"
      },
      "source": [
        "3.6 model.evaluate(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129/129 [==============================] - 0s 863us/step - loss: 2.4378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4377660751342773"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYuF2BagAAYt"
      },
      "source": [
        "# Wide and Deep Network--IInd version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYrB04GJNwrN"
      },
      "source": [
        "# 4.0 We have two inputs\r\n",
        "inputsA = tf.keras.Input(shape = X_train[:,:4].shape[1:])\r\n",
        "inputsB = tf.keras.Input(shape = X_train[:,1:8].shape[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1iVJhhcRTIi"
      },
      "source": [
        "# 4.1 One arm of network\r\n",
        "x = layers.Dense(100, activation = 'relu')(inputsB)\r\n",
        "x = layers.Dense(100,activation= 'relu')(x)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6iLjl8gSa69"
      },
      "source": [
        "# 4.2 Concatenate one input with output of another arm\r\n",
        "concat = layers.concatenate([x,inputsA])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG3SQHycS1Tu"
      },
      "source": [
        "# 4.3 Output layer\r\n",
        "out = layers.Dense(1,activation = 'sigmoid')(concat)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x90xNBumS_bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d320e37-778a-4ed9-aebf-19ce933f06eb"
      },
      "source": [
        "# 4.4 Create model and show summary\r\n",
        "model2 = Model(inputs = [inputsA,inputsB], outputs = [out])\r\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_18 (InputLayer)           [(None, 7)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 100)          800         input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 100)          10100       dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_17 (InputLayer)           [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 104)          0           dense_29[0][0]                   \n",
            "                                                                 input_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 1)            105         concatenate_9[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 11,005\n",
            "Trainable params: 11,005\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWwLnTskTPDi",
        "outputId": "5f8a2cf2-8fd6-492b-c513-1514f58fd354"
      },
      "source": [
        "# 4.5 Compile model\r\n",
        "model2.compile(\r\n",
        "               optimizer='rmsprop', \r\n",
        "               loss = 'mean_squared_error'\r\n",
        "               )\r\n",
        "\r\n",
        "# 4.6 Train the model now.\r\n",
        "#     Note the two train inputs\r\n",
        "model2.fit(\r\n",
        "            [X_train[:,:4], X_train[:,1:8]],\r\n",
        "            y_train,\r\n",
        "            epochs = 100\r\n",
        "           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.6578\n",
            "Epoch 2/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4948\n",
            "Epoch 3/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4901\n",
            "Epoch 4/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4797\n",
            "Epoch 5/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4541\n",
            "Epoch 6/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4691\n",
            "Epoch 7/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4731\n",
            "Epoch 8/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4998\n",
            "Epoch 9/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5091\n",
            "Epoch 10/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4542\n",
            "Epoch 11/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4512\n",
            "Epoch 12/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4789\n",
            "Epoch 13/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4811\n",
            "Epoch 14/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4869\n",
            "Epoch 15/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5363\n",
            "Epoch 16/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4781\n",
            "Epoch 17/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5037\n",
            "Epoch 18/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4610\n",
            "Epoch 19/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4809\n",
            "Epoch 20/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5035\n",
            "Epoch 21/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4914\n",
            "Epoch 22/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4429\n",
            "Epoch 23/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4965\n",
            "Epoch 24/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4645\n",
            "Epoch 25/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4459\n",
            "Epoch 26/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4714\n",
            "Epoch 27/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4697\n",
            "Epoch 28/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5143\n",
            "Epoch 29/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4575\n",
            "Epoch 30/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5729\n",
            "Epoch 31/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4698\n",
            "Epoch 32/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5025\n",
            "Epoch 33/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5378\n",
            "Epoch 34/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4445\n",
            "Epoch 35/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4885\n",
            "Epoch 36/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4371\n",
            "Epoch 37/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4203\n",
            "Epoch 38/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4827\n",
            "Epoch 39/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5119\n",
            "Epoch 40/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4790\n",
            "Epoch 41/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4559\n",
            "Epoch 42/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4504\n",
            "Epoch 43/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5163\n",
            "Epoch 44/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4440\n",
            "Epoch 45/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4931\n",
            "Epoch 46/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5242\n",
            "Epoch 47/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5359\n",
            "Epoch 48/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4938\n",
            "Epoch 49/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5103\n",
            "Epoch 50/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4931\n",
            "Epoch 51/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5341\n",
            "Epoch 52/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5141\n",
            "Epoch 53/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5196\n",
            "Epoch 54/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4987\n",
            "Epoch 55/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5157\n",
            "Epoch 56/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5070\n",
            "Epoch 57/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4943\n",
            "Epoch 58/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4640\n",
            "Epoch 59/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5108\n",
            "Epoch 60/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5197\n",
            "Epoch 61/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4418\n",
            "Epoch 62/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4828\n",
            "Epoch 63/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4737\n",
            "Epoch 64/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4872\n",
            "Epoch 65/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4442\n",
            "Epoch 66/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4393\n",
            "Epoch 67/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4949\n",
            "Epoch 68/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4189\n",
            "Epoch 69/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4954\n",
            "Epoch 70/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4745\n",
            "Epoch 71/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4598\n",
            "Epoch 72/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4241\n",
            "Epoch 73/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5375\n",
            "Epoch 74/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5019\n",
            "Epoch 75/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4823\n",
            "Epoch 76/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5538\n",
            "Epoch 77/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4443\n",
            "Epoch 78/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4503\n",
            "Epoch 79/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4703\n",
            "Epoch 80/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5018\n",
            "Epoch 81/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4757\n",
            "Epoch 82/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4117\n",
            "Epoch 83/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4730\n",
            "Epoch 84/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5220\n",
            "Epoch 85/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4587\n",
            "Epoch 86/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4905\n",
            "Epoch 87/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5263\n",
            "Epoch 88/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4712\n",
            "Epoch 89/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4886\n",
            "Epoch 90/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4464\n",
            "Epoch 91/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4934\n",
            "Epoch 92/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5552\n",
            "Epoch 93/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4752\n",
            "Epoch 94/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5213\n",
            "Epoch 95/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4983\n",
            "Epoch 96/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4724\n",
            "Epoch 97/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4810\n",
            "Epoch 98/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4708\n",
            "Epoch 99/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4802\n",
            "Epoch 100/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe6993e0668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "F5MvbvlUTYd3",
        "outputId": "e3d494d5-8388-4ab8-8504-8e0cc280d043"
      },
      "source": [
        "# 4.7 Plot our model\r\n",
        "plot_model(model2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqjA96pAERaO"
      },
      "source": [
        "# Two inputs and two outputs model--IIIrd ver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Bg8Lm2Ei0y",
        "outputId": "c24e307a-963c-4eed-94e5-830193b74428"
      },
      "source": [
        "# 5.0 We have two inputs.\r\n",
        "#     To distiguish them, we give names to each\r\n",
        "inputsA = tf.keras.Input(\r\n",
        "                          shape = X_train[:,:4].shape[1:],\r\n",
        "                          name = \"in_a\"\r\n",
        "                         )\r\n",
        "\r\n",
        "# 5.1\r\n",
        "inputsB = tf.keras.Input(\r\n",
        "                          shape = X_train[:,1:8].shape[1:],\r\n",
        "                          name = \"in_b\"\r\n",
        "                        )\r\n",
        "\r\n",
        "\r\n",
        "# 5.2 One arm of network\r\n",
        "x = layers.Dense(100, activation = 'relu')(inputsB)\r\n",
        "x = layers.Dense(100,activation= 'relu')(x)\r\n",
        "\r\n",
        "# 5.3 Concatenate an input with output of one arm\r\n",
        "concat = layers.concatenate([x,inputsA])\r\n",
        "\r\n",
        "# 5.4 Output layers\r\n",
        "#     We have two output layers. To distiguish them, we give names to each\r\n",
        "out_x = layers.Dense(1,activation = 'sigmoid' , name = \"out_a\")(concat)\r\n",
        "out_y = layers.Dense(1,activation = 'sigmoid', name = \"out_b\")(x)\r\n",
        "\r\n",
        "# 5.5 Create model and show summary\r\n",
        "#     While outputs are two, model is one\r\n",
        "main_model = Model(inputs = [inputsA,inputsB], outputs = [out_x, out_y])\r\n",
        "main_model.summary()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "in_b (InputLayer)               [(None, 7)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100)          800         in_b[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          10100       dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "in_a (InputLayer)               [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 104)          0           dense_1[0][0]                    \n",
            "                                                                 in_a[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "out_a (Dense)                   (None, 1)            105         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "out_b (Dense)                   (None, 1)            101         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 11,106\n",
            "Trainable params: 11,106\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "BOmnKemAlzJ6",
        "outputId": "c8cc27f1-acfa-4f2c-cb8e-be25ed6690ea"
      },
      "source": [
        "# 5.6 Plot the model now\r\n",
        "plot_model(main_model, show_shapes = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvp16EsGHulp",
        "outputId": "c32df922-e7a4-44e7-ebe7-37f436bba0ea"
      },
      "source": [
        "help(Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Model in module tensorflow.python.keras.engine.training:\n",
            "\n",
            "class Model(tensorflow.python.keras.engine.base_layer.Layer, tensorflow.python.keras.utils.version_utils.ModelVersionSelector)\n",
            " |  `Model` groups layers into an object with training and inference features.\n",
            " |  \n",
            " |  Arguments:\n",
            " |      inputs: The input(s) of the model: a `keras.Input` object or list of\n",
            " |          `keras.Input` objects.\n",
            " |      outputs: The output(s) of the model. See Functional API example below.\n",
            " |      name: String, the name of the model.\n",
            " |  \n",
            " |  There are two ways to instantiate a `Model`:\n",
            " |  \n",
            " |  1 - With the \"Functional API\", where you start from `Input`,\n",
            " |  you chain layer calls to specify the model's forward pass,\n",
            " |  and finally you create your model from inputs and outputs:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  inputs = tf.keras.Input(shape=(3,))\n",
            " |  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n",
            " |  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
            " |  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
            " |  ```\n",
            " |  \n",
            " |  2 - By subclassing the `Model` class: in that case, you should define your\n",
            " |  layers in `__init__` and you should implement the model's forward pass\n",
            " |  in `call`.\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super(MyModel, self).__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |  \n",
            " |    def call(self, inputs):\n",
            " |      x = self.dense1(inputs)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  If you subclass `Model`, you can optionally have\n",
            " |  a `training` argument (boolean) in `call`, which you can use to specify\n",
            " |  a different behavior in training and inference:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super(MyModel, self).__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |      self.dropout = tf.keras.layers.Dropout(0.5)\n",
            " |  \n",
            " |    def call(self, inputs, training=False):\n",
            " |      x = self.dense1(inputs)\n",
            " |      if training:\n",
            " |        x = self.dropout(x, training=training)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  Once the model is created, you can config the model with losses and metrics\n",
            " |  with `model.compile()`, train the model with `model.fit()`, or use the model\n",
            " |  to do prediction with `model.predict()`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Model\n",
            " |      tensorflow.python.keras.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
            " |      tensorflow.python.keras.utils.version_utils.ModelVersionSelector\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *args, **kwargs)\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Builds the model based on input shapes received.\n",
            " |      \n",
            " |      This is to be used for subclassed models, which do not know at instantiation\n",
            " |      time what their inputs look like.\n",
            " |      \n",
            " |      This method only exists for users who want to call `model.build()` in a\n",
            " |      standalone way (as a substitute for calling the model on real data to\n",
            " |      build it). It will never be called by the framework (and thus it will\n",
            " |      never throw unexpected errors in an unrelated workflow).\n",
            " |      \n",
            " |      Args:\n",
            " |       input_shape: Single tuple, TensorShape, or list/dict of shapes, where\n",
            " |           shapes are tuples, integers, or TensorShapes.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError:\n",
            " |          1. In case of invalid user-provided data (not of type tuple,\n",
            " |             list, TensorShape, or dict).\n",
            " |          2. If the model requires call arguments that are agnostic\n",
            " |             to the input shapes (positional or kwarg in call signature).\n",
            " |          3. If not all layers were properly built.\n",
            " |          4. If float type inputs are not supported within the layers.\n",
            " |      \n",
            " |        In each of these cases, the user should build their model by calling it\n",
            " |        on real tensor data.\n",
            " |  \n",
            " |  call(self, inputs, training=None, mask=None)\n",
            " |      Calls the model on new inputs.\n",
            " |      \n",
            " |      In this case `call` just reapplies\n",
            " |      all ops in the graph to the new inputs\n",
            " |      (e.g. build a new computational graph from the provided inputs).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: A tensor or list of tensors.\n",
            " |          training: Boolean or boolean scalar tensor, indicating whether to run\n",
            " |            the `Network` in training mode or inference mode.\n",
            " |          mask: A mask or list of masks. A mask can be\n",
            " |              either a tensor or None (no mask).\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor if there is a single output, or\n",
            " |          a list of tensors if there are more than one outputs.\n",
            " |  \n",
            " |  compile(self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, **kwargs)\n",
            " |      Configures the model for training.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          optimizer: String (name of optimizer) or optimizer instance. See\n",
            " |            `tf.keras.optimizers`.\n",
            " |          loss: String (name of objective function), objective function or\n",
            " |            `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n",
            " |            function is any callable with the signature `loss = fn(y_true,\n",
            " |            y_pred)`, where y_true = ground truth values with shape =\n",
            " |            `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse\n",
            " |            categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.\n",
            " |            y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It\n",
            " |            returns a weighted loss float tensor. If a custom `Loss` instance is\n",
            " |            used and reduction is set to NONE, return value has the shape\n",
            " |            [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values;\n",
            " |            otherwise, it is a scalar. If the model has multiple outputs, you can\n",
            " |            use a different loss on each output by passing a dictionary or a list\n",
            " |            of losses. The loss value that will be minimized by the model will\n",
            " |            then be the sum of all individual losses.\n",
            " |          metrics: List of metrics to be evaluated by the model during training\n",
            " |            and testing. Each of this can be a string (name of a built-in\n",
            " |            function), function or a `tf.keras.metrics.Metric` instance. See\n",
            " |            `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n",
            " |            function is any callable with the signature `result = fn(y_true,\n",
            " |            y_pred)`. To specify different metrics for different outputs of a\n",
            " |            multi-output model, you could also pass a dictionary, such as\n",
            " |              `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n",
            " |                You can also pass a list (len = len(outputs)) of lists of metrics\n",
            " |                such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n",
            " |                `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
            " |                strings 'accuracy' or 'acc', we convert this to one of\n",
            " |                `tf.keras.metrics.BinaryAccuracy`,\n",
            " |                `tf.keras.metrics.CategoricalAccuracy`,\n",
            " |                `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n",
            " |                function used and the model output shape. We do a similar\n",
            " |                conversion for the strings 'crossentropy' and 'ce' as well.\n",
            " |          loss_weights: Optional list or dictionary specifying scalar coefficients\n",
            " |            (Python floats) to weight the loss contributions of different model\n",
            " |            outputs. The loss value that will be minimized by the model will then\n",
            " |            be the *weighted sum* of all individual losses, weighted by the\n",
            " |            `loss_weights` coefficients.\n",
            " |              If a list, it is expected to have a 1:1 mapping to the model's\n",
            " |                outputs. If a dict, it is expected to map output names (strings)\n",
            " |                to scalar coefficients.\n",
            " |          weighted_metrics: List of metrics to be evaluated and weighted by\n",
            " |            sample_weight or class_weight during training and testing.\n",
            " |          run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
            " |            logic will not be wrapped in a `tf.function`. Recommended to leave\n",
            " |            this as `None` unless your `Model` cannot be run inside a\n",
            " |            `tf.function`.\n",
            " |          steps_per_execution: Int. Defaults to 1. The number of batches to\n",
            " |            run during each `tf.function` call. Running multiple batches\n",
            " |            inside a single `tf.function` call can greatly improve performance\n",
            " |            on TPUs or small models with a large Python overhead.\n",
            " |            At most, one full epoch will be run each\n",
            " |            execution. If a number larger than the size of the epoch is passed,\n",
            " |            the execution will be truncated to the size of the epoch.\n",
            " |            Note that if `steps_per_execution` is set to `N`,\n",
            " |            `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n",
            " |            will only be called every `N` batches\n",
            " |            (i.e. before/after each `tf.function` execution).\n",
            " |          **kwargs: Arguments supported for backwards compatibility only.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: In case of invalid arguments for\n",
            " |              `optimizer`, `loss` or `metrics`.\n",
            " |  \n",
            " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False)\n",
            " |      Returns the loss value & metrics values for the model in test mode.\n",
            " |      \n",
            " |      Computation is done in batches (see the `batch_size` arg.)\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely). If\n",
            " |            `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\n",
            " |            should not be specified (since targets will be obtained from the\n",
            " |            iterator/dataset).\n",
            " |          batch_size: Integer or `None`. Number of samples per batch of\n",
            " |            computation. If unspecified, `batch_size` will default to 32. Do not\n",
            " |            specify the `batch_size` if your data is in the form of a dataset,\n",
            " |            generators, or `keras.utils.Sequence` instances (since they generate\n",
            " |            batches).\n",
            " |          verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n",
            " |          sample_weight: Optional Numpy array of weights for the test samples,\n",
            " |            used for weighting the loss function. You can either pass a flat (1D)\n",
            " |            Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples), or in the case of\n",
            " |                temporal data, you can pass a 2D array with shape `(samples,\n",
            " |                sequence_length)`, to apply a different weight to every timestep\n",
            " |                of every sample. This argument is not supported when `x` is a\n",
            " |                dataset, instead pass sample weights as the third element of `x`.\n",
            " |          steps: Integer or `None`. Total number of steps (batches of samples)\n",
            " |            before declaring the evaluation round finished. Ignored with the\n",
            " |            default value of `None`. If x is a `tf.data` dataset and `steps` is\n",
            " |            None, 'evaluate' will run until the dataset is exhausted. This\n",
            " |            argument is not supported with array inputs.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances. List of\n",
            " |            callbacks to apply during evaluation. See\n",
            " |            [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |            input only. Maximum size for the generator queue. If unspecified,\n",
            " |            `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |            only. Maximum number of processes to spin up when using process-based\n",
            " |            threading. If unspecified, `workers` will default to 1. If 0, will\n",
            " |            execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |            threading. If unspecified, `use_multiprocessing` will default to\n",
            " |            `False`. Note that because this implementation relies on\n",
            " |            multiprocessing, you should not pass non-picklable arguments to the\n",
            " |            generator as they can't be passed easily to children processes.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.evaluate` is wrapped in `tf.function`.\n",
            " |          ValueError: in case of invalid arguments.\n",
            " |  \n",
            " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Evaluates the model on a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.evaluate` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given below.\n",
            " |          y: Target data. Like the input data `x`,\n",
            " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
            " |            tensor targets, or inversely). If `x` is a dataset, generator,\n",
            " |            or `keras.utils.Sequence` instance, `y` should\n",
            " |            not be specified (since targets will be obtained from `x`).\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per gradient update.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          epochs: Integer. Number of epochs to train the model.\n",
            " |              An epoch is an iteration over the entire `x` and `y`\n",
            " |              data provided.\n",
            " |              Note that in conjunction with `initial_epoch`,\n",
            " |              `epochs` is to be understood as \"final epoch\".\n",
            " |              The model is not trained for a number of iterations\n",
            " |              given by `epochs`, but merely until the epoch\n",
            " |              of index `epochs` is reached.\n",
            " |          verbose: 0, 1, or 2. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            " |              Note that the progress bar is not particularly useful when\n",
            " |              logged to a file, so verbose=2 is recommended when not running\n",
            " |              interactively (eg, in a production environment).\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training.\n",
            " |              See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
            " |              and `tf.keras.callbacks.History` callbacks are created automatically\n",
            " |              and need not be passed into `model.fit`.\n",
            " |              `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
            " |              `verbose` argument to `model.fit`.\n",
            " |          validation_split: Float between 0 and 1.\n",
            " |              Fraction of the training data to be used as validation data.\n",
            " |              The model will set apart this fraction of the training data,\n",
            " |              will not train on it, and will evaluate\n",
            " |              the loss and any model metrics\n",
            " |              on this data at the end of each epoch.\n",
            " |              The validation data is selected from the last samples\n",
            " |              in the `x` and `y` data provided, before shuffling. This argument is\n",
            " |              not supported when `x` is a dataset, generator or\n",
            " |             `keras.utils.Sequence` instance.\n",
            " |          validation_data: Data on which to evaluate\n",
            " |              the loss and any model metrics at the end of each epoch.\n",
            " |              The model will not be trained on this data. Thus, note the fact\n",
            " |              that the validation loss of data provided using `validation_split`\n",
            " |              or `validation_data` is not affected by regularization layers like\n",
            " |              noise and dropout.\n",
            " |              `validation_data` will override `validation_split`.\n",
            " |              `validation_data` could be:\n",
            " |                - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
            " |                - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
            " |                - dataset\n",
            " |              For the first two cases, `batch_size` must be provided.\n",
            " |              For the last case, `validation_steps` could be provided.\n",
            " |              Note that `validation_data` does not support all the data types that\n",
            " |              are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
            " |          shuffle: Boolean (whether to shuffle the training data\n",
            " |              before each epoch) or str (for 'batch'). This argument is ignored\n",
            " |              when `x` is a generator. 'batch' is a special option for dealing\n",
            " |              with the limitations of HDF5 data; it shuffles in batch-sized\n",
            " |              chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers)\n",
            " |              to a weight (float) value, used for weighting the loss function\n",
            " |              (during training only).\n",
            " |              This can be useful to tell the model to\n",
            " |              \"pay more attention\" to samples from\n",
            " |              an under-represented class.\n",
            " |          sample_weight: Optional Numpy array of weights for\n",
            " |              the training samples, used for weighting the loss function\n",
            " |              (during training only). You can either pass a flat (1D)\n",
            " |              Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples),\n",
            " |              or in the case of temporal data,\n",
            " |              you can pass a 2D array with shape\n",
            " |              `(samples, sequence_length)`,\n",
            " |              to apply a different weight to every timestep of every sample. This\n",
            " |              argument is not supported when `x` is a dataset, generator, or\n",
            " |             `keras.utils.Sequence` instance, instead provide the sample_weights\n",
            " |              as the third element of `x`.\n",
            " |          initial_epoch: Integer.\n",
            " |              Epoch at which to start training\n",
            " |              (useful for resuming a previous training run).\n",
            " |          steps_per_epoch: Integer or `None`.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              before declaring one epoch finished and starting the\n",
            " |              next epoch. When training with input tensors such as\n",
            " |              TensorFlow data tensors, the default `None` is equal to\n",
            " |              the number of samples in your dataset divided by\n",
            " |              the batch size, or 1 if that cannot be determined. If x is a\n",
            " |              `tf.data` dataset, and 'steps_per_epoch'\n",
            " |              is None, the epoch will run until the input dataset is exhausted.\n",
            " |              When passing an infinitely repeating dataset, you must specify the\n",
            " |              `steps_per_epoch` argument. This argument is not supported with\n",
            " |              array inputs.\n",
            " |          validation_steps: Only relevant if `validation_data` is provided and\n",
            " |              is a `tf.data` dataset. Total number of steps (batches of\n",
            " |              samples) to draw before stopping when performing validation\n",
            " |              at the end of every epoch. If 'validation_steps' is None, validation\n",
            " |              will run until the `validation_data` dataset is exhausted. In the\n",
            " |              case of an infinitely repeated dataset, it will run into an\n",
            " |              infinite loop. If 'validation_steps' is specified and only part of\n",
            " |              the dataset will be consumed, the evaluation will start from the\n",
            " |              beginning of the dataset at each epoch. This ensures that the same\n",
            " |              validation samples are used every time.\n",
            " |          validation_batch_size: Integer or `None`.\n",
            " |              Number of samples per validation batch.\n",
            " |              If unspecified, will default to `batch_size`.\n",
            " |              Do not specify the `validation_batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          validation_freq: Only relevant if validation data is provided. Integer\n",
            " |              or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
            " |              If an integer, specifies how many training epochs to run before a\n",
            " |              new validation run is performed, e.g. `validation_freq=2` runs\n",
            " |              validation every 2 epochs. If a Container, specifies the epochs on\n",
            " |              which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            " |              validation at the end of the 1st, 2nd, and 10th epochs.\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up\n",
            " |              when using process-based threading. If unspecified, `workers`\n",
            " |              will default to 1. If 0, will execute the generator on the main\n",
            " |              thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      Unpacking behavior for iterator-like inputs:\n",
            " |          A common pattern is to pass a tf.data.Dataset, generator, or\n",
            " |        tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            " |        yield not only features (x) but optionally targets (y) and sample weights.\n",
            " |        Keras requires that the output of such iterator-likes be unambiguous. The\n",
            " |        iterator should return a tuple of length 1, 2, or 3, where the optional\n",
            " |        second and third elements will be used for y and sample_weight\n",
            " |        respectively. Any other type provided will be wrapped in a length one\n",
            " |        tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
            " |        should still adhere to the top-level tuple structure.\n",
            " |        e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            " |        features, targets, and weights from the keys of a single dict.\n",
            " |          A notable unsupported data type is the namedtuple. The reason is that\n",
            " |        it behaves like both an ordered datatype (tuple) and a mapping\n",
            " |        datatype (dict). So given a namedtuple of the form:\n",
            " |            `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            " |        it is ambiguous whether to reverse the order of the elements when\n",
            " |        interpreting the value. Even worse is a tuple of the form:\n",
            " |            `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            " |        where it is unclear if the tuple was intended to be unpacked into x, y,\n",
            " |        and sample_weight or passed through as a single element to `x`. As a\n",
            " |        result the data processing code will simply raise a ValueError if it\n",
            " |        encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          A `History` object. Its `History.history` attribute is\n",
            " |          a record of training loss values and metrics values\n",
            " |          at successive epochs, as well as validation loss values\n",
            " |          and validation metrics values (if applicable).\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: 1. If the model was never compiled or,\n",
            " |          2. If `model.fit` is  wrapped in `tf.function`.\n",
            " |      \n",
            " |          ValueError: In case of mismatch between the provided input data\n",
            " |              and what the model expects or when the input data is empty.\n",
            " |  \n",
            " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
            " |      Fits the model on data yielded batch-by-batch by a Python generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.fit` now supports generators, so there is no longer any need to use\n",
            " |        this endpoint.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  get_layer(self, name=None, index=None)\n",
            " |      Retrieves a layer based on either its name (unique) or index.\n",
            " |      \n",
            " |      If `name` and `index` are both provided, `index` will take precedence.\n",
            " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          name: String, name of layer.\n",
            " |          index: Integer, index of layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: In case of invalid layer name or index.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Retrieves the weights of the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A flat list of Numpy arrays.\n",
            " |  \n",
            " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None)\n",
            " |      Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n",
            " |      \n",
            " |      If `by_name` is False weights are loaded based on the network's\n",
            " |      topology. This means the architecture should be the same as when the weights\n",
            " |      were saved.  Note that layers that don't have weights are not taken into\n",
            " |      account in the topological ordering, so adding or removing layers is fine as\n",
            " |      long as they don't have weights.\n",
            " |      \n",
            " |      If `by_name` is True, weights are loaded into layers only if they share the\n",
            " |      same name. This is useful for fine-tuning or transfer-learning models where\n",
            " |      some of the layers have changed.\n",
            " |      \n",
            " |      Only topological loading (`by_name=False`) is supported when loading weights\n",
            " |      from the TensorFlow format. Note that topological loading differs slightly\n",
            " |      between TensorFlow and HDF5 formats for user-defined classes inheriting from\n",
            " |      `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n",
            " |      TensorFlow format loads based on the object-local names of attributes to\n",
            " |      which layers are assigned in the `Model`'s constructor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String, path to the weights file to load. For weight files in\n",
            " |              TensorFlow format, this is the file prefix (the same as was passed\n",
            " |              to `save_weights`).\n",
            " |          by_name: Boolean, whether to load weights by name or by topological\n",
            " |              order. Only topological loading is supported for weight files in\n",
            " |              TensorFlow format.\n",
            " |          skip_mismatch: Boolean, whether to skip loading of layers where there is\n",
            " |              a mismatch in the number of weights, or a mismatch in the shape of\n",
            " |              the weight (only valid when `by_name=True`).\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for loading weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          When loading a weight file in TensorFlow format, returns the same status\n",
            " |          object as `tf.train.Checkpoint.restore`. When graph building, restore\n",
            " |          ops are run automatically as soon as the network is built (on first call\n",
            " |          for user-defined classes inheriting from `Model`, immediately if it is\n",
            " |          already built).\n",
            " |      \n",
            " |          When loading weights in HDF5 format, returns `None`.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If h5py is not available and the weight file is in HDF5\n",
            " |              format.\n",
            " |          ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n",
            " |            `False`.\n",
            " |  \n",
            " |  make_predict_function(self)\n",
            " |      Creates a function that executes one step of inference.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.predict` and `Model.predict_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.predict_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.predict` or\n",
            " |      `Model.predict_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return the outputs of the `Model`.\n",
            " |  \n",
            " |  make_test_function(self)\n",
            " |      Creates a function that executes one step of evaluation.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.evaluate` and `Model.test_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.test_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.evaluate` or\n",
            " |      `Model.test_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_test_batch_end`.\n",
            " |  \n",
            " |  make_train_function(self)\n",
            " |      Creates a function that executes one step of training.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.fit` and `Model.train_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual training\n",
            " |      logic to `Model.train_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.fit` or\n",
            " |      `Model.train_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Generates output predictions for the input samples.\n",
            " |      \n",
            " |      Computation is done in batches. This method is designed for performance in\n",
            " |      large scale inputs. For small amount of inputs that fit in one batch,\n",
            " |      directly using `__call__` is recommended for faster execution, e.g.,\n",
            " |      `model(x)`, or `model(x, training=False)` if you have layers such as\n",
            " |      `tf.keras.layers.BatchNormalization` that behaves differently during\n",
            " |      inference. Also, note the fact that test loss is not affected by\n",
            " |      regularization layers like noise and dropout.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input samples. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A `tf.data` dataset.\n",
            " |            - A generator or `keras.utils.Sequence` instance.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per batch.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of dataset, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          verbose: Verbosity mode, 0 or 1.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              before declaring the prediction round finished.\n",
            " |              Ignored with the default value of `None`. If x is a `tf.data`\n",
            " |              dataset and `steps` is None, `predict` will\n",
            " |              run until the input dataset is exhausted.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during prediction.\n",
            " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up when using\n",
            " |              process-based threading. If unspecified, `workers` will default\n",
            " |              to 1. If 0, will execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`. Note that Model.predict uses the same interpretation rules as\n",
            " |      `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n",
            " |      three methods.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of mismatch between the provided\n",
            " |              input data and the model's expectations,\n",
            " |              or in case a stateful model receives a number of samples\n",
            " |              that is not a multiple of the batch size.\n",
            " |  \n",
            " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Generates predictions for the input samples from a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.predict` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  predict_on_batch(self, x)\n",
            " |      Returns predictions for a single batch of samples.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
            " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
            " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict_on_batch` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of mismatch between given number of inputs and\n",
            " |            expectations of the model.\n",
            " |  \n",
            " |  predict_step(self, data)\n",
            " |      The logic for one inference step.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.make_predict_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of inference.\n",
            " |      This typically includes the forward pass.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_predict_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The result of one inference step, typically the output of calling the\n",
            " |        `Model` on data.\n",
            " |  \n",
            " |  reset_metrics(self)\n",
            " |      Resets the state of all the metrics in the model.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> _ = model.fit(x, y, verbose=0)\n",
            " |      >>> assert all(float(m.result()) for m in model.metrics)\n",
            " |      \n",
            " |      >>> model.reset_metrics()\n",
            " |      >>> assert all(float(m.result()) == 0 for m in model.metrics)\n",
            " |  \n",
            " |  reset_states(self)\n",
            " |  \n",
            " |  save(self, filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None, save_traces=True)\n",
            " |      Saves the model to Tensorflow SavedModel or a single HDF5 file.\n",
            " |      \n",
            " |      Please see `tf.keras.models.save_model` or the\n",
            " |      [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)\n",
            " |      for details.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String, PathLike, path to SavedModel or H5 file to save the\n",
            " |              model.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          include_optimizer: If True, save optimizer's state together.\n",
            " |          save_format: Either `'tf'` or `'h5'`, indicating whether to save the\n",
            " |              model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,\n",
            " |              and 'h5' in TF 1.X.\n",
            " |          signatures: Signatures to save with the SavedModel. Applicable to the\n",
            " |              'tf' format only. Please see the `signatures` argument in\n",
            " |              `tf.saved_model.save` for details.\n",
            " |          options: (only applies to SavedModel format)\n",
            " |              `tf.saved_model.SaveOptions` object that specifies options for\n",
            " |              saving to SavedModel.\n",
            " |          save_traces: (only applies to SavedModel format) When enabled, the\n",
            " |              SavedModel will store the function traces for each layer. This\n",
            " |              can be disabled, so that only the configs of each layer are stored.\n",
            " |              Defaults to `True`. Disabling this will decrease serialization time\n",
            " |              and reduce file size, but it requires that all custom layers/models\n",
            " |              implement a `get_config()` method.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      from keras.models import load_model\n",
            " |      \n",
            " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
            " |      del model  # deletes the existing model\n",
            " |      \n",
            " |      # returns a compiled model\n",
            " |      # identical to the previous one\n",
            " |      model = load_model('my_model.h5')\n",
            " |      ```\n",
            " |  \n",
            " |  save_weights(self, filepath, overwrite=True, save_format=None, options=None)\n",
            " |      Saves all layer weights.\n",
            " |      \n",
            " |      Either saves in HDF5 or in TensorFlow format based on the `save_format`\n",
            " |      argument.\n",
            " |      \n",
            " |      When saving in HDF5 format, the weight file has:\n",
            " |        - `layer_names` (attribute), a list of strings\n",
            " |            (ordered names of model layers).\n",
            " |        - For every layer, a `group` named `layer.name`\n",
            " |            - For every such layer group, a group attribute `weight_names`,\n",
            " |                a list of strings\n",
            " |                (ordered names of weights tensor of the layer).\n",
            " |            - For every weight in the layer, a dataset\n",
            " |                storing the weight value, named after the weight tensor.\n",
            " |      \n",
            " |      When saving in TensorFlow format, all objects referenced by the network are\n",
            " |      saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n",
            " |      instances or `Optimizer` instances assigned to object attributes. For\n",
            " |      networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n",
            " |      outputs)`, `Layer` instances used by the network are tracked/saved\n",
            " |      automatically. For user-defined classes which inherit from `tf.keras.Model`,\n",
            " |      `Layer` instances must be assigned to object attributes, typically in the\n",
            " |      constructor. See the documentation of `tf.train.Checkpoint` and\n",
            " |      `tf.keras.Model` for details.\n",
            " |      \n",
            " |      While the formats are the same, do not mix `save_weights` and\n",
            " |      `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n",
            " |      loaded using `Model.load_weights`. Checkpoints saved using\n",
            " |      `tf.train.Checkpoint.save` should be restored using the corresponding\n",
            " |      `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n",
            " |      `save_weights` for training checkpoints.\n",
            " |      \n",
            " |      The TensorFlow format matches objects and variables by starting at a root\n",
            " |      object, `self` for `save_weights`, and greedily matching attribute\n",
            " |      names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n",
            " |      is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n",
            " |      means saving a `tf.keras.Model` using `save_weights` and loading into a\n",
            " |      `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n",
            " |      the `Model`'s variables. See the [guide to training\n",
            " |      checkpoints](https://www.tensorflow.org/guide/checkpoint) for details\n",
            " |      on the TensorFlow format.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String or PathLike, path to the file to save the weights to.\n",
            " |              When saving in TensorFlow format, this is the prefix used for\n",
            " |              checkpoint files (multiple files are generated). Note that the '.h5'\n",
            " |              suffix causes weights to be saved in HDF5 format.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n",
            " |              '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n",
            " |              `None` defaults to 'tf'.\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for saving weights.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If h5py is not available when attempting to save in HDF5\n",
            " |              format.\n",
            " |          ValueError: For invalid/unknown format arguments.\n",
            " |  \n",
            " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
            " |      Prints a string summary of the network.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          line_length: Total length of printed lines\n",
            " |              (e.g. set this to adapt the display to different\n",
            " |              terminal window sizes).\n",
            " |          positions: Relative or absolute positions of log elements\n",
            " |              in each line. If not provided,\n",
            " |              defaults to `[.33, .55, .67, 1.]`.\n",
            " |          print_fn: Print function to use. Defaults to `print`.\n",
            " |              It will be called on each line of the summary.\n",
            " |              You can set it to a custom function\n",
            " |              in order to capture the string summary.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if `summary()` is called before the model is built.\n",
            " |  \n",
            " |  test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Test the model on a single batch of samples.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
            " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
            " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors, if\n",
            " |            the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of invalid user-provided arguments.\n",
            " |  \n",
            " |  test_step(self, data)\n",
            " |      The logic for one evaluation step.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.make_test_function`.\n",
            " |      \n",
            " |      This function should contain the mathematical logic for one step of\n",
            " |      evaluation.\n",
            " |      This typically includes the forward pass, loss calculation, and metrics\n",
            " |      updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_test_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned.\n",
            " |  \n",
            " |  to_json(self, **kwargs)\n",
            " |      Returns a JSON string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a JSON save file, use\n",
            " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `json.dumps()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A JSON string.\n",
            " |  \n",
            " |  to_yaml(self, **kwargs)\n",
            " |      Returns a yaml string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a yaml save file, use\n",
            " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
            " |      \n",
            " |      `custom_objects` should be a dictionary mapping\n",
            " |      the names of custom losses / layers / etc to the corresponding\n",
            " |      functions / classes.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `yaml.dump()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A YAML string.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: if yaml module is not found.\n",
            " |  \n",
            " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Runs a single gradient update on a single batch of data.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |                if the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers) to a\n",
            " |            weight (float) to apply to the model's loss for the samples from this\n",
            " |            class during training. This can be useful to tell the model to \"pay\n",
            " |            more attention\" to samples from an under-represented class.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar training loss\n",
            " |          (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If `model.train_on_batch` is wrapped in `tf.function`.\n",
            " |        ValueError: In case of invalid user-provided arguments.\n",
            " |  \n",
            " |  train_step(self, data)\n",
            " |      The logic for one training step.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.make_train_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of training.\n",
            " |      This typically includes the forward pass, loss calculation, backpropagation,\n",
            " |      and metric updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_train_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned. Example:\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_config(config, custom_objects=None) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  distribute_strategy\n",
            " |      The `tf.distribute.Strategy` this model was created under.\n",
            " |  \n",
            " |  layers\n",
            " |  \n",
            " |  metrics\n",
            " |      Returns the model's metrics added using `compile`, `add_metric` APIs.\n",
            " |      \n",
            " |      Note: Metrics passed to `compile()` are available only after a `keras.Model`\n",
            " |      has been trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.add_metric(\n",
            " |      ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc', 'mean']\n",
            " |  \n",
            " |  metrics_names\n",
            " |      Returns the model's display labels for all outputs.\n",
            " |      \n",
            " |      Note: `metrics_names` are available only after a `keras.Model` has been\n",
            " |      trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> model.metrics_names\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc']\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Non-trainable weights are *not* updated during training. They are expected\n",
            " |      to be updated manually in `call()`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of non-trainable variables.\n",
            " |  \n",
            " |  run_eagerly\n",
            " |      Settable attribute indicating whether the model should run eagerly.\n",
            " |      \n",
            " |      Running eagerly means that your model will be run step by step,\n",
            " |      like Python code. Your model might run slower, but it should become easier\n",
            " |      for you to debug it by stepping into individual layer calls.\n",
            " |      \n",
            " |      By default, we will attempt to compile your model to a static graph to\n",
            " |      deliver the best execution performance.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Boolean, whether the model should run eagerly.\n",
            " |  \n",
            " |  state_updates\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Returns the `updates` from all layers that are stateful.\n",
            " |      \n",
            " |      This is useful for separating training updates and\n",
            " |      state updates, e.g. when we need to update a layer's internal state\n",
            " |      during prediction.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of update ops.\n",
            " |  \n",
            " |  trainable_weights\n",
            " |      List of all trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Trainable weights are updated via gradient descent during training.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of trainable variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        *args: Positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
            " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_loss(self, losses, **kwargs)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be dependent\n",
            " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
            " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
            " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(self, inputs):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any loss Tensors passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      losses become part of the model's topology and are tracked in `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Activity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss references\n",
            " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
            " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
            " |      topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      d = tf.keras.layers.Dense(10)\n",
            " |      x = d(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      Arguments:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
            " |          may also be zero-argument callables which create a loss tensor.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |            inputs - Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_metric(self, value, name=None, **kwargs)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      This method can be used inside the `call()` method of a subclassed layer\n",
            " |      or model.\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
            " |        def __init__(self):\n",
            " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
            " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
            " |      \n",
            " |        def call(self, inputs):\n",
            " |          self.add_metric(self.mean(x))\n",
            " |          self.add_metric(tf.reduce_sum(x), name='metric_2')\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any tensor passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      metrics become part of the model's topology and are tracked when you\n",
            " |      save the model via `save()`.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
            " |      Functional Model, as shown in the example below, is not supported. This is\n",
            " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        name: String metric name.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |          `aggregation` - When the `value` tensor provided is not the result of\n",
            " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
            " |          using a `keras.Metric.Mean`.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Add update op(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and variance\n",
            " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
            " |      when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case, variable\n",
            " |      updates are run on the fly and thus do not need to be tracked for later\n",
            " |      execution).\n",
            " |      \n",
            " |      Arguments:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |        inputs: Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        use_resource: Whether to use `ResourceVariable`.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            " |          `AUTO` and the current `DistributionStrategy` chooses\n",
            " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
            " |          `trainable` must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The variable created.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
            " |  \n",
            " |  apply(self, inputs, *args, **kwargs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      This is an alias of `self.__call__`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      If the layer has not been built, this method will call `build` on the\n",
            " |      layer. This assumes that the layer will later be used with inputs that\n",
            " |      match the input shape provided here.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
            " |          how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves losses relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of loss tensors of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves updates relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of update ops of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from Numpy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      sets the weight values from numpy arrays. The weight values should be\n",
            " |      passed in the order they are created by the layer. Note that the layer's\n",
            " |      weights must be instantiated before calling this function by calling\n",
            " |      the layer.\n",
            " |      \n",
            " |      For example, a Dense layer returns a list of two values-- per-output\n",
            " |      weights and the bias value. These can be used to set the weights of another\n",
            " |      Dense layer:\n",
            " |      \n",
            " |      >>> a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b.set_weights(a.get_weights())\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Arguments:\n",
            " |          weights: a list of Numpy arrays. The number\n",
            " |              of arrays and their shape must match\n",
            " |              number of the dimensions of the weights\n",
            " |              of the layer (i.e. it should match the\n",
            " |              output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the provided weights list does not match the\n",
            " |              layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  compute_dtype\n",
            " |      The dtype of the layer's computations.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
            " |      the weights.\n",
            " |      \n",
            " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
            " |      computations and the output to be in the compute dtype as well. This is done\n",
            " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
            " |      these casts if implementing your own layer.\n",
            " |      \n",
            " |      Layers often perform certain internal computations in higher precision when\n",
            " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
            " |      will still typically be float16 or bfloat16 in such cases.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The layer's compute dtype.\n",
            " |  \n",
            " |  dtype\n",
            " |      The dtype of the layer weights.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
            " |      dtype of the layer's computations.\n",
            " |  \n",
            " |  dtype_policy\n",
            " |      The dtype policy associated with this layer.\n",
            " |      \n",
            " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
            " |  \n",
            " |  dynamic\n",
            " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  input_spec\n",
            " |      `InputSpec` instance(s) describing the input format for this layer.\n",
            " |      \n",
            " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
            " |      the layer to run input compatibility checks when it is called.\n",
            " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
            " |      of rank 4. As such, you can set, in `__init__()`:\n",
            " |      \n",
            " |      ```python\n",
            " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
            " |      ```\n",
            " |      \n",
            " |      Now, if you try to call the layer on an input that isn't rank 4\n",
            " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
            " |      error:\n",
            " |      \n",
            " |      ```\n",
            " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
            " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
            " |      ```\n",
            " |      \n",
            " |      Input checks that can be specified via `input_spec` include:\n",
            " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
            " |      - Shape\n",
            " |      - Rank (ndim)\n",
            " |      - Dtype\n",
            " |      \n",
            " |      For more information, see `tf.keras.layers.InputSpec`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
            " |  \n",
            " |  losses\n",
            " |      List of losses added using the `add_loss()` API.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is accessed,\n",
            " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
            " |      propagate gradients back to the corresponding variables.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
            " |      ...   def call(self, inputs):\n",
            " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |      ...     return inputs\n",
            " |      >>> l = MyLayer()\n",
            " |      >>> l(np.ones((10, 1)))\n",
            " |      >>> l.losses\n",
            " |      [1.0]\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Activity regularization.\n",
            " |      >>> len(model.losses)\n",
            " |      0\n",
            " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      >>> len(model.losses)\n",
            " |      1\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
            " |      >>> x = d(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Weight regularization.\n",
            " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      >>> model.losses\n",
            " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  name\n",
            " |      Name of the layer (string), set in the constructor.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  supports_masking\n",
            " |      Whether this layer supports computing a mask using `compute_mask`.\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variable_dtype\n",
            " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      >>> class MyModule(tf.Module):\n",
            " |      ...   @tf.Module.with_name_scope\n",
            " |      ...   def __call__(self, x):\n",
            " |      ...     if not hasattr(self, 'w'):\n",
            " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
            " |      ...     return tf.matmul(x, self.w)\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      >>> mod = MyModule()\n",
            " |      >>> mod(tf.ones([1, 2]))\n",
            " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
            " |      >>> mod.w\n",
            " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
            " |      numpy=..., dtype=float32)>\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      >>> a = tf.Module()\n",
            " |      >>> b = tf.Module()\n",
            " |      >>> c = tf.Module()\n",
            " |      >>> a.b = b\n",
            " |      >>> b.c = c\n",
            " |      >>> list(a.submodules) == [b, c]\n",
            " |      True\n",
            " |      >>> list(b.submodules) == [c]\n",
            " |      True\n",
            " |      >>> list(c.submodules) == []\n",
            " |      True\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVEiWb3bHy_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc78494-ebff-4395-e2e1-5171d37872e2"
      },
      "source": [
        "help(main_model.fit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method fit in module tensorflow.python.keras.engine.training:\n",
            "\n",
            "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) method of tensorflow.python.keras.engine.functional.Functional instance\n",
            "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            "    \n",
            "    Arguments:\n",
            "        x: Input data. It could be:\n",
            "          - A Numpy array (or array-like), or a list of arrays\n",
            "            (in case the model has multiple inputs).\n",
            "          - A TensorFlow tensor, or a list of tensors\n",
            "            (in case the model has multiple inputs).\n",
            "          - A dict mapping input names to the corresponding array/tensors,\n",
            "            if the model has named inputs.\n",
            "          - A `tf.data` dataset. Should return a tuple\n",
            "            of either `(inputs, targets)` or\n",
            "            `(inputs, targets, sample_weights)`.\n",
            "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            "            or `(inputs, targets, sample_weights)`.\n",
            "          A more detailed description of unpacking behavior for iterator types\n",
            "          (Dataset, generator, Sequence) is given below.\n",
            "        y: Target data. Like the input data `x`,\n",
            "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
            "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
            "          or `keras.utils.Sequence` instance, `y` should\n",
            "          not be specified (since targets will be obtained from `x`).\n",
            "        batch_size: Integer or `None`.\n",
            "            Number of samples per gradient update.\n",
            "            If unspecified, `batch_size` will default to 32.\n",
            "            Do not specify the `batch_size` if your data is in the\n",
            "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
            "            (since they generate batches).\n",
            "        epochs: Integer. Number of epochs to train the model.\n",
            "            An epoch is an iteration over the entire `x` and `y`\n",
            "            data provided.\n",
            "            Note that in conjunction with `initial_epoch`,\n",
            "            `epochs` is to be understood as \"final epoch\".\n",
            "            The model is not trained for a number of iterations\n",
            "            given by `epochs`, but merely until the epoch\n",
            "            of index `epochs` is reached.\n",
            "        verbose: 0, 1, or 2. Verbosity mode.\n",
            "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            "            Note that the progress bar is not particularly useful when\n",
            "            logged to a file, so verbose=2 is recommended when not running\n",
            "            interactively (eg, in a production environment).\n",
            "        callbacks: List of `keras.callbacks.Callback` instances.\n",
            "            List of callbacks to apply during training.\n",
            "            See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
            "            and `tf.keras.callbacks.History` callbacks are created automatically\n",
            "            and need not be passed into `model.fit`.\n",
            "            `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
            "            `verbose` argument to `model.fit`.\n",
            "        validation_split: Float between 0 and 1.\n",
            "            Fraction of the training data to be used as validation data.\n",
            "            The model will set apart this fraction of the training data,\n",
            "            will not train on it, and will evaluate\n",
            "            the loss and any model metrics\n",
            "            on this data at the end of each epoch.\n",
            "            The validation data is selected from the last samples\n",
            "            in the `x` and `y` data provided, before shuffling. This argument is\n",
            "            not supported when `x` is a dataset, generator or\n",
            "           `keras.utils.Sequence` instance.\n",
            "        validation_data: Data on which to evaluate\n",
            "            the loss and any model metrics at the end of each epoch.\n",
            "            The model will not be trained on this data. Thus, note the fact\n",
            "            that the validation loss of data provided using `validation_split`\n",
            "            or `validation_data` is not affected by regularization layers like\n",
            "            noise and dropout.\n",
            "            `validation_data` will override `validation_split`.\n",
            "            `validation_data` could be:\n",
            "              - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
            "              - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
            "              - dataset\n",
            "            For the first two cases, `batch_size` must be provided.\n",
            "            For the last case, `validation_steps` could be provided.\n",
            "            Note that `validation_data` does not support all the data types that\n",
            "            are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
            "        shuffle: Boolean (whether to shuffle the training data\n",
            "            before each epoch) or str (for 'batch'). This argument is ignored\n",
            "            when `x` is a generator. 'batch' is a special option for dealing\n",
            "            with the limitations of HDF5 data; it shuffles in batch-sized\n",
            "            chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
            "        class_weight: Optional dictionary mapping class indices (integers)\n",
            "            to a weight (float) value, used for weighting the loss function\n",
            "            (during training only).\n",
            "            This can be useful to tell the model to\n",
            "            \"pay more attention\" to samples from\n",
            "            an under-represented class.\n",
            "        sample_weight: Optional Numpy array of weights for\n",
            "            the training samples, used for weighting the loss function\n",
            "            (during training only). You can either pass a flat (1D)\n",
            "            Numpy array with the same length as the input samples\n",
            "            (1:1 mapping between weights and samples),\n",
            "            or in the case of temporal data,\n",
            "            you can pass a 2D array with shape\n",
            "            `(samples, sequence_length)`,\n",
            "            to apply a different weight to every timestep of every sample. This\n",
            "            argument is not supported when `x` is a dataset, generator, or\n",
            "           `keras.utils.Sequence` instance, instead provide the sample_weights\n",
            "            as the third element of `x`.\n",
            "        initial_epoch: Integer.\n",
            "            Epoch at which to start training\n",
            "            (useful for resuming a previous training run).\n",
            "        steps_per_epoch: Integer or `None`.\n",
            "            Total number of steps (batches of samples)\n",
            "            before declaring one epoch finished and starting the\n",
            "            next epoch. When training with input tensors such as\n",
            "            TensorFlow data tensors, the default `None` is equal to\n",
            "            the number of samples in your dataset divided by\n",
            "            the batch size, or 1 if that cannot be determined. If x is a\n",
            "            `tf.data` dataset, and 'steps_per_epoch'\n",
            "            is None, the epoch will run until the input dataset is exhausted.\n",
            "            When passing an infinitely repeating dataset, you must specify the\n",
            "            `steps_per_epoch` argument. This argument is not supported with\n",
            "            array inputs.\n",
            "        validation_steps: Only relevant if `validation_data` is provided and\n",
            "            is a `tf.data` dataset. Total number of steps (batches of\n",
            "            samples) to draw before stopping when performing validation\n",
            "            at the end of every epoch. If 'validation_steps' is None, validation\n",
            "            will run until the `validation_data` dataset is exhausted. In the\n",
            "            case of an infinitely repeated dataset, it will run into an\n",
            "            infinite loop. If 'validation_steps' is specified and only part of\n",
            "            the dataset will be consumed, the evaluation will start from the\n",
            "            beginning of the dataset at each epoch. This ensures that the same\n",
            "            validation samples are used every time.\n",
            "        validation_batch_size: Integer or `None`.\n",
            "            Number of samples per validation batch.\n",
            "            If unspecified, will default to `batch_size`.\n",
            "            Do not specify the `validation_batch_size` if your data is in the\n",
            "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
            "            (since they generate batches).\n",
            "        validation_freq: Only relevant if validation data is provided. Integer\n",
            "            or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
            "            If an integer, specifies how many training epochs to run before a\n",
            "            new validation run is performed, e.g. `validation_freq=2` runs\n",
            "            validation every 2 epochs. If a Container, specifies the epochs on\n",
            "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
            "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            "            input only. Maximum size for the generator queue.\n",
            "            If unspecified, `max_queue_size` will default to 10.\n",
            "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            "            only. Maximum number of processes to spin up\n",
            "            when using process-based threading. If unspecified, `workers`\n",
            "            will default to 1. If 0, will execute the generator on the main\n",
            "            thread.\n",
            "        use_multiprocessing: Boolean. Used for generator or\n",
            "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            "            threading. If unspecified, `use_multiprocessing` will default to\n",
            "            `False`. Note that because this implementation relies on\n",
            "            multiprocessing, you should not pass non-picklable arguments to\n",
            "            the generator as they can't be passed easily to children processes.\n",
            "    \n",
            "    Unpacking behavior for iterator-like inputs:\n",
            "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
            "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            "      yield not only features (x) but optionally targets (y) and sample weights.\n",
            "      Keras requires that the output of such iterator-likes be unambiguous. The\n",
            "      iterator should return a tuple of length 1, 2, or 3, where the optional\n",
            "      second and third elements will be used for y and sample_weight\n",
            "      respectively. Any other type provided will be wrapped in a length one\n",
            "      tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
            "      should still adhere to the top-level tuple structure.\n",
            "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            "      features, targets, and weights from the keys of a single dict.\n",
            "        A notable unsupported data type is the namedtuple. The reason is that\n",
            "      it behaves like both an ordered datatype (tuple) and a mapping\n",
            "      datatype (dict). So given a namedtuple of the form:\n",
            "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            "      it is ambiguous whether to reverse the order of the elements when\n",
            "      interpreting the value. Even worse is a tuple of the form:\n",
            "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            "      where it is unclear if the tuple was intended to be unpacked into x, y,\n",
            "      and sample_weight or passed through as a single element to `x`. As a\n",
            "      result the data processing code will simply raise a ValueError if it\n",
            "      encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
            "    \n",
            "    Returns:\n",
            "        A `History` object. Its `History.history` attribute is\n",
            "        a record of training loss values and metrics values\n",
            "        at successive epochs, as well as validation loss values\n",
            "        and validation metrics values (if applicable).\n",
            "    \n",
            "    Raises:\n",
            "        RuntimeError: 1. If the model was never compiled or,\n",
            "        2. If `model.fit` is  wrapped in `tf.function`.\n",
            "    \n",
            "        ValueError: In case of mismatch between the provided input data\n",
            "            and what the model expects or when the input data is empty.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qGf1FQf7AkD"
      },
      "source": [
        "# 6.0 Compile model now\r\n",
        "# Ref: Model.compile: \r\n",
        "#      https://wwwa.tensorflow.org/api_docs/python/tf/keras/Model\r\n",
        "\r\n",
        "main_model.compile(\r\n",
        "                     loss = ['mse', 'mse'],        # Could also be in dict() format\r\n",
        "                     metrics = \"mse\",\r\n",
        "                     loss_weights= {\"out_a\": 0.9,   # More weight to error here\r\n",
        "                                    \"out_b\" : 0.1   # Less weight to error here\r\n",
        "                                    }\r\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEeYQ5aD7oJV",
        "outputId": "4ebb46d8-fd39-4416-99ce-10077a9226f5"
      },
      "source": [
        "# 6.1\r\n",
        "main_model.fit(\r\n",
        "               {                            #[X_train[:,:4],X_train[:,1:8]]\r\n",
        "                   \"in_a\" : X_train[:,:4],  # One input\r\n",
        "                   \"in_b\" : X_train[:,1:8]  # IInd input     \r\n",
        "               },              \r\n",
        "               [y_train,y_train],\r\n",
        "               epochs = 100\r\n",
        "           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "516/516 [==============================] - 2s 1ms/step - loss: 2.5911 - out_a_loss: 2.5820 - out_b_loss: 2.6737 - out_a_mse: 2.5820 - out_b_mse: 2.6737\n",
            "Epoch 2/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4350 - out_a_loss: 2.4350 - out_b_loss: 2.4350 - out_a_mse: 2.4350 - out_b_mse: 2.4350\n",
            "Epoch 3/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4574 - out_a_loss: 2.4574 - out_b_loss: 2.4574 - out_a_mse: 2.4574 - out_b_mse: 2.4574\n",
            "Epoch 4/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4582 - out_a_loss: 2.4582 - out_b_loss: 2.4582 - out_a_mse: 2.4582 - out_b_mse: 2.4582\n",
            "Epoch 5/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4532 - out_a_loss: 2.4532 - out_b_loss: 2.4533 - out_a_mse: 2.4532 - out_b_mse: 2.4533\n",
            "Epoch 6/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4427 - out_a_loss: 2.4427 - out_b_loss: 2.4428 - out_a_mse: 2.4427 - out_b_mse: 2.4428\n",
            "Epoch 7/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4674 - out_a_loss: 2.4673 - out_b_loss: 2.4675 - out_a_mse: 2.4673 - out_b_mse: 2.4675\n",
            "Epoch 8/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4591 - out_a_loss: 2.4591 - out_b_loss: 2.4592 - out_a_mse: 2.4591 - out_b_mse: 2.4592\n",
            "Epoch 9/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4436 - out_a_loss: 2.4436 - out_b_loss: 2.4437 - out_a_mse: 2.4436 - out_b_mse: 2.4437\n",
            "Epoch 10/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4898 - out_a_loss: 2.4898 - out_b_loss: 2.4899 - out_a_mse: 2.4898 - out_b_mse: 2.4899\n",
            "Epoch 11/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5099 - out_a_loss: 2.5099 - out_b_loss: 2.5100 - out_a_mse: 2.5099 - out_b_mse: 2.5100\n",
            "Epoch 12/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4986 - out_a_loss: 2.4986 - out_b_loss: 2.4988 - out_a_mse: 2.4986 - out_b_mse: 2.4988\n",
            "Epoch 13/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4484 - out_a_loss: 2.4484 - out_b_loss: 2.4486 - out_a_mse: 2.4484 - out_b_mse: 2.4486\n",
            "Epoch 14/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4271 - out_a_loss: 2.4271 - out_b_loss: 2.4273 - out_a_mse: 2.4271 - out_b_mse: 2.4273\n",
            "Epoch 15/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4411 - out_a_loss: 2.4410 - out_b_loss: 2.4413 - out_a_mse: 2.4410 - out_b_mse: 2.4413\n",
            "Epoch 16/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4360 - out_a_loss: 2.4360 - out_b_loss: 2.4362 - out_a_mse: 2.4360 - out_b_mse: 2.4362\n",
            "Epoch 17/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4699 - out_a_loss: 2.4699 - out_b_loss: 2.4701 - out_a_mse: 2.4699 - out_b_mse: 2.4701\n",
            "Epoch 18/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5292 - out_a_loss: 2.5292 - out_b_loss: 2.5294 - out_a_mse: 2.5292 - out_b_mse: 2.5294\n",
            "Epoch 19/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4796 - out_a_loss: 2.4795 - out_b_loss: 2.4798 - out_a_mse: 2.4795 - out_b_mse: 2.4798\n",
            "Epoch 20/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4125 - out_a_loss: 2.4125 - out_b_loss: 2.4127 - out_a_mse: 2.4125 - out_b_mse: 2.4127\n",
            "Epoch 21/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4889 - out_a_loss: 2.4888 - out_b_loss: 2.4890 - out_a_mse: 2.4888 - out_b_mse: 2.4890\n",
            "Epoch 22/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4562 - out_a_loss: 2.4561 - out_b_loss: 2.4565 - out_a_mse: 2.4561 - out_b_mse: 2.4565\n",
            "Epoch 23/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4430 - out_a_loss: 2.4430 - out_b_loss: 2.4433 - out_a_mse: 2.4430 - out_b_mse: 2.4433\n",
            "Epoch 24/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4581 - out_a_loss: 2.4581 - out_b_loss: 2.4583 - out_a_mse: 2.4581 - out_b_mse: 2.4583\n",
            "Epoch 25/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4487 - out_a_loss: 2.4486 - out_b_loss: 2.4489 - out_a_mse: 2.4486 - out_b_mse: 2.4489\n",
            "Epoch 26/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4573 - out_a_loss: 2.4573 - out_b_loss: 2.4575 - out_a_mse: 2.4573 - out_b_mse: 2.4575\n",
            "Epoch 27/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4628 - out_a_loss: 2.4627 - out_b_loss: 2.4630 - out_a_mse: 2.4627 - out_b_mse: 2.4630\n",
            "Epoch 28/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5600 - out_a_loss: 2.5600 - out_b_loss: 2.5602 - out_a_mse: 2.5600 - out_b_mse: 2.5602\n",
            "Epoch 29/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4565 - out_a_loss: 2.4565 - out_b_loss: 2.4567 - out_a_mse: 2.4565 - out_b_mse: 2.4567\n",
            "Epoch 30/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4409 - out_a_loss: 2.4409 - out_b_loss: 2.4412 - out_a_mse: 2.4409 - out_b_mse: 2.4412\n",
            "Epoch 31/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4835 - out_a_loss: 2.4834 - out_b_loss: 2.4837 - out_a_mse: 2.4834 - out_b_mse: 2.4837\n",
            "Epoch 32/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4467 - out_a_loss: 2.4467 - out_b_loss: 2.4470 - out_a_mse: 2.4467 - out_b_mse: 2.4470\n",
            "Epoch 33/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4756 - out_a_loss: 2.4756 - out_b_loss: 2.4759 - out_a_mse: 2.4756 - out_b_mse: 2.4759\n",
            "Epoch 34/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5118 - out_a_loss: 2.5117 - out_b_loss: 2.5121 - out_a_mse: 2.5117 - out_b_mse: 2.5121\n",
            "Epoch 35/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4832 - out_a_loss: 2.4831 - out_b_loss: 2.4834 - out_a_mse: 2.4831 - out_b_mse: 2.4834\n",
            "Epoch 36/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5273 - out_a_loss: 2.5273 - out_b_loss: 2.5274 - out_a_mse: 2.5273 - out_b_mse: 2.5274\n",
            "Epoch 37/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4942 - out_a_loss: 2.4942 - out_b_loss: 2.4945 - out_a_mse: 2.4942 - out_b_mse: 2.4945\n",
            "Epoch 38/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5033 - out_a_loss: 2.5033 - out_b_loss: 2.5035 - out_a_mse: 2.5033 - out_b_mse: 2.5035\n",
            "Epoch 39/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4355 - out_a_loss: 2.4355 - out_b_loss: 2.4357 - out_a_mse: 2.4355 - out_b_mse: 2.4357\n",
            "Epoch 40/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4905 - out_a_loss: 2.4905 - out_b_loss: 2.4907 - out_a_mse: 2.4905 - out_b_mse: 2.4907\n",
            "Epoch 41/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4172 - out_a_loss: 2.4171 - out_b_loss: 2.4174 - out_a_mse: 2.4171 - out_b_mse: 2.4174\n",
            "Epoch 42/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4704 - out_a_loss: 2.4703 - out_b_loss: 2.4706 - out_a_mse: 2.4703 - out_b_mse: 2.4706\n",
            "Epoch 43/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4840 - out_a_loss: 2.4840 - out_b_loss: 2.4842 - out_a_mse: 2.4840 - out_b_mse: 2.4842\n",
            "Epoch 44/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4912 - out_a_loss: 2.4911 - out_b_loss: 2.4913 - out_a_mse: 2.4911 - out_b_mse: 2.4913\n",
            "Epoch 45/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4381 - out_a_loss: 2.4380 - out_b_loss: 2.4383 - out_a_mse: 2.4380 - out_b_mse: 2.4383\n",
            "Epoch 46/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4690 - out_a_loss: 2.4689 - out_b_loss: 2.4692 - out_a_mse: 2.4689 - out_b_mse: 2.4692\n",
            "Epoch 47/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4898 - out_a_loss: 2.4897 - out_b_loss: 2.4900 - out_a_mse: 2.4897 - out_b_mse: 2.4900\n",
            "Epoch 48/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4628 - out_a_loss: 2.4628 - out_b_loss: 2.4630 - out_a_mse: 2.4628 - out_b_mse: 2.4630\n",
            "Epoch 49/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4624 - out_a_loss: 2.4624 - out_b_loss: 2.4626 - out_a_mse: 2.4624 - out_b_mse: 2.4626\n",
            "Epoch 50/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5195 - out_a_loss: 2.5195 - out_b_loss: 2.5197 - out_a_mse: 2.5195 - out_b_mse: 2.5197\n",
            "Epoch 51/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4737 - out_a_loss: 2.4736 - out_b_loss: 2.4739 - out_a_mse: 2.4736 - out_b_mse: 2.4739\n",
            "Epoch 52/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4519 - out_a_loss: 2.4519 - out_b_loss: 2.4522 - out_a_mse: 2.4519 - out_b_mse: 2.4522\n",
            "Epoch 53/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4936 - out_a_loss: 2.4935 - out_b_loss: 2.4938 - out_a_mse: 2.4935 - out_b_mse: 2.4938\n",
            "Epoch 54/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4075 - out_a_loss: 2.4075 - out_b_loss: 2.4078 - out_a_mse: 2.4075 - out_b_mse: 2.4078\n",
            "Epoch 55/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5435 - out_a_loss: 2.5435 - out_b_loss: 2.5437 - out_a_mse: 2.5435 - out_b_mse: 2.5437\n",
            "Epoch 56/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4904 - out_a_loss: 2.4904 - out_b_loss: 2.4907 - out_a_mse: 2.4904 - out_b_mse: 2.4907\n",
            "Epoch 57/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5147 - out_a_loss: 2.5146 - out_b_loss: 2.5149 - out_a_mse: 2.5146 - out_b_mse: 2.5149\n",
            "Epoch 58/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4771 - out_a_loss: 2.4771 - out_b_loss: 2.4774 - out_a_mse: 2.4771 - out_b_mse: 2.4774\n",
            "Epoch 59/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4851 - out_a_loss: 2.4851 - out_b_loss: 2.4853 - out_a_mse: 2.4851 - out_b_mse: 2.4853\n",
            "Epoch 60/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4336 - out_a_loss: 2.4336 - out_b_loss: 2.4338 - out_a_mse: 2.4336 - out_b_mse: 2.4338\n",
            "Epoch 61/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4309 - out_a_loss: 2.4308 - out_b_loss: 2.4311 - out_a_mse: 2.4308 - out_b_mse: 2.4311\n",
            "Epoch 62/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4544 - out_a_loss: 2.4544 - out_b_loss: 2.4547 - out_a_mse: 2.4544 - out_b_mse: 2.4547\n",
            "Epoch 63/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5337 - out_a_loss: 2.5337 - out_b_loss: 2.5340 - out_a_mse: 2.5337 - out_b_mse: 2.5340\n",
            "Epoch 64/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4933 - out_a_loss: 2.4932 - out_b_loss: 2.4934 - out_a_mse: 2.4932 - out_b_mse: 2.4934\n",
            "Epoch 65/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4342 - out_a_loss: 2.4341 - out_b_loss: 2.4343 - out_a_mse: 2.4341 - out_b_mse: 2.4343\n",
            "Epoch 66/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4835 - out_a_loss: 2.4835 - out_b_loss: 2.4836 - out_a_mse: 2.4835 - out_b_mse: 2.4836\n",
            "Epoch 67/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4765 - out_a_loss: 2.4765 - out_b_loss: 2.4767 - out_a_mse: 2.4765 - out_b_mse: 2.4767\n",
            "Epoch 68/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4427 - out_a_loss: 2.4427 - out_b_loss: 2.4430 - out_a_mse: 2.4427 - out_b_mse: 2.4430\n",
            "Epoch 69/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4669 - out_a_loss: 2.4668 - out_b_loss: 2.4670 - out_a_mse: 2.4668 - out_b_mse: 2.4670\n",
            "Epoch 70/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4657 - out_a_loss: 2.4657 - out_b_loss: 2.4659 - out_a_mse: 2.4657 - out_b_mse: 2.4659\n",
            "Epoch 71/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4559 - out_a_loss: 2.4559 - out_b_loss: 2.4561 - out_a_mse: 2.4559 - out_b_mse: 2.4561\n",
            "Epoch 72/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4404 - out_a_loss: 2.4403 - out_b_loss: 2.4405 - out_a_mse: 2.4403 - out_b_mse: 2.4405\n",
            "Epoch 73/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5044 - out_a_loss: 2.5044 - out_b_loss: 2.5046 - out_a_mse: 2.5044 - out_b_mse: 2.5046\n",
            "Epoch 74/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4506 - out_a_loss: 2.4506 - out_b_loss: 2.4508 - out_a_mse: 2.4506 - out_b_mse: 2.4508\n",
            "Epoch 75/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5175 - out_a_loss: 2.5175 - out_b_loss: 2.5177 - out_a_mse: 2.5175 - out_b_mse: 2.5177\n",
            "Epoch 76/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4438 - out_a_loss: 2.4437 - out_b_loss: 2.4440 - out_a_mse: 2.4437 - out_b_mse: 2.4440\n",
            "Epoch 77/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4701 - out_a_loss: 2.4700 - out_b_loss: 2.4703 - out_a_mse: 2.4700 - out_b_mse: 2.4703\n",
            "Epoch 78/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5124 - out_a_loss: 2.5123 - out_b_loss: 2.5127 - out_a_mse: 2.5123 - out_b_mse: 2.5127\n",
            "Epoch 79/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4659 - out_a_loss: 2.4659 - out_b_loss: 2.4662 - out_a_mse: 2.4659 - out_b_mse: 2.4662\n",
            "Epoch 80/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4961 - out_a_loss: 2.4961 - out_b_loss: 2.4963 - out_a_mse: 2.4961 - out_b_mse: 2.4963\n",
            "Epoch 81/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4690 - out_a_loss: 2.4690 - out_b_loss: 2.4692 - out_a_mse: 2.4690 - out_b_mse: 2.4692\n",
            "Epoch 82/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5120 - out_a_loss: 2.5120 - out_b_loss: 2.5122 - out_a_mse: 2.5120 - out_b_mse: 2.5122\n",
            "Epoch 83/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4664 - out_a_loss: 2.4664 - out_b_loss: 2.4667 - out_a_mse: 2.4664 - out_b_mse: 2.4667\n",
            "Epoch 84/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4494 - out_a_loss: 2.4494 - out_b_loss: 2.4497 - out_a_mse: 2.4494 - out_b_mse: 2.4497\n",
            "Epoch 85/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4348 - out_a_loss: 2.4348 - out_b_loss: 2.4350 - out_a_mse: 2.4348 - out_b_mse: 2.4350\n",
            "Epoch 86/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4874 - out_a_loss: 2.4874 - out_b_loss: 2.4877 - out_a_mse: 2.4874 - out_b_mse: 2.4877\n",
            "Epoch 87/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5013 - out_a_loss: 2.5013 - out_b_loss: 2.5015 - out_a_mse: 2.5013 - out_b_mse: 2.5015\n",
            "Epoch 88/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4829 - out_a_loss: 2.4829 - out_b_loss: 2.4831 - out_a_mse: 2.4829 - out_b_mse: 2.4831\n",
            "Epoch 89/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4747 - out_a_loss: 2.4746 - out_b_loss: 2.4748 - out_a_mse: 2.4746 - out_b_mse: 2.4748\n",
            "Epoch 90/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4713 - out_a_loss: 2.4713 - out_b_loss: 2.4715 - out_a_mse: 2.4713 - out_b_mse: 2.4715\n",
            "Epoch 91/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4154 - out_a_loss: 2.4154 - out_b_loss: 2.4157 - out_a_mse: 2.4154 - out_b_mse: 2.4157\n",
            "Epoch 92/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4763 - out_a_loss: 2.4763 - out_b_loss: 2.4765 - out_a_mse: 2.4763 - out_b_mse: 2.4765\n",
            "Epoch 93/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4171 - out_a_loss: 2.4171 - out_b_loss: 2.4173 - out_a_mse: 2.4171 - out_b_mse: 2.4173\n",
            "Epoch 94/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5176 - out_a_loss: 2.5176 - out_b_loss: 2.5178 - out_a_mse: 2.5176 - out_b_mse: 2.5178\n",
            "Epoch 95/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4747 - out_a_loss: 2.4747 - out_b_loss: 2.4749 - out_a_mse: 2.4747 - out_b_mse: 2.4749\n",
            "Epoch 96/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5004 - out_a_loss: 2.5004 - out_b_loss: 2.5006 - out_a_mse: 2.5004 - out_b_mse: 2.5006\n",
            "Epoch 97/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4754 - out_a_loss: 2.4754 - out_b_loss: 2.4755 - out_a_mse: 2.4754 - out_b_mse: 2.4755\n",
            "Epoch 98/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4470 - out_a_loss: 2.4470 - out_b_loss: 2.4473 - out_a_mse: 2.4470 - out_b_mse: 2.4473\n",
            "Epoch 99/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4850 - out_a_loss: 2.4850 - out_b_loss: 2.4853 - out_a_mse: 2.4850 - out_b_mse: 2.4853\n",
            "Epoch 100/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4775 - out_a_loss: 2.4775 - out_b_loss: 2.4778 - out_a_mse: 2.4775 - out_b_mse: 2.4778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdb8ac1ee80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaDhO7KXFy2m",
        "outputId": "0043cf12-d858-4580-ca5b-0fb4983dd413"
      },
      "source": [
        "# 6.2 One can also write outputs in a dictionary form, as:\r\n",
        "\r\n",
        "main_model.fit(\r\n",
        "               {                            #[X_train[:,:4],X_train[:,1:8]]\r\n",
        "                   \"in_a\" : X_train[:,:4],  # One input\r\n",
        "                   \"in_b\" : X_train[:,1:8]  # IInd input     \r\n",
        "               },              \r\n",
        "               {\r\n",
        "                   \"out_a\" : y_train,\r\n",
        "                   \"out_b\" : y_train\r\n",
        "               },\r\n",
        "               epochs = 100\r\n",
        "           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4699 - out_a_loss: 2.4699 - out_b_loss: 2.4702 - out_a_mse: 2.4699 - out_b_mse: 2.4702\n",
            "Epoch 2/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4699 - out_a_loss: 2.4698 - out_b_loss: 2.4701 - out_a_mse: 2.4698 - out_b_mse: 2.4701\n",
            "Epoch 3/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4701 - out_a_mse: 2.4698 - out_b_mse: 2.4701\n",
            "Epoch 4/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4700 - out_a_mse: 2.4698 - out_b_mse: 2.4700\n",
            "Epoch 5/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4699 - out_a_loss: 2.4699 - out_b_loss: 2.4701 - out_a_mse: 2.4699 - out_b_mse: 2.4701\n",
            "Epoch 6/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4701 - out_a_mse: 2.4698 - out_b_mse: 2.4701\n",
            "Epoch 7/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4700 - out_a_mse: 2.4698 - out_b_mse: 2.4700\n",
            "Epoch 8/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4699 - out_a_loss: 2.4699 - out_b_loss: 2.4701 - out_a_mse: 2.4699 - out_b_mse: 2.4701\n",
            "Epoch 9/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4699 - out_a_loss: 2.4699 - out_b_loss: 2.4702 - out_a_mse: 2.4699 - out_b_mse: 2.4702\n",
            "Epoch 10/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4701 - out_a_mse: 2.4698 - out_b_mse: 2.4701\n",
            "Epoch 11/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 12/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 13/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4700 - out_a_mse: 2.4698 - out_b_mse: 2.4700\n",
            "Epoch 14/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 15/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4701 - out_a_mse: 2.4698 - out_b_mse: 2.4701\n",
            "Epoch 16/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 17/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4700 - out_a_mse: 2.4698 - out_b_mse: 2.4700\n",
            "Epoch 18/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 19/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 20/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4699 - out_a_mse: 2.4697 - out_b_mse: 2.4699\n",
            "Epoch 21/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4699 - out_a_mse: 2.4697 - out_b_mse: 2.4699\n",
            "Epoch 22/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 23/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4697 - out_b_loss: 2.4701 - out_a_mse: 2.4697 - out_b_mse: 2.4701\n",
            "Epoch 24/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4700 - out_a_mse: 2.4698 - out_b_mse: 2.4700\n",
            "Epoch 25/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4699 - out_a_mse: 2.4697 - out_b_mse: 2.4699\n",
            "Epoch 26/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 27/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 28/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 29/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 30/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 31/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 32/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 33/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 34/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 35/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4699 - out_a_mse: 2.4697 - out_b_mse: 2.4699\n",
            "Epoch 36/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 37/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 38/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 39/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 40/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 41/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 42/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 43/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4699 - out_a_mse: 2.4697 - out_b_mse: 2.4699\n",
            "Epoch 44/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 45/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 46/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 47/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 48/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 49/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 50/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4699 - out_a_mse: 2.4697 - out_b_mse: 2.4699\n",
            "Epoch 51/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 52/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 53/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 54/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 55/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 56/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 57/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 58/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 59/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 60/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 61/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 62/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 63/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 64/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 65/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 66/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 67/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 68/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 69/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 70/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 71/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 72/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 73/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 74/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 75/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 76/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 77/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 78/100\n",
            "516/516 [==============================] - 2s 4ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 79/100\n",
            "516/516 [==============================] - 2s 4ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 80/100\n",
            "516/516 [==============================] - 2s 4ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 81/100\n",
            "516/516 [==============================] - 2s 4ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 82/100\n",
            "516/516 [==============================] - 2s 4ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 83/100\n",
            "516/516 [==============================] - 2s 4ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 84/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 85/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 86/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 87/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 88/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 89/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 90/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 91/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 92/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 93/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 94/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 95/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 96/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 97/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 98/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 99/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 100/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdb885219e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqDTxCZr8fxs",
        "outputId": "060319d0-bb72-4b0a-dc48-4a770770b3db"
      },
      "source": [
        "# 7.0 To evaluate, we must also supply two inputs\r\n",
        "main_model.evaluate(\r\n",
        "                     [X_test[:,:4],X_test[:,1:8]],\r\n",
        "                      y_test\r\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129/129 [==============================] - 0s 1ms/step - loss: 2.4382 - out_a_loss: 2.4382 - out_b_loss: 2.4385 - out_a_mse: 2.4382 - out_b_mse: 2.4385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.438199758529663,\n",
              " 2.4381649494171143,\n",
              " 2.43851637840271,\n",
              " 2.4381649494171143,\n",
              " 2.43851637840271]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MhQF056p09N"
      },
      "source": [
        "########### It is done ##############"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}