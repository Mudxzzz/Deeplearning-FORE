{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_functional.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QBKy2GzK76sc",
        "dYuF2BagAAYt"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP692/QVz//KU5uvg0xNITK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/deeplearning/blob/main/keras_functional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhHxdIbdcR6p"
      },
      "source": [
        "<table align=\"left\">\r\n",
        "  <td>\r\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/harnalashok/deeplearning/blob/main/keras_functional.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\r\n",
        "  </td>\r\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzHM-gho6Z4X"
      },
      "source": [
        "# Last amended: 16th Jan, 2021\r\n",
        "# Ref: Hands-On Machine Learningwith Scikit-Learn, Keras, and TensorFlow by Aurelien Geron\r\n",
        "#      Page: 308-312\r\n",
        "# Using keras functional API"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYl2nTZg7MRe"
      },
      "source": [
        "# 1.0 Import libraries\r\n",
        "import pandas as pd\r\n",
        "from sklearn.datasets import fetch_california_housing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "\r\n",
        "# Import tensorflow/keras \r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers \r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtYhfSLVGVC_"
      },
      "source": [
        "# 1.1 Display multiple outputs from a Cell\r\n",
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzwqXlRY6uH7",
        "outputId": "99f4173d-026f-4673-a569-d63553f74b83"
      },
      "source": [
        "# 2.0 Get Data\r\n",
        "#     The data needs little processing\r\n",
        "housing = fetch_california_housing(return_X_y= False)\r\n",
        "type(housing)   # sklearn.utils.Bunch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sklearn.utils.Bunch"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxD2EQQ56_Z4",
        "outputId": "81ea0c11-9765-46f4-e916-4ed4db5f1982"
      },
      "source": [
        "# 2.1 Seperate X,y\r\n",
        "X = housing.data\r\n",
        "y = housing.target\r\n",
        "X.shape   # (20640, 8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20640, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha_HncyCmS9y"
      },
      "source": [
        "# 2.2 Normalize input data\r\n",
        "ss = StandardScaler()\r\n",
        "X = ss.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uogWyNdXGfm3",
        "outputId": "024f8ca4-54a2-4850-d432-c0c5e37f3d8b"
      },
      "source": [
        "# 2.3 Show data field names\r\n",
        "print(housing.DESCR)\r\n",
        "housing.feature_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block\n",
            "        - HouseAge      median house age in block\n",
            "        - AveRooms      average number of rooms\n",
            "        - AveBedrms     average number of bedrooms\n",
            "        - Population    block population\n",
            "        - AveOccup      average house occupancy\n",
            "        - Latitude      house block latitude\n",
            "        - Longitude     house block longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "http://lib.stat.cmu.edu/datasets/\n",
            "\n",
            "The target variable is the median house value for California districts.\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MedInc',\n",
              " 'HouseAge',\n",
              " 'AveRooms',\n",
              " 'AveBedrms',\n",
              " 'Population',\n",
              " 'AveOccup',\n",
              " 'Latitude',\n",
              " 'Longitude']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzWFuUd9Mf-0",
        "outputId": "7e54bdaa-4165-4682-f781-513964ac894d"
      },
      "source": [
        "# 3.0 Split train/test data\r\n",
        "X_train,X_test, y_train,y_test = train_test_split(X,y,test_size = 0.2)\r\n",
        "X_train.shape   # (16512, 8)\r\n",
        "X_test.shape    # (4128, 8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16512, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4128, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBKy2GzK76sc"
      },
      "source": [
        "# Wide and Deep Network--Ist version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JouACnATIIUF"
      },
      "source": [
        "# 3.1 Design model\r\n",
        "\r\n",
        "# 3.1.1 Inputs to model. Note that inputs is NOT\r\n",
        "#       a part of layers object\r\n",
        "inputs = tf.keras.Input(shape = X.shape[1:])\r\n",
        "# 3.1.2 Add layers\r\n",
        "x = layers.Dense(100, activation = 'relu')(inputs)\r\n",
        "x = layers.Dense(100, activation = 'relu')(x)\r\n",
        "x = tf.keras.layers.concatenate([x,inputs])\r\n",
        "out = layers.Dense(1,activation = 'sigmoid')(x)\r\n",
        "# 3.1.3 Create model now\r\n",
        "model = Model(inputs = [inputs], outputs = [out])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZmSNOwJJ2cf",
        "outputId": "c2bc469a-2d6c-4879-8a5e-53d7cc4768ec"
      },
      "source": [
        "# 3.2 Print model summary\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_14 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 100)          900         input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 100)          10100       dense_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 108)          0           dense_26[0][0]                   \n",
            "                                                                 input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 1)            109         concatenate_8[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 11,109\n",
            "Trainable params: 11,109\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBZwG9ht70_A",
        "outputId": "6c600044-b10c-48f2-862f-048ac0a284ad"
      },
      "source": [
        "# 3.3 `Model` groups layers into an object \r\n",
        "#       with training and inference features.\r\n",
        "help(Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Model in module tensorflow.python.keras.engine.training:\n",
            "\n",
            "class Model(tensorflow.python.keras.engine.base_layer.Layer, tensorflow.python.keras.utils.version_utils.ModelVersionSelector)\n",
            " |  `Model` groups layers into an object with training and inference features.\n",
            " |  \n",
            " |  Arguments:\n",
            " |      inputs: The input(s) of the model: a `keras.Input` object or list of\n",
            " |          `keras.Input` objects.\n",
            " |      outputs: The output(s) of the model. See Functional API example below.\n",
            " |      name: String, the name of the model.\n",
            " |  \n",
            " |  There are two ways to instantiate a `Model`:\n",
            " |  \n",
            " |  1 - With the \"Functional API\", where you start from `Input`,\n",
            " |  you chain layer calls to specify the model's forward pass,\n",
            " |  and finally you create your model from inputs and outputs:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  inputs = tf.keras.Input(shape=(3,))\n",
            " |  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n",
            " |  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
            " |  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
            " |  ```\n",
            " |  \n",
            " |  2 - By subclassing the `Model` class: in that case, you should define your\n",
            " |  layers in `__init__` and you should implement the model's forward pass\n",
            " |  in `call`.\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super(MyModel, self).__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |  \n",
            " |    def call(self, inputs):\n",
            " |      x = self.dense1(inputs)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  If you subclass `Model`, you can optionally have\n",
            " |  a `training` argument (boolean) in `call`, which you can use to specify\n",
            " |  a different behavior in training and inference:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super(MyModel, self).__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |      self.dropout = tf.keras.layers.Dropout(0.5)\n",
            " |  \n",
            " |    def call(self, inputs, training=False):\n",
            " |      x = self.dense1(inputs)\n",
            " |      if training:\n",
            " |        x = self.dropout(x, training=training)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  Once the model is created, you can config the model with losses and metrics\n",
            " |  with `model.compile()`, train the model with `model.fit()`, or use the model\n",
            " |  to do prediction with `model.predict()`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Model\n",
            " |      tensorflow.python.keras.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
            " |      tensorflow.python.keras.utils.version_utils.ModelVersionSelector\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *args, **kwargs)\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Builds the model based on input shapes received.\n",
            " |      \n",
            " |      This is to be used for subclassed models, which do not know at instantiation\n",
            " |      time what their inputs look like.\n",
            " |      \n",
            " |      This method only exists for users who want to call `model.build()` in a\n",
            " |      standalone way (as a substitute for calling the model on real data to\n",
            " |      build it). It will never be called by the framework (and thus it will\n",
            " |      never throw unexpected errors in an unrelated workflow).\n",
            " |      \n",
            " |      Args:\n",
            " |       input_shape: Single tuple, TensorShape, or list/dict of shapes, where\n",
            " |           shapes are tuples, integers, or TensorShapes.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError:\n",
            " |          1. In case of invalid user-provided data (not of type tuple,\n",
            " |             list, TensorShape, or dict).\n",
            " |          2. If the model requires call arguments that are agnostic\n",
            " |             to the input shapes (positional or kwarg in call signature).\n",
            " |          3. If not all layers were properly built.\n",
            " |          4. If float type inputs are not supported within the layers.\n",
            " |      \n",
            " |        In each of these cases, the user should build their model by calling it\n",
            " |        on real tensor data.\n",
            " |  \n",
            " |  call(self, inputs, training=None, mask=None)\n",
            " |      Calls the model on new inputs.\n",
            " |      \n",
            " |      In this case `call` just reapplies\n",
            " |      all ops in the graph to the new inputs\n",
            " |      (e.g. build a new computational graph from the provided inputs).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: A tensor or list of tensors.\n",
            " |          training: Boolean or boolean scalar tensor, indicating whether to run\n",
            " |            the `Network` in training mode or inference mode.\n",
            " |          mask: A mask or list of masks. A mask can be\n",
            " |              either a tensor or None (no mask).\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor if there is a single output, or\n",
            " |          a list of tensors if there are more than one outputs.\n",
            " |  \n",
            " |  compile(self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, **kwargs)\n",
            " |      Configures the model for training.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          optimizer: String (name of optimizer) or optimizer instance. See\n",
            " |            `tf.keras.optimizers`.\n",
            " |          loss: String (name of objective function), objective function or\n",
            " |            `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n",
            " |            function is any callable with the signature `loss = fn(y_true,\n",
            " |            y_pred)`, where y_true = ground truth values with shape =\n",
            " |            `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse\n",
            " |            categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.\n",
            " |            y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It\n",
            " |            returns a weighted loss float tensor. If a custom `Loss` instance is\n",
            " |            used and reduction is set to NONE, return value has the shape\n",
            " |            [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values;\n",
            " |            otherwise, it is a scalar. If the model has multiple outputs, you can\n",
            " |            use a different loss on each output by passing a dictionary or a list\n",
            " |            of losses. The loss value that will be minimized by the model will\n",
            " |            then be the sum of all individual losses.\n",
            " |          metrics: List of metrics to be evaluated by the model during training\n",
            " |            and testing. Each of this can be a string (name of a built-in\n",
            " |            function), function or a `tf.keras.metrics.Metric` instance. See\n",
            " |            `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n",
            " |            function is any callable with the signature `result = fn(y_true,\n",
            " |            y_pred)`. To specify different metrics for different outputs of a\n",
            " |            multi-output model, you could also pass a dictionary, such as\n",
            " |              `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n",
            " |                You can also pass a list (len = len(outputs)) of lists of metrics\n",
            " |                such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n",
            " |                `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
            " |                strings 'accuracy' or 'acc', we convert this to one of\n",
            " |                `tf.keras.metrics.BinaryAccuracy`,\n",
            " |                `tf.keras.metrics.CategoricalAccuracy`,\n",
            " |                `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n",
            " |                function used and the model output shape. We do a similar\n",
            " |                conversion for the strings 'crossentropy' and 'ce' as well.\n",
            " |          loss_weights: Optional list or dictionary specifying scalar coefficients\n",
            " |            (Python floats) to weight the loss contributions of different model\n",
            " |            outputs. The loss value that will be minimized by the model will then\n",
            " |            be the *weighted sum* of all individual losses, weighted by the\n",
            " |            `loss_weights` coefficients.\n",
            " |              If a list, it is expected to have a 1:1 mapping to the model's\n",
            " |                outputs. If a dict, it is expected to map output names (strings)\n",
            " |                to scalar coefficients.\n",
            " |          weighted_metrics: List of metrics to be evaluated and weighted by\n",
            " |            sample_weight or class_weight during training and testing.\n",
            " |          run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
            " |            logic will not be wrapped in a `tf.function`. Recommended to leave\n",
            " |            this as `None` unless your `Model` cannot be run inside a\n",
            " |            `tf.function`.\n",
            " |          steps_per_execution: Int. Defaults to 1. The number of batches to\n",
            " |            run during each `tf.function` call. Running multiple batches\n",
            " |            inside a single `tf.function` call can greatly improve performance\n",
            " |            on TPUs or small models with a large Python overhead.\n",
            " |            At most, one full epoch will be run each\n",
            " |            execution. If a number larger than the size of the epoch is passed,\n",
            " |            the execution will be truncated to the size of the epoch.\n",
            " |            Note that if `steps_per_execution` is set to `N`,\n",
            " |            `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n",
            " |            will only be called every `N` batches\n",
            " |            (i.e. before/after each `tf.function` execution).\n",
            " |          **kwargs: Arguments supported for backwards compatibility only.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: In case of invalid arguments for\n",
            " |              `optimizer`, `loss` or `metrics`.\n",
            " |  \n",
            " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False)\n",
            " |      Returns the loss value & metrics values for the model in test mode.\n",
            " |      \n",
            " |      Computation is done in batches (see the `batch_size` arg.)\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely). If\n",
            " |            `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\n",
            " |            should not be specified (since targets will be obtained from the\n",
            " |            iterator/dataset).\n",
            " |          batch_size: Integer or `None`. Number of samples per batch of\n",
            " |            computation. If unspecified, `batch_size` will default to 32. Do not\n",
            " |            specify the `batch_size` if your data is in the form of a dataset,\n",
            " |            generators, or `keras.utils.Sequence` instances (since they generate\n",
            " |            batches).\n",
            " |          verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n",
            " |          sample_weight: Optional Numpy array of weights for the test samples,\n",
            " |            used for weighting the loss function. You can either pass a flat (1D)\n",
            " |            Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples), or in the case of\n",
            " |                temporal data, you can pass a 2D array with shape `(samples,\n",
            " |                sequence_length)`, to apply a different weight to every timestep\n",
            " |                of every sample. This argument is not supported when `x` is a\n",
            " |                dataset, instead pass sample weights as the third element of `x`.\n",
            " |          steps: Integer or `None`. Total number of steps (batches of samples)\n",
            " |            before declaring the evaluation round finished. Ignored with the\n",
            " |            default value of `None`. If x is a `tf.data` dataset and `steps` is\n",
            " |            None, 'evaluate' will run until the dataset is exhausted. This\n",
            " |            argument is not supported with array inputs.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances. List of\n",
            " |            callbacks to apply during evaluation. See\n",
            " |            [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |            input only. Maximum size for the generator queue. If unspecified,\n",
            " |            `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |            only. Maximum number of processes to spin up when using process-based\n",
            " |            threading. If unspecified, `workers` will default to 1. If 0, will\n",
            " |            execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |            threading. If unspecified, `use_multiprocessing` will default to\n",
            " |            `False`. Note that because this implementation relies on\n",
            " |            multiprocessing, you should not pass non-picklable arguments to the\n",
            " |            generator as they can't be passed easily to children processes.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.evaluate` is wrapped in `tf.function`.\n",
            " |          ValueError: in case of invalid arguments.\n",
            " |  \n",
            " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Evaluates the model on a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.evaluate` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given below.\n",
            " |          y: Target data. Like the input data `x`,\n",
            " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
            " |            tensor targets, or inversely). If `x` is a dataset, generator,\n",
            " |            or `keras.utils.Sequence` instance, `y` should\n",
            " |            not be specified (since targets will be obtained from `x`).\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per gradient update.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          epochs: Integer. Number of epochs to train the model.\n",
            " |              An epoch is an iteration over the entire `x` and `y`\n",
            " |              data provided.\n",
            " |              Note that in conjunction with `initial_epoch`,\n",
            " |              `epochs` is to be understood as \"final epoch\".\n",
            " |              The model is not trained for a number of iterations\n",
            " |              given by `epochs`, but merely until the epoch\n",
            " |              of index `epochs` is reached.\n",
            " |          verbose: 0, 1, or 2. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            " |              Note that the progress bar is not particularly useful when\n",
            " |              logged to a file, so verbose=2 is recommended when not running\n",
            " |              interactively (eg, in a production environment).\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training.\n",
            " |              See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
            " |              and `tf.keras.callbacks.History` callbacks are created automatically\n",
            " |              and need not be passed into `model.fit`.\n",
            " |              `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
            " |              `verbose` argument to `model.fit`.\n",
            " |          validation_split: Float between 0 and 1.\n",
            " |              Fraction of the training data to be used as validation data.\n",
            " |              The model will set apart this fraction of the training data,\n",
            " |              will not train on it, and will evaluate\n",
            " |              the loss and any model metrics\n",
            " |              on this data at the end of each epoch.\n",
            " |              The validation data is selected from the last samples\n",
            " |              in the `x` and `y` data provided, before shuffling. This argument is\n",
            " |              not supported when `x` is a dataset, generator or\n",
            " |             `keras.utils.Sequence` instance.\n",
            " |          validation_data: Data on which to evaluate\n",
            " |              the loss and any model metrics at the end of each epoch.\n",
            " |              The model will not be trained on this data. Thus, note the fact\n",
            " |              that the validation loss of data provided using `validation_split`\n",
            " |              or `validation_data` is not affected by regularization layers like\n",
            " |              noise and dropout.\n",
            " |              `validation_data` will override `validation_split`.\n",
            " |              `validation_data` could be:\n",
            " |                - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
            " |                - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
            " |                - dataset\n",
            " |              For the first two cases, `batch_size` must be provided.\n",
            " |              For the last case, `validation_steps` could be provided.\n",
            " |              Note that `validation_data` does not support all the data types that\n",
            " |              are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
            " |          shuffle: Boolean (whether to shuffle the training data\n",
            " |              before each epoch) or str (for 'batch'). This argument is ignored\n",
            " |              when `x` is a generator. 'batch' is a special option for dealing\n",
            " |              with the limitations of HDF5 data; it shuffles in batch-sized\n",
            " |              chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers)\n",
            " |              to a weight (float) value, used for weighting the loss function\n",
            " |              (during training only).\n",
            " |              This can be useful to tell the model to\n",
            " |              \"pay more attention\" to samples from\n",
            " |              an under-represented class.\n",
            " |          sample_weight: Optional Numpy array of weights for\n",
            " |              the training samples, used for weighting the loss function\n",
            " |              (during training only). You can either pass a flat (1D)\n",
            " |              Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples),\n",
            " |              or in the case of temporal data,\n",
            " |              you can pass a 2D array with shape\n",
            " |              `(samples, sequence_length)`,\n",
            " |              to apply a different weight to every timestep of every sample. This\n",
            " |              argument is not supported when `x` is a dataset, generator, or\n",
            " |             `keras.utils.Sequence` instance, instead provide the sample_weights\n",
            " |              as the third element of `x`.\n",
            " |          initial_epoch: Integer.\n",
            " |              Epoch at which to start training\n",
            " |              (useful for resuming a previous training run).\n",
            " |          steps_per_epoch: Integer or `None`.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              before declaring one epoch finished and starting the\n",
            " |              next epoch. When training with input tensors such as\n",
            " |              TensorFlow data tensors, the default `None` is equal to\n",
            " |              the number of samples in your dataset divided by\n",
            " |              the batch size, or 1 if that cannot be determined. If x is a\n",
            " |              `tf.data` dataset, and 'steps_per_epoch'\n",
            " |              is None, the epoch will run until the input dataset is exhausted.\n",
            " |              When passing an infinitely repeating dataset, you must specify the\n",
            " |              `steps_per_epoch` argument. This argument is not supported with\n",
            " |              array inputs.\n",
            " |          validation_steps: Only relevant if `validation_data` is provided and\n",
            " |              is a `tf.data` dataset. Total number of steps (batches of\n",
            " |              samples) to draw before stopping when performing validation\n",
            " |              at the end of every epoch. If 'validation_steps' is None, validation\n",
            " |              will run until the `validation_data` dataset is exhausted. In the\n",
            " |              case of an infinitely repeated dataset, it will run into an\n",
            " |              infinite loop. If 'validation_steps' is specified and only part of\n",
            " |              the dataset will be consumed, the evaluation will start from the\n",
            " |              beginning of the dataset at each epoch. This ensures that the same\n",
            " |              validation samples are used every time.\n",
            " |          validation_batch_size: Integer or `None`.\n",
            " |              Number of samples per validation batch.\n",
            " |              If unspecified, will default to `batch_size`.\n",
            " |              Do not specify the `validation_batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          validation_freq: Only relevant if validation data is provided. Integer\n",
            " |              or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
            " |              If an integer, specifies how many training epochs to run before a\n",
            " |              new validation run is performed, e.g. `validation_freq=2` runs\n",
            " |              validation every 2 epochs. If a Container, specifies the epochs on\n",
            " |              which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            " |              validation at the end of the 1st, 2nd, and 10th epochs.\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up\n",
            " |              when using process-based threading. If unspecified, `workers`\n",
            " |              will default to 1. If 0, will execute the generator on the main\n",
            " |              thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      Unpacking behavior for iterator-like inputs:\n",
            " |          A common pattern is to pass a tf.data.Dataset, generator, or\n",
            " |        tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            " |        yield not only features (x) but optionally targets (y) and sample weights.\n",
            " |        Keras requires that the output of such iterator-likes be unambiguous. The\n",
            " |        iterator should return a tuple of length 1, 2, or 3, where the optional\n",
            " |        second and third elements will be used for y and sample_weight\n",
            " |        respectively. Any other type provided will be wrapped in a length one\n",
            " |        tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
            " |        should still adhere to the top-level tuple structure.\n",
            " |        e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            " |        features, targets, and weights from the keys of a single dict.\n",
            " |          A notable unsupported data type is the namedtuple. The reason is that\n",
            " |        it behaves like both an ordered datatype (tuple) and a mapping\n",
            " |        datatype (dict). So given a namedtuple of the form:\n",
            " |            `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            " |        it is ambiguous whether to reverse the order of the elements when\n",
            " |        interpreting the value. Even worse is a tuple of the form:\n",
            " |            `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            " |        where it is unclear if the tuple was intended to be unpacked into x, y,\n",
            " |        and sample_weight or passed through as a single element to `x`. As a\n",
            " |        result the data processing code will simply raise a ValueError if it\n",
            " |        encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          A `History` object. Its `History.history` attribute is\n",
            " |          a record of training loss values and metrics values\n",
            " |          at successive epochs, as well as validation loss values\n",
            " |          and validation metrics values (if applicable).\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: 1. If the model was never compiled or,\n",
            " |          2. If `model.fit` is  wrapped in `tf.function`.\n",
            " |      \n",
            " |          ValueError: In case of mismatch between the provided input data\n",
            " |              and what the model expects or when the input data is empty.\n",
            " |  \n",
            " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
            " |      Fits the model on data yielded batch-by-batch by a Python generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.fit` now supports generators, so there is no longer any need to use\n",
            " |        this endpoint.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  get_layer(self, name=None, index=None)\n",
            " |      Retrieves a layer based on either its name (unique) or index.\n",
            " |      \n",
            " |      If `name` and `index` are both provided, `index` will take precedence.\n",
            " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          name: String, name of layer.\n",
            " |          index: Integer, index of layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: In case of invalid layer name or index.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Retrieves the weights of the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A flat list of Numpy arrays.\n",
            " |  \n",
            " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None)\n",
            " |      Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n",
            " |      \n",
            " |      If `by_name` is False weights are loaded based on the network's\n",
            " |      topology. This means the architecture should be the same as when the weights\n",
            " |      were saved.  Note that layers that don't have weights are not taken into\n",
            " |      account in the topological ordering, so adding or removing layers is fine as\n",
            " |      long as they don't have weights.\n",
            " |      \n",
            " |      If `by_name` is True, weights are loaded into layers only if they share the\n",
            " |      same name. This is useful for fine-tuning or transfer-learning models where\n",
            " |      some of the layers have changed.\n",
            " |      \n",
            " |      Only topological loading (`by_name=False`) is supported when loading weights\n",
            " |      from the TensorFlow format. Note that topological loading differs slightly\n",
            " |      between TensorFlow and HDF5 formats for user-defined classes inheriting from\n",
            " |      `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n",
            " |      TensorFlow format loads based on the object-local names of attributes to\n",
            " |      which layers are assigned in the `Model`'s constructor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String, path to the weights file to load. For weight files in\n",
            " |              TensorFlow format, this is the file prefix (the same as was passed\n",
            " |              to `save_weights`).\n",
            " |          by_name: Boolean, whether to load weights by name or by topological\n",
            " |              order. Only topological loading is supported for weight files in\n",
            " |              TensorFlow format.\n",
            " |          skip_mismatch: Boolean, whether to skip loading of layers where there is\n",
            " |              a mismatch in the number of weights, or a mismatch in the shape of\n",
            " |              the weight (only valid when `by_name=True`).\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for loading weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          When loading a weight file in TensorFlow format, returns the same status\n",
            " |          object as `tf.train.Checkpoint.restore`. When graph building, restore\n",
            " |          ops are run automatically as soon as the network is built (on first call\n",
            " |          for user-defined classes inheriting from `Model`, immediately if it is\n",
            " |          already built).\n",
            " |      \n",
            " |          When loading weights in HDF5 format, returns `None`.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If h5py is not available and the weight file is in HDF5\n",
            " |              format.\n",
            " |          ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n",
            " |            `False`.\n",
            " |  \n",
            " |  make_predict_function(self)\n",
            " |      Creates a function that executes one step of inference.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.predict` and `Model.predict_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.predict_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.predict` or\n",
            " |      `Model.predict_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return the outputs of the `Model`.\n",
            " |  \n",
            " |  make_test_function(self)\n",
            " |      Creates a function that executes one step of evaluation.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.evaluate` and `Model.test_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.test_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.evaluate` or\n",
            " |      `Model.test_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_test_batch_end`.\n",
            " |  \n",
            " |  make_train_function(self)\n",
            " |      Creates a function that executes one step of training.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.fit` and `Model.train_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual training\n",
            " |      logic to `Model.train_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.fit` or\n",
            " |      `Model.train_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Generates output predictions for the input samples.\n",
            " |      \n",
            " |      Computation is done in batches. This method is designed for performance in\n",
            " |      large scale inputs. For small amount of inputs that fit in one batch,\n",
            " |      directly using `__call__` is recommended for faster execution, e.g.,\n",
            " |      `model(x)`, or `model(x, training=False)` if you have layers such as\n",
            " |      `tf.keras.layers.BatchNormalization` that behaves differently during\n",
            " |      inference. Also, note the fact that test loss is not affected by\n",
            " |      regularization layers like noise and dropout.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input samples. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A `tf.data` dataset.\n",
            " |            - A generator or `keras.utils.Sequence` instance.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per batch.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of dataset, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          verbose: Verbosity mode, 0 or 1.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              before declaring the prediction round finished.\n",
            " |              Ignored with the default value of `None`. If x is a `tf.data`\n",
            " |              dataset and `steps` is None, `predict` will\n",
            " |              run until the input dataset is exhausted.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during prediction.\n",
            " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up when using\n",
            " |              process-based threading. If unspecified, `workers` will default\n",
            " |              to 1. If 0, will execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`. Note that Model.predict uses the same interpretation rules as\n",
            " |      `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n",
            " |      three methods.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of mismatch between the provided\n",
            " |              input data and the model's expectations,\n",
            " |              or in case a stateful model receives a number of samples\n",
            " |              that is not a multiple of the batch size.\n",
            " |  \n",
            " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Generates predictions for the input samples from a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.predict` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  predict_on_batch(self, x)\n",
            " |      Returns predictions for a single batch of samples.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
            " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
            " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict_on_batch` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of mismatch between given number of inputs and\n",
            " |            expectations of the model.\n",
            " |  \n",
            " |  predict_step(self, data)\n",
            " |      The logic for one inference step.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.make_predict_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of inference.\n",
            " |      This typically includes the forward pass.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_predict_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The result of one inference step, typically the output of calling the\n",
            " |        `Model` on data.\n",
            " |  \n",
            " |  reset_metrics(self)\n",
            " |      Resets the state of all the metrics in the model.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> _ = model.fit(x, y, verbose=0)\n",
            " |      >>> assert all(float(m.result()) for m in model.metrics)\n",
            " |      \n",
            " |      >>> model.reset_metrics()\n",
            " |      >>> assert all(float(m.result()) == 0 for m in model.metrics)\n",
            " |  \n",
            " |  reset_states(self)\n",
            " |  \n",
            " |  save(self, filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None, save_traces=True)\n",
            " |      Saves the model to Tensorflow SavedModel or a single HDF5 file.\n",
            " |      \n",
            " |      Please see `tf.keras.models.save_model` or the\n",
            " |      [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)\n",
            " |      for details.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String, PathLike, path to SavedModel or H5 file to save the\n",
            " |              model.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          include_optimizer: If True, save optimizer's state together.\n",
            " |          save_format: Either `'tf'` or `'h5'`, indicating whether to save the\n",
            " |              model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,\n",
            " |              and 'h5' in TF 1.X.\n",
            " |          signatures: Signatures to save with the SavedModel. Applicable to the\n",
            " |              'tf' format only. Please see the `signatures` argument in\n",
            " |              `tf.saved_model.save` for details.\n",
            " |          options: (only applies to SavedModel format)\n",
            " |              `tf.saved_model.SaveOptions` object that specifies options for\n",
            " |              saving to SavedModel.\n",
            " |          save_traces: (only applies to SavedModel format) When enabled, the\n",
            " |              SavedModel will store the function traces for each layer. This\n",
            " |              can be disabled, so that only the configs of each layer are stored.\n",
            " |              Defaults to `True`. Disabling this will decrease serialization time\n",
            " |              and reduce file size, but it requires that all custom layers/models\n",
            " |              implement a `get_config()` method.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      from keras.models import load_model\n",
            " |      \n",
            " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
            " |      del model  # deletes the existing model\n",
            " |      \n",
            " |      # returns a compiled model\n",
            " |      # identical to the previous one\n",
            " |      model = load_model('my_model.h5')\n",
            " |      ```\n",
            " |  \n",
            " |  save_weights(self, filepath, overwrite=True, save_format=None, options=None)\n",
            " |      Saves all layer weights.\n",
            " |      \n",
            " |      Either saves in HDF5 or in TensorFlow format based on the `save_format`\n",
            " |      argument.\n",
            " |      \n",
            " |      When saving in HDF5 format, the weight file has:\n",
            " |        - `layer_names` (attribute), a list of strings\n",
            " |            (ordered names of model layers).\n",
            " |        - For every layer, a `group` named `layer.name`\n",
            " |            - For every such layer group, a group attribute `weight_names`,\n",
            " |                a list of strings\n",
            " |                (ordered names of weights tensor of the layer).\n",
            " |            - For every weight in the layer, a dataset\n",
            " |                storing the weight value, named after the weight tensor.\n",
            " |      \n",
            " |      When saving in TensorFlow format, all objects referenced by the network are\n",
            " |      saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n",
            " |      instances or `Optimizer` instances assigned to object attributes. For\n",
            " |      networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n",
            " |      outputs)`, `Layer` instances used by the network are tracked/saved\n",
            " |      automatically. For user-defined classes which inherit from `tf.keras.Model`,\n",
            " |      `Layer` instances must be assigned to object attributes, typically in the\n",
            " |      constructor. See the documentation of `tf.train.Checkpoint` and\n",
            " |      `tf.keras.Model` for details.\n",
            " |      \n",
            " |      While the formats are the same, do not mix `save_weights` and\n",
            " |      `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n",
            " |      loaded using `Model.load_weights`. Checkpoints saved using\n",
            " |      `tf.train.Checkpoint.save` should be restored using the corresponding\n",
            " |      `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n",
            " |      `save_weights` for training checkpoints.\n",
            " |      \n",
            " |      The TensorFlow format matches objects and variables by starting at a root\n",
            " |      object, `self` for `save_weights`, and greedily matching attribute\n",
            " |      names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n",
            " |      is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n",
            " |      means saving a `tf.keras.Model` using `save_weights` and loading into a\n",
            " |      `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n",
            " |      the `Model`'s variables. See the [guide to training\n",
            " |      checkpoints](https://www.tensorflow.org/guide/checkpoint) for details\n",
            " |      on the TensorFlow format.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String or PathLike, path to the file to save the weights to.\n",
            " |              When saving in TensorFlow format, this is the prefix used for\n",
            " |              checkpoint files (multiple files are generated). Note that the '.h5'\n",
            " |              suffix causes weights to be saved in HDF5 format.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n",
            " |              '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n",
            " |              `None` defaults to 'tf'.\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for saving weights.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If h5py is not available when attempting to save in HDF5\n",
            " |              format.\n",
            " |          ValueError: For invalid/unknown format arguments.\n",
            " |  \n",
            " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
            " |      Prints a string summary of the network.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          line_length: Total length of printed lines\n",
            " |              (e.g. set this to adapt the display to different\n",
            " |              terminal window sizes).\n",
            " |          positions: Relative or absolute positions of log elements\n",
            " |              in each line. If not provided,\n",
            " |              defaults to `[.33, .55, .67, 1.]`.\n",
            " |          print_fn: Print function to use. Defaults to `print`.\n",
            " |              It will be called on each line of the summary.\n",
            " |              You can set it to a custom function\n",
            " |              in order to capture the string summary.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if `summary()` is called before the model is built.\n",
            " |  \n",
            " |  test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Test the model on a single batch of samples.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
            " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
            " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors, if\n",
            " |            the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of invalid user-provided arguments.\n",
            " |  \n",
            " |  test_step(self, data)\n",
            " |      The logic for one evaluation step.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.make_test_function`.\n",
            " |      \n",
            " |      This function should contain the mathematical logic for one step of\n",
            " |      evaluation.\n",
            " |      This typically includes the forward pass, loss calculation, and metrics\n",
            " |      updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_test_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned.\n",
            " |  \n",
            " |  to_json(self, **kwargs)\n",
            " |      Returns a JSON string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a JSON save file, use\n",
            " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `json.dumps()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A JSON string.\n",
            " |  \n",
            " |  to_yaml(self, **kwargs)\n",
            " |      Returns a yaml string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a yaml save file, use\n",
            " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
            " |      \n",
            " |      `custom_objects` should be a dictionary mapping\n",
            " |      the names of custom losses / layers / etc to the corresponding\n",
            " |      functions / classes.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `yaml.dump()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A YAML string.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: if yaml module is not found.\n",
            " |  \n",
            " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Runs a single gradient update on a single batch of data.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |                if the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers) to a\n",
            " |            weight (float) to apply to the model's loss for the samples from this\n",
            " |            class during training. This can be useful to tell the model to \"pay\n",
            " |            more attention\" to samples from an under-represented class.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar training loss\n",
            " |          (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If `model.train_on_batch` is wrapped in `tf.function`.\n",
            " |        ValueError: In case of invalid user-provided arguments.\n",
            " |  \n",
            " |  train_step(self, data)\n",
            " |      The logic for one training step.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.make_train_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of training.\n",
            " |      This typically includes the forward pass, loss calculation, backpropagation,\n",
            " |      and metric updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_train_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned. Example:\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_config(config, custom_objects=None) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  distribute_strategy\n",
            " |      The `tf.distribute.Strategy` this model was created under.\n",
            " |  \n",
            " |  layers\n",
            " |  \n",
            " |  metrics\n",
            " |      Returns the model's metrics added using `compile`, `add_metric` APIs.\n",
            " |      \n",
            " |      Note: Metrics passed to `compile()` are available only after a `keras.Model`\n",
            " |      has been trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.add_metric(\n",
            " |      ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc', 'mean']\n",
            " |  \n",
            " |  metrics_names\n",
            " |      Returns the model's display labels for all outputs.\n",
            " |      \n",
            " |      Note: `metrics_names` are available only after a `keras.Model` has been\n",
            " |      trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> model.metrics_names\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc']\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Non-trainable weights are *not* updated during training. They are expected\n",
            " |      to be updated manually in `call()`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of non-trainable variables.\n",
            " |  \n",
            " |  run_eagerly\n",
            " |      Settable attribute indicating whether the model should run eagerly.\n",
            " |      \n",
            " |      Running eagerly means that your model will be run step by step,\n",
            " |      like Python code. Your model might run slower, but it should become easier\n",
            " |      for you to debug it by stepping into individual layer calls.\n",
            " |      \n",
            " |      By default, we will attempt to compile your model to a static graph to\n",
            " |      deliver the best execution performance.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Boolean, whether the model should run eagerly.\n",
            " |  \n",
            " |  state_updates\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Returns the `updates` from all layers that are stateful.\n",
            " |      \n",
            " |      This is useful for separating training updates and\n",
            " |      state updates, e.g. when we need to update a layer's internal state\n",
            " |      during prediction.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of update ops.\n",
            " |  \n",
            " |  trainable_weights\n",
            " |      List of all trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Trainable weights are updated via gradient descent during training.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of trainable variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        *args: Positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
            " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_loss(self, losses, **kwargs)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be dependent\n",
            " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
            " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
            " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(self, inputs):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any loss Tensors passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      losses become part of the model's topology and are tracked in `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Activity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss references\n",
            " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
            " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
            " |      topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      d = tf.keras.layers.Dense(10)\n",
            " |      x = d(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      Arguments:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
            " |          may also be zero-argument callables which create a loss tensor.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |            inputs - Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_metric(self, value, name=None, **kwargs)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      This method can be used inside the `call()` method of a subclassed layer\n",
            " |      or model.\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
            " |        def __init__(self):\n",
            " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
            " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
            " |      \n",
            " |        def call(self, inputs):\n",
            " |          self.add_metric(self.mean(x))\n",
            " |          self.add_metric(tf.reduce_sum(x), name='metric_2')\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any tensor passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      metrics become part of the model's topology and are tracked when you\n",
            " |      save the model via `save()`.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
            " |      Functional Model, as shown in the example below, is not supported. This is\n",
            " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        name: String metric name.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |          `aggregation` - When the `value` tensor provided is not the result of\n",
            " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
            " |          using a `keras.Metric.Mean`.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Add update op(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and variance\n",
            " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
            " |      when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case, variable\n",
            " |      updates are run on the fly and thus do not need to be tracked for later\n",
            " |      execution).\n",
            " |      \n",
            " |      Arguments:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |        inputs: Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        use_resource: Whether to use `ResourceVariable`.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            " |          `AUTO` and the current `DistributionStrategy` chooses\n",
            " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
            " |          `trainable` must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The variable created.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
            " |  \n",
            " |  apply(self, inputs, *args, **kwargs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      This is an alias of `self.__call__`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      If the layer has not been built, this method will call `build` on the\n",
            " |      layer. This assumes that the layer will later be used with inputs that\n",
            " |      match the input shape provided here.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
            " |          how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves losses relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of loss tensors of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves updates relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of update ops of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from Numpy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      sets the weight values from numpy arrays. The weight values should be\n",
            " |      passed in the order they are created by the layer. Note that the layer's\n",
            " |      weights must be instantiated before calling this function by calling\n",
            " |      the layer.\n",
            " |      \n",
            " |      For example, a Dense layer returns a list of two values-- per-output\n",
            " |      weights and the bias value. These can be used to set the weights of another\n",
            " |      Dense layer:\n",
            " |      \n",
            " |      >>> a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b.set_weights(a.get_weights())\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Arguments:\n",
            " |          weights: a list of Numpy arrays. The number\n",
            " |              of arrays and their shape must match\n",
            " |              number of the dimensions of the weights\n",
            " |              of the layer (i.e. it should match the\n",
            " |              output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the provided weights list does not match the\n",
            " |              layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  compute_dtype\n",
            " |      The dtype of the layer's computations.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
            " |      the weights.\n",
            " |      \n",
            " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
            " |      computations and the output to be in the compute dtype as well. This is done\n",
            " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
            " |      these casts if implementing your own layer.\n",
            " |      \n",
            " |      Layers often perform certain internal computations in higher precision when\n",
            " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
            " |      will still typically be float16 or bfloat16 in such cases.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The layer's compute dtype.\n",
            " |  \n",
            " |  dtype\n",
            " |      The dtype of the layer weights.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
            " |      dtype of the layer's computations.\n",
            " |  \n",
            " |  dtype_policy\n",
            " |      The dtype policy associated with this layer.\n",
            " |      \n",
            " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
            " |  \n",
            " |  dynamic\n",
            " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  input_spec\n",
            " |      `InputSpec` instance(s) describing the input format for this layer.\n",
            " |      \n",
            " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
            " |      the layer to run input compatibility checks when it is called.\n",
            " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
            " |      of rank 4. As such, you can set, in `__init__()`:\n",
            " |      \n",
            " |      ```python\n",
            " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
            " |      ```\n",
            " |      \n",
            " |      Now, if you try to call the layer on an input that isn't rank 4\n",
            " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
            " |      error:\n",
            " |      \n",
            " |      ```\n",
            " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
            " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
            " |      ```\n",
            " |      \n",
            " |      Input checks that can be specified via `input_spec` include:\n",
            " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
            " |      - Shape\n",
            " |      - Rank (ndim)\n",
            " |      - Dtype\n",
            " |      \n",
            " |      For more information, see `tf.keras.layers.InputSpec`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
            " |  \n",
            " |  losses\n",
            " |      List of losses added using the `add_loss()` API.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is accessed,\n",
            " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
            " |      propagate gradients back to the corresponding variables.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
            " |      ...   def call(self, inputs):\n",
            " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |      ...     return inputs\n",
            " |      >>> l = MyLayer()\n",
            " |      >>> l(np.ones((10, 1)))\n",
            " |      >>> l.losses\n",
            " |      [1.0]\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Activity regularization.\n",
            " |      >>> len(model.losses)\n",
            " |      0\n",
            " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      >>> len(model.losses)\n",
            " |      1\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
            " |      >>> x = d(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Weight regularization.\n",
            " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      >>> model.losses\n",
            " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  name\n",
            " |      Name of the layer (string), set in the constructor.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  supports_masking\n",
            " |      Whether this layer supports computing a mask using `compute_mask`.\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variable_dtype\n",
            " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      >>> class MyModule(tf.Module):\n",
            " |      ...   @tf.Module.with_name_scope\n",
            " |      ...   def __call__(self, x):\n",
            " |      ...     if not hasattr(self, 'w'):\n",
            " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
            " |      ...     return tf.matmul(x, self.w)\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      >>> mod = MyModule()\n",
            " |      >>> mod(tf.ones([1, 2]))\n",
            " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
            " |      >>> mod.w\n",
            " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
            " |      numpy=..., dtype=float32)>\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      >>> a = tf.Module()\n",
            " |      >>> b = tf.Module()\n",
            " |      >>> c = tf.Module()\n",
            " |      >>> a.b = b\n",
            " |      >>> b.c = c\n",
            " |      >>> list(a.submodules) == [b, c]\n",
            " |      True\n",
            " |      >>> list(b.submodules) == [c]\n",
            " |      True\n",
            " |      >>> list(c.submodules) == []\n",
            " |      True\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "2ex5iFJiHkDR",
        "outputId": "0ef6bc23-e7db-47f1-e27c-bca93fa986dc"
      },
      "source": [
        "# 3.4 Display model now\r\n",
        "# Ref: https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model\r\n",
        "plot_model(model, show_shapes= True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAAIECAIAAACiyHIsAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzde0ATR/4A8ElISAgkEOQpCvJUERWtWglaamlR4QSRolS0VduKqEV8FRFBBHwVDzks1PoovZNWAaFAFbRVDy2K/GqVgvhCBBQRARESILzC/v7Y3l4OIYQQsoF8P381M8PsdwfLN7s7O0PBMAwBAAAAQ49KdgAAAABUBaQcAAAACgIpBwAAgIJAygEAAKAgNLIDUHb5+fkxMTFkRwEAGJlSU1PJDkGh4CqnH8+ePTt79izZUYBh4+bNmzdv3iQ7iiFXVVUF/18MkmqOIVzlSEXVvokAmXl7eyMV+AeTkpKybNmyEX+aQwofQ7KjUDS4ygEAAKAgkHIAAAAoCKQcAAAACgIpBwAAgIJAygEAAKAgkHIAIF92dra2tvbPP/9MdiBytm7dOsp/rFixQrzq0qVLwcHBaWlpFhYWeIOVK1eKN3BxcWGz2WpqapMmTbp9+7ZiA/+vH3/8cebMmWw228zMbPXq1TU1NXh5VlbWwYMHRSIR0TIjI4M4WT09PZLiVXaQcgAg3whe0F1XVzcnJ+fhw4cnT54kCnfv3h0XF7dz504vL68nT55YWlqOGjUqKSnp/PnzRJtffvklNTV10aJFJSUl06dPJyN2lJyc7Ovr6+3tXVVVlZmZee3atYULF3Z1dSGE3N3dmUyms7NzY2Mj3tjDw6OqquratWuurq6kRDssQMoBgHxubm5NTU2LFi0a6gMJhUIejzfURxGnoaGxYMECGxsbBoOBlxw4cODMmTMpKSlsNptoFhcXR6VS/fz8mpqaFBmeZN9+++3o0aO3b9+ura1tb2+/ZcuWwsLCgoICvHbTpk1Tp051dXXFkxCFQjExMZk7d661tTWpUSs1SDkAqJCTJ0/W1taSGMDjx49DQ0P37NnDZDLFy3k8XmBg4PPnz7dt20ZWbG969uyZsbExhULBP44dOxYhVFlZSTQIDw8vLCyMjY0lJ75hCFIOACTLy8szNTWlUChff/01QighIUFTU5PFYmVmZi5cuJDD4YwZM+b06dN447i4OCaTaWBgsG7dOmNjYyaTyePxiO/dAQEB6urqRkZG+McNGzZoampSKJT6+nqEUGBg4NatW8vKyigUipWVFULowoULHA5n7969CjvZuLg4DMPc3d3frIqKirKxsTlx4sSlS5d6/VkMw2JiYiZOnMhgMLhc7uLFix88eIBXSR40hJBIJAoLCzM1NdXQ0JgyZUpycrI00VpYWIhnaPxBjoWFBVHC5XKdnJxiY2NH8K1ROcOARPg/TbKjAMPGhx9++OGHHw70p549e4YQOnLkCP4xJCQEIXT58uWmpqba2tq5c+dqamp2dHTgtX5+fpqamvfu3WtrayspKcEfbj99+hSv9fX1NTQ0JHqOjo5GCNXV1eEfvby8LC0tidpz586x2eyIiIiBBizl/xd+fn4mJibiJRYWFra2tj2aWVpalpeXYxh248YNKpU6bty45uZmDMNycnI8PDyIZmFhYerq6qdOnWpsbCwqKpo+fbqenl5NTQ1eK3nQtm3bxmAwzp49+/r16507d1Kp1N9//73f+HNzc+l0elxcHJ/Pv3v37sSJE+fPn9+jTXBwMELozp07RMmmTZtGjRrVb+eq+bcFrnIAUFI8Ho/D4ejr6/v4+LS0tDx9+pSootFo+Jd9W1vbhIQEgUCQmJgowyHc3Nz4fH5oaKj8opakpaWlvLzc0tKyrwYODg6bN2+uqKjYsWNHjyqhUBgTE7NkyZIVK1Zoa2tPnjz56NGj9fX1x44dE2/W66C1tbUlJCR4enp6eXnp6Ojs2rWLTqdLM2JOTk5BQUEBAQEcDsfOzk4gEJw4caJHG/zJTXFxsZSDoOIg5QCg7NTV1RFCnZ2dvdbOmDGDxWIRt5iUWW1tLYZhLBZLQpuoqKjx48fHx8fn5eWJl5eUlDQ3N8+YMYMomTlzprq6OnFTsQfxQXv48GFra6udnR1epaGhYWRkJM2IhYSEHDt27PLly83NzU+ePOHxeA4ODvglKQE/nZcvX/bbG0CQcgAYARgMRl1dHdlR9K+trQ0hRExd6xWTyUxMTKRQKGvWrBEKhUQ5PhdZS0tLvLGOjo5AIOj3uC0tLQihXbt2Ee/NVFZWtra2Sv6pFy9eHDx4cO3ate+9956mpqa5ufnx48erq6vxe5UEDQ0N4tRAvyDlADC8dXZ2NjY2jhkzhuxA+of/dRZ/fbJXDg4OW7ZsKS0tjYyMJAp1dHQQQj0SjJQnrq+vjxA6fPiw+EOF/Px8yT9VWloqEolGjx5NlHA4HF1d3ZKSEvFmHR0dxKmBfkHKAWB4y83NxTBs9uzZ+EcajdbXLTjSGRgYUCgUad68iYyMnDBhwp07d4gSOzs7LS2tW7duESUFBQUdHR1vvfVWv72NHTuWyWQWFhYOKFo8mb148YIoEQgEDQ0N+FRpAn46hoaGA+pcZUHKAWD46e7ufv36dVdXV1FRUWBgoKmp6apVq/AqKyurhoaGjIyMzs7Ouro68ZdIEEK6urrV1dUVFRUCgaCzszMnJ0eRk6RZLJaFhUVVVVW/LfHba2pqauIlW7duTU9PT0pK4vP5xcXF/v7+xsbGfn5+0vS2evXq06dPJyQk8Pl8kUhUVVWF5xIfHx9DQ8NeF9QxNzefN2/e8ePHr127JhQKnz17hh/r008/FW+Gn87kyZP7DQMgpHpT9AZKNScyApnJMEn6yJEj+Js0LBbL3d09Pj4efyJtbW1dVlZ27NgxDoeDEDIzM3v06BGGYX5+fnQ63cTEhEajcTicxYsXl5WVEb29evVq3rx5TCbT3Nz8iy++2L59O0LIysoKn0V9+/ZtMzMzDQ2NOXPm1NTUZGdns9nsqKiogZ6mzJOkAwIC6HR6a2sr/jE9PR2fwKanp7dx48YeP759+3bxSdLd3d3R0dHW1tZ0Op3L5Xp6ej58+BCv6nfQ2tvbg4KCTE1NaTSavr6+l5dXSUkJhmGenp4IobCwsF7jr6+vDwwMtLKyYjAYWlpajo6OP/30U482bm5uJiYm3d3dRAlMkpZA5U54oFTznwWQmWzv5QyIn5+frq7ukB6iXzKnnNLSUhqNdurUqSELbWBEItHcuXNPnjwp24/X19czmcxDhw6JF0LKkQBurAEw/PT7BF55CIXCixcvlpaW4o/ZraysIiIiIiIimpubyQ4NiUSijIwMgUDg4+MjWw/h4eH29vYBAQEIIQzDqqur8/LyHj9+LNcwRxRIOQCAIdTQ0IAv67lmzRq8JDg42Nvb28fHh/QVPHNzc9PS0nJyciS/KtSXmJiYwsLC7OxsOp2OEMrMzMSX9RRfDxv0AClHPpR5v5Pu7u7Dhw9LXj+4ra1twoQJu3btkqbDmzdvTpw4kUqlUigUQ0PDqKgoOUXaP/HtVYyMjHpswaIKdu7cmZiY2NTUZG5ufvbsWbLD6cfRo0eJOypJSUlE+d69ewMCAvbv309ibAghZ2fnH374gViSbkAyMzPb29tzc3O5XC5esnjxYuJk8UXtwJtoZAcwQmDKuqhfaWnp6tWrr1+/PnXqVAnNQkJCHj58KGWfs2fPvn///oIFCy5evPjw4UP8hQnF8PLy8vLysrKyqq+vJzbLUin79u3bt28f2VHIgYuLi4uLC9lRyM7Dw8PDw4PsKIYfuMqRD+Xc7+TPP//csWOHv7+/vb29hGY3bty4e/euPKIbEorf4gUAMEQg5QwzA9rvZOrUqWlpab6+vhKWGBEKhdu3b1fmDT9I3+IFACAvkHLkgMT9TgYvJCRkw4YN+Iog4ga0k4qynfJvv/1ma2urra3NZDInT5588eJFhNBnn32GPwSytLTEX2tfvXo1i8XS1tbOyspCfWyp8tVXX7FYLDabXVtbu3XrVhMTE+nvQAIAelL8vOzhRcq582TtdyKlt99+e+rUqW+W5+Xlubu7YxiGLwoZEhJCVPW7k8r8+fMRQq9fv1b8KVtaWmpra0s439TU1PDw8IaGhlevXs2ePZt4ScLLy0tNTe358+dEy+XLl2dlZeH/3deWKvipbdq06ciRI0uWLLl//76EQyvgvRxloJrvlMiXao4hXOUMIQXsdzIYQqEwMDAwISGh11rZdlJRklP+8MMPd+/ezeVydXV13d3dX716hedUf39/kUhEHJfP5//++++urq5Iii1VDhw4sHHjxrS0tAkTJgxR2ACMeDBjTRGUc7+TnTt3rl271sTEZCg6V55Txt+ZwN+dfO+992xsbL777rudO3dSKJQzZ874+PjgC3nJvKXKm86ePUuhUOR3BspLRU4TyBGkHKWg+P1O8vLyiouLY2JiFHlQcUN6yufPn4+Oji4pKeHz+eJpj0KhrFu3bsuWLZcvX37//ff/9a9//fDDD3gVsaWK+MtJxsbGMhx99uzZmzdvHtwZKLv8/PzY2Fj81hCQDT6GZEehaJByyEfKficnT568fPkylfo/d1b37t27d+/e33//XXzvxaEwFKd87dq1P/74Y/PmzU+fPvX09FyyZMl33303evToI0eOfPnll0SzVatW7dy588SJE2PHjuVwOGZmZng5saVKYGDgICMZM2bM0qVLB9mJ8ouNjVWF0xxSKphy4FkO+UjZ7yQxMVH8mZ749IGhzjdoaE75jz/+0NTURAgVFxd3dnauX7/ewsKCyWT2uPnD5XKXLVuWkZFx6NChzz//nCiXbUsVAMCAQMohh7z2Oxm6COW+k8rQnXJnZ+fLly9zc3PxlGNqaooQunTpUltbW2lpKTEbm+Dv79/e3n7u3DnxV3clbKkCAJAbMqbJDSfSTGQkcb8TyYHl5+c7OjoSDySMjIx4PN7Vq1ffbPnmJGkJO6ncvHlz0qRJ+E05IyOjvXv3KuyUv/nmG3x7lV6lp6fjHQYFBenq6uro6Hh7e+MvS1laWhJzsjEMmzZtWnBwcI/z6nVLlYMHD+J7DI8dO1aaJfdhkjSQkmqOocqd8EANxT8LZdjvRMGU7ZRdXV2fPHkyFD1DygFSUs0xhBtr5BhG+53IC+mnTNyUKyoqwq+oyI0HABUEKWe4evDgAaVvMm85NYIFBQWVlpY+evRo9erVkZGRZIejEtatW0f8m+yx08SlS5eCg4PFd6NYuXKleAMXFxc2m62mpjZp0qTbt28rNvD/+vHHH/HFMszMzFavXk2sX56VlXXw4EHxL1IZGRnEyerp6ZEUr9Ij+zJL2cn94jc4OBh/TXLcuHGpqaly7FlpKckph4SEUKnUsWPHEivcDAW4sSYOv6Gak5Pz8OHDtrY2ojwsLGzRokV8Ph//aGlpOWrUKITQuXPnxH88JyfHw8NDvpEPyJkzZxBCBw8ebGxsvHPnjoWFhb29fWdnJ14bGxvr5ORErPnU3d1dVVV17do1V1dX2Ii6Lyp3wgOlmv8sgMwUkHJaW1sdHBzI7Ur6lGNiYtKjcP/+/TY2NkKhkCixtLT84YcfqFSqiYlJY2MjUU56ypk3b97o0aO7u7vxj/hUlLy8PKJBQECAg4MDkYRwmzZtgpTTF7ixBsAwI8fdHBS/McTjx49DQ0P37NnDZDLFy3k8XmBg4PPnz7dt26bIeCR79uyZsbEx8WrX2LFjEULik/jDw8MLCwtV8I1OmUHKAYAEGIbFxMTg65xyudzFixcT67kNaDcH+W4MMaAdK2QTFxeHYZi7u/ubVVFRUTY2NidOnLh06VKvPyth0CRvn4H62JmiXxYWFuIpGX+QY2FhQZRwuVwnJ6fY2FhMWfcFVjrkXmQpP9W8+AUyk/LGWlhYmLq6+qlTpxobG4uKiqZPn66np0e8aDWg3RzkuDFEvztWEGS+sWZhYWFra9ujmaWlZXl5OYZhN27coFKp48aNa25uxt64sSZ50CRvn9HXzhSS5ebm0un0uLg4Pp9/9+7diRMnzp8/v0eb4OBghNCdO3eIErixJgFc5QCgaEKhMCYmZsmSJStWrNDW1p48efLRo0fr6+uPHTsmW4fy2hhCth0rpNfS0lJeXi7hZV4HB4fNmzdXVFTs2LGjR5WUg9br9hn97kzRFycnp6CgoICAAA6HY2dnJxAITpw40aONtbU1Qqi4uFjKQVBxkHIAULSSkpLm5mbxtexmzpyprq7+5to8MiBrLwxp1NbWYhiGL1TRl6ioqPHjx8fHx+fl5YmXD3TQxLfPkHlnipCQkGPHjl2+fLm5ufnJkyc8Hs/BwQHfj5GAn87Lly/77Q0gSDkAKF5jYyNCSEtLS7xQR0dHIBDIpX/F74Uhpba2NoQQg8GQ0IbJZCYmJlIolDVr1giFQqJ8MING7ExBvDdTWVnZ2toq+adevHhx8ODBtWvXvvfee5qamubm5sePH6+ursZvThLw9ZDwUwP9gpQDgKLp6OgghHr8rZTXbg6k7IUhJfyvc7/rUDg4OGzZsqW0tFT8jd3BDBqxM4X4Q4X8/HzJP1VaWioSiUaPHk2UcDgcXV3dkpIS8WYdHR3EqYF+QcoBQNHs7Oy0tLRu3bpFlBQUFHR0dLz11lv4x8Hs5kDKXhhSMjAwoFAoTU1N/baMjIycMGHCnTt3iJJ+B00C2XamwJOZ+GriAoGgoaEBnypNwE/H0NBwQJ2rLEg5ACgak8ncunVrenp6UlISn88vLi729/c3Njb28/PDGwx0Nwd5bQwh9x0remCxWBYWFlVVVf22xG+v4XuEEyWSB01yb33tTOHj42NoaNjrgjrm5ubz5s07fvz4tWvXhELhs2fP8GN9+umn4s3w05k8eXK/YQCEVG+K3kCp5kRGIDMpJ0l3d3dHR0dbW1vT6XQul+vp6fnw4UOidkAbWMhxLwwJO1b0IPMk6YCAADqd3train9MT0/HJ7Dp6elt3Lixx49v375dfJK0hEHrd/uMXnemwDDM09MTIRQWFtZr/PX19YGBgVZWVgwGQ0tLy9HR8aeffurRxs3NzcTEhFihAINJ0hKp3AkPlGr+swAyU/waa6RsDCFzyiktLaXRaNLsPKQYIpFo7ty5J0+elO3H6+vrmUzmoUOHxAsh5UgAN9YAGPZI3xhCAqFQePHixdLSUvwxu5WVVURERERERHNzM9mhIZFIlJGRIRAIZF55PTw83N7ePiAgACGEYVh1dXVeXt7jx4/lGuaIAikHADCEGhoaFixYYGNjs2bNGrwkODjY29vbx8dHmnkEQyo3NzctLS0nJ0fyq0J9iYmJKSwszM7OptPpCKHMzEwTE5O5c+eeP39e3pGOHJByABjGdu7cmZiY2NTUZG5ufvbsWbLD6eno0aPEHZWkpCSifO/evQEBAfv37ycxNoSQs7PzDz/8QKxBNyCZmZnt7e25ublcLhcvWbx4MXGy+Cp24E00sgMAAMhu3759+/btIzsKWbi4uLi4uJAdhew8PDw8PDzIjmL4gascAAAACgIpBwAAgIJAygEAAKAgkHIAAAAoCEwfkEpKSgrZIYDhAV/+ZMT/g8HXxBzxpzmk+l1XdESiYLB/qkQpKSnLli0jOwoAwMikan+BIeUA1dLY2Pjee+8JBIKrV6+Kr0s/MrS3t/v6+ubk5KSlpS1YsIDscADoCZ7lABXC5/Pnz59fV1f366+/jrx8gxBiMBjJyckfffSRh4cHvoQXAEoFnuUAVSEUCj08PCoqKq5evTpu3Diywxkqampqx48f53K5H330UU1NzaZNm8iOCID/gpQDVEJHR4e3t3dhYeGVK1cmTJhAdjhDi0KhREdH6+npBQYGvnjx4sCBA2RHBMBfIOWAkU8kEq1cufK33367dOnStGnTyA5HQYKCgjgczsaNG6lU6jBdFAeMPJBywAiHYdjatWvPnTuXnZ09c+ZMssNRKH9/f01NzTVr1mAYRvoamgAgSDlgZMMwbP369UlJSRkZGU5OTmSHQ4KPP/6YwWCsWLGiq6srOjqa7HCAqoOUA0ayHTt2HD9+/PTp0wsXLiQ7FtLgL5atWLECw7BDhw6RHQ5QaZBywIi1e/fuQ4cOnTp1ytvbm+xYSEZkHYQQZB1AIkg5YGT6xz/+ERkZ+c033yxfvpzsWJTCsmXLurq6PvnkEzqdDs91AFkg5YAR6Lvvvtu8efPBgwf9/PzIjkWJ+Pr6dnd3r1q1isvlfvnll2SHA1QRpBww0iQlJX3++ecRERHbt28nOxals3LlSoFAsHHjRi6X+/nnn5MdDlA5kHLAiJKRkbF69eqAgIBdu3aRHYuSWr9+fW1trb+/v46ODjzlAgoGKQeMHL/++quPj8/atWsPHz5MdixKLTw8vKmpacWKFRwOZ/78+WSHA1QIrCQNRojr16/Pnz/fy8srMTGRSoX1avuBYdiaNWtSUlJ+/fVXHo9HdjhAVUDKASNBQUHBBx988P7776ekpNBocO0ulc7OTg8Pj1u3bt28edPCwoLscIBKgJQDhr2ioqJ58+bNmDEjKyuLwWCQHc5w0tLS8s4777S1td24cUNbW5vscMDIBykHDG+lpaXvvPOOra3tuXPnNDQ0yA5n+Kmurn777bcnTZp07tw5uEAEQw1ueYNh7OnTpx988IGZmVlGRgbkG9mMHj06MzPzt99+27p1K9mxgJEPUg4Yrp4/fz5v3jxtbe3s7Gw2m012OMPY9OnT//Wvfx05cuSbb74hOxYwwqmFh4eTHQMAA1ZXV+fs7IwQunLlir6+PtnhDHu2trYIoeDgYB6PB1MJwNCBZzlg+GlqanJ2dq6rq/vtt99MTU3JDmeEwDDM19f3woULt2/fHsEbdQNyQcoBw0xra+v8+fPLy8t/++03c3NzssMZUYRC4dtvv62hofHbb7+pq6uTHQ4YgeBZDhhOhELh3/72t4cPH/7666+Qb+ROQ0MjLS3twYMHO3bsIDsWMDJBygHDRmdn59KlS2/fvn3hwoWJEyeSHc7IZG1tfezYsdjY2PT0dLJjASMQ3FgDw4NIJPL19T137tyFCxfmzJlDdjgj3Nq1a1NTU2/fvg2XkkC+IOWAYQDDsM8///zHH3/Mzs5+9913yQ5n5Gtra+PxeDQaLS8vDx7qADmCG2tA2WEYtnHjxlOnTqWmpkK+UQwmk3n69OkHDx4EBweTHQsYUSDlAGUXHBz87bffnjp1ys3NjexYVMj48ePj4uJiY2OvX79Odixg5IAba0CpRUREhIeHHz9+/NNPPyU7FlW0ePHi+/fvFxYWwnpCQC7gKgcor7i4uPDw8Pj4eMg3ZElISKirq9u9ezfZgYARAlIOUFLff/99YGDggQMH/P39yY5FdY0ePfqrr776+9//DrfXgFzAjTWgjM6ePevj4xMaGgrfr0mHYZirq2tFRcWdO3eYTCbZ4YDhDa5ygNLJzMz86KOPNm7cCPlGGVAolG+//fb58+eRkZFkxwKGPUg5gDStra2nT5/uUXjp0iUfH5+PP/748OHDpEQF3mRqanrgwIHo6OiHDx+SHQsY3iDlANL861//8vX1PXLkCFFy48YNT0/PDz/88Pjx4xQKhcTYQA/r1q2bMmXKtm3byA4EDG/wLAeQo7u728rKqry8HCG0b9++4ODgwsLCefPmvfvuu6mpqbAjshL697///d577128eNHFxYXsWMBwBSkHkOOnn35asmQJ/t8UCuWzzz5LS0ubNWtWZmYmrLCitDw8PB4/fvznn3/CdwIgG0g5gByzZ8++deuWSCTCP1Kp1Lfeeuvf//63pqYmuYEBCcrKyiZNmhQXF7d27VqyYwHDEjzLAST4/fffCwoKiHyDEOru7v7jjz/Wrl3b1dVFYmBAMktLS39//9DQ0KamJrJjAcMSpBxAggMHDtDp9B6F3d3dycnJH330UWdnJylRAWmEhYWJRKL9+/eTHQgYluDGGlC08vJyKyur7u7uvhosWrQoNTWVwWAoMiogvcOHD+/atau8vNzAwIDsWMAwA1c5QNFiY2PV1NR6raJSqYaGhm5ublQq/MtUXv7+/tra2jExMWQHAoYfuMoBCvX69WsTExOhUNijnEajsdnsoKCgTZs2waoqyi86OnrPnj3l5eX6+vpkxwKGE/guCRTqm2++6fGoRk1NTVtbOyoqqqqqKigoCPLNsLB+/XoWiyX+Gi8A0oCrHKA4HR0dJiYm9fX1+EcajaahobFly5YtW7ZwOBxyYwMDtXv37m+++aayshK20gHSg6scoDg//vgjnm9oNBqLxQoJCXn27Fl4eDjkm+Fow4YNAoHg1KlTZAcChhO4ygEKgmGYra3tgwcPmEzm5s2bt23bpqurS3ZQYFDWrFlTUFBw9+5dWBAPSAsTk5ycTHY4AABlkZycjEl0584dhNCVK1ckNwOA0MtCSZB4hrv8/PzY2Fhl+z0WFBTY2NhwuVw59rls2bLAwEAHBwc59glwy5Yt67eNvb3922+//e23386bN08BIYERoJeUs3TpUsXHAeQrNjZW2X6PQxHPsmXLHBwclO1MRwZpUg5CyM/Pb926dbW1tfBaKJAGTB8AAMjOx8dHU1MzMTGR7EDA8AApBwAgOw0NjRUrVhw7dkzCCkYAECDlAAAGxd/fv7y8/NKlS2QHAoYBSDkAgEGZOHGio6Pjt99+S3YgYBiAlAMAGCw/P7+srKzq6mqyAwHKDlIO+Et2dra2tvbPP/9MdiDkuHTpUnBwcFpamoWFBYVCoVAoK1euFG/g4uLCZrPV1NQmTZp0+/ZtsuL88ccfZ86cyWazzczMVq9eXVNTg5dnZWUdPHhQfNc7RfL29uZyud999x0pRwfDCKQc8BdMhdeh2L17d1xc3M6dO728vJ48eWJpaTlq1KikpKTz588TbX755ZfU1NRFixaVlJRMnz6dlDiTk5N9fX29vb2rqqoyMzOvXbu2cOFCfB9Vd3d3JpPp7Ozc2Nio+MAYDMbHH3987NgxsnIeGC4g5YC/uLm5NTU1LVq0aKgPJBQKeTzeUB9FegcOHDhz5kxKSgqbzSYK4+LiqFSqn5+fUu24/O233xWlrMwAACAASURBVI4ePXr79u3a2tr29vZbtmwpLCwsKCjAazdt2jR16lRXV1dSNvP29/evqqq6cOGC4g8NhhFIOUDRTp48WVtbS3YUf3n8+HFoaOiePXt6bJrA4/ECAwOfP3++bds2smJ707Nnz4yNjYkFzcaOHYsQqqysJBqEh4cXFhbGxsYqPjZLS8t58+bBJAIgGaQcgBBCeXl5pqamFArl66+/RgglJCRoamqyWKzMzMyFCxdyOJwxY8acPn0abxwXF8dkMg0MDNatW2dsbMxkMnk8HvFdOyAgQF1d3cjICP+4YcMGTU1NCoWCryEdGBi4devWsrIyCoViZWWFELpw4QKHw9m7dy8Jp41QXFwchmHu7u5vVkVFRdnY2Jw4caKv6b8YhsXExEycOJHBYHC53MWLFz948ACvkjyACCGRSBQWFmZqaqqhoTFlyhQpVyeysLAQz9b4gxwLCwuihMvlOjk5xcbGknKbdN26ddnZ2c+ePVP8ocGwIb7gGv7vnoyl3oA8yfZ7xP9SHDlyBP8YEhKCELp8+XJTU1Ntbe3cuXM1NTU7OjrwWj8/P01NzXv37rW1tZWUlOAPtJ8+fYrX+vr6GhoaEj1HR0cjhOrq6vCPXl5elpaWRO25c+fYbHZERIQMZ4qkWHpSMgsLC1tb2x6FlpaW5eXlGIbduHGDSqWOGzeuubkZw7CcnBwPDw+iWVhYmLq6+qlTpxobG4uKiqZPn66np1dTU4PXSh7Abdu2MRiMs2fPvn79eufOnVQq9ffff+832tzcXDqdHhcXx+fz7969O3HixPnz5/doExwcjBC6c+eOjCPyHzKMbXt7u56eXlRU1CAPDUYwuMoBkvB4PA6Ho6+v7+Pj09LS8vTpU6KKRqPhX/BtbW0TEhIEAoFsq564ubnx+fzQ0FD5RS2tlpaW8vJyS0vLvho4ODhs3ry5oqJix44dPaqEQmFMTMySJUtWrFihra09efLko0eP1tfXHzt2TLxZrwPY1taWkJDg6enp5eWlo6Oza9cuOp0uzeg5OTkFBQUFBARwOBw7OzuBQHDixIkebaytrRFCxcXFUg6CHKmrqy9fvvyf//wnpsJTUYBkkHKAVNTV1RFCPfaQJsyYMYPFYhG3lYaL2tpaDMNYLJaENlFRUePHj4+Pj8/LyxMvLykpaW5unjFjBlEyc+ZMdXV14gZjD+ID+PDhw9bWVjs7O7xKQ0PDyMhImtELCQk5duzY5cuXm5ubnzx5wuPxHBwcetzIwk/n5cuX/fY2FD755JPS0tL8/HxSjg6UH6QcIB8MBqOuro7sKAamra0NIcRgMCS0YTKZiYmJFAplzZo1QqGQKMfnImtpaYk31tHREQgE/R63paUFIbRr1y7Kf1RWVra2tkr+qRcvXhw8eHDt2rXvvfeepqamubn58ePHq6ur8fuWBHxbaPzUFG/69OlTpkz55z//ScrRgfKDlAPkoLOzs7GxccyYMWQHMjD4X+d+XyVxcHDYsmVLaWlpZGQkUaijo4MQ6pFgpBwEfX19hNDhw4fF73H3e2VQWloqEolGjx5NlHA4HF1d3ZKSEvFmHR0dxKmR4uOPPz5z5ky/GRSoJkg5QA5yc3MxDJs9ezb+kUaj9XULTqkYGBhQKBRp3ryJjIycMGECvgkmzs7OTktL69atW0RJQUFBR0fHW2+91W9vY8eOZTKZhYWFA4oWT2YvXrwgSgQCQUNDAz5VmoCfjqGh4YA6l6OVK1cKhcKMjAyyAgDKDFIOkFF3d/fr16+7urqKiooCAwNNTU1XrVqFV1lZWTU0NGRkZHR2dtbV1Ym/OIIQ0tXVra6urqioEAgEnZ2dOTk5ZE2SZrFYFhYWVVVV/bbEb6+pqamJl2zdujU9PT0pKYnP5xcXF/v7+xsbG/v5+UnT2+rVq0+fPp2QkMDn80UiUVVVFZ5LfHx8DA0Ne11Qx9zcfN68ecePH7927ZpQKHz27Bl+rE8//VS8GX46kydP7jeMIWJgYLBgwQK4twZ6J35pD5OkRwYZfo9HjhzB36RhsVju7u7x8fH4U2hra+uysrJjx45xOByEkJmZ2aNHjzAM8/Pzo9PpJiYmNBqNw+EsXry4rKyM6O3Vq1fz5s1jMpnm5uZffPHF9u3bEUJWVlb4LOrbt2+bmZlpaGjMmTOnpqYmOzubzWbLNrMWDXqSdEBAAJ1Ob21txT+mp6fjE9j09PQ2btzYo/H27dvFJ0l3d3dHR0dbW1vT6XQul+vp6fnw4UO8qt8BbG9vDwoKMjU1pdFo+vr6Xl5eJSUlGIZ5enoihMLCwnqNtr6+PjAw0MrKisFgaGlpOTo6/vTTTz3auLm5mZiYdHd3D2ZYsMGN7dmzZ6lUKjFpHgACpJwRSAG/Rz8/P11d3SE9hDQGn3JKS0tpNNqpU6fkFdIgiUSiuXPnnjx5UrYfr6+vZzKZhw4dGnwkgxlb/AWdffv2DT4MMMLAjTUgo5GxgKOVlVVERERERERzczPZsSCRSJSRkSEQCHx8fGTrITw83N7ePiAgQL6BDZS6urqPj8/333+PwQs64H8NNuV89tlnbDabQqEM9Fno0ImIiLC1teVwOAwGw8rK6ssvvxT/axIVFUX5X8TrEZKJL2uPU1dXNzAwePfdd6Ojo1+/fj1kJwSGVnBwsLe3t4+PD+kreObm5qalpeXk5Eh+VagvMTExhYWF2dnZdDpd7rEN1CeffPLo0aObN2+SHQhQLoNNOSdOnDh+/LhcQpGXK1eubNy4saKior6+ft++fbGxsd7e3oPvlljWXltbG8Ow7u7u2tralJQUc3PzoKCgSZMmiU9eGtl27tyZmJjY1NRkbm5+9uxZssORg7179wYEBOzfv5/cMJydnX/44QdieboByczMbG9vz83N5XK5cg9MBjNmzLCzs4NJBKAn8btssj0DwBcrHPyaTvLi5ubW1dVFfFy6dClCiHiSGRkZOZgb90TKEZeamkqlUg0MDBobG2XuWY5U55kcGvSzHNCXwY/tgQMHRo0aRSwrBwAml2c5xFLqSuLcuXPik1n19PQQQkP6YtqHH364atWq2trao0ePDt1RABheli5d2tDQcOXKFbIDAUpElpSDYVh0dPT48eMZDIa2tjY+BZbQ66rs/a7lfvXq1VmzZrFYLA6HM3nyZD6f31dXA/X8+XMNDQ1zc3NpGsu8kD7+SkpOTg7+UdkGAQDFMzc3nzFjRmpqKtmBAGUifskj5Q2ZkJAQCoXy97///fXr162trfHx8Ujsxlpfq7JLWMu9ubmZw+EcPHhQKBTW1NQsWbIEX+hetgXexbW0tLDZ7ICAAKIkMjJyzJgxOjo6dDp93LhxHh4e//d//0fU9ruQfq831jAMw9PD2LFjlWEQ4MYaGDy5jO1XX33F5XLb29vlEhIYAQacclpbW1ks1gcffECUiD/LEQqFLBbLx8eHaMxgMNavX4/956+tUCjEq/BE9fjxYwzD7t69ixA6d+6c+IEkdCW9kJAQGxsbPp9PlDx9+vT27dsCgaC9vT0/P3/atGkaGhp3796VssO+Ug6GYRQKRUdHR3LkihkESDlg8OQytk+fPqVQKNnZ2XIJCYwAtIFeFT1+/Li1tdXZ2bnXWulXZRdfy93CwsLAwGDFihWbNm1atWrVuHHjBtRVX9LT01NSUn755RfxPe3Hjh1LrEk1e/bsxMREe3v7+Pj4hIQE6Xt+U0tLC4Zh+BvmSjIIKSkpgzmj4QLWyVdmY8eOnTVrVmpq6sKFC8mOBSgH8fwjzbfj7OxshJD429HiVznXr19/8xCzZ8/G3viCj0+tvn//Pv7x7t27f/vb32g0GoVCWbZsWWtrq4SupHH69OmZM2c+f/5ccjORSKSmpubs7Cxlt31d5eCLYrm4uCjDIMDzHiAXcrmCPHTokI6ODtxbA7gBTx9gMpkIofb29l5rZVuVHSE0adKkn3/+ubq6OigoKDk5+dChQzJ3hRA6cuRIUlLSlStXxFd671V3d3d3d7fkHVOkceHCBYQQ/lVOSQZh6P7RKA8EN9aGzGD+dxC3dOnSpqamX3/9VV4dgmFtwCnHzs6OSqVevXq111rZVmWvrq6+d+8eQkhfX3///v3Tp0+/d++ebF1hGBYUFFRcXJyRkdFj+yzc/PnzxT/ij+IdHBwGdJQeampqDh8+PGbMmDVr1iAlGAQAlMfYsWNnz54N89YAbsApB1/19uzZsydPnuTz+UVFReKbvUtYlV2C6urqdevWPXjwoKOj486dO5WVlbNnz5atq3v37n311VfHjx+n0+nii9McOnQIb/D8+fMzZ840NjZ2dnbm5+d/9tlnpqam/v7+eK00C+ljGNbc3Iyv1FtXV5ecnOzo6KimppaRkYE/yyF9EABQKt7e3j/99BNZG5UC5SJ+KS3lTCeBQPDZZ5+NGjVKS0trzpw5YWFhCKExY8b8+eefWB+rsktey72iooLH43G5XDU1tdGjR4eEhODLB/S1wLsExcXFvZ5mdHQ03mDr1q2Wlpaampo0Gm3MmDGff/55dXU18eMSFtLPysqaMmUKi8VSV1enUqkIIXyK2qxZsyIiIl69eiXemNxBgBlrYPDkOLbPnj2jUqlZWVly6Q0MaxRM7KZtSkrKsmXLMFj8dZhTnd8jhUJJTk7G1zQC8iXfseXxeBMmTPjuu+/k0hsYvmDzAgDAkHNzc8vJyVGFr0FAsmGWch48eEDpm8y7jAAAhpSrq2tNTc2dO3fIDgSQbJilnAkTJki4S3jmzBmyAwTK69KlS8HBweL7Hq1cuVK8gYuLC5vNVlNTmzRpEv6iFVm6u7sPHz7M4/HerMrLy3N0dGSxWMbGxkFBQT1eV+irNisr6+DBgyTuqmdvb29iYnL+/HmyAgBKYpilHABks3v37ri4uJ07dxL7Ho0aNSopKUn8j+Avv/ySmpq6aNGikpKS6dOnkxVqaWnpO++8s2XLljeXPy8pKXFxcXF2dq6rq0tPT//uu++IyZaSa93d3ZlMprOzc2Njo+LORAyFQlmwYAGx7i1QWZBygCyEQmGv38HJ7aovBw4cOHPmTEpKivjSR3FxcVQq1c/Pj/TNQMX9+eefO3bs8Pf3t7e3f7M2MjLSyMhoz549mpqaDg4OQUFB33//PbEAkuTaTZs2TZ061dXVtaurS3HnI8bV1bWgoKC2tpaUowMlASkHyOLkyZPy+tshx6569fjx49DQ0D179uALZxB4PF5gYODz58+3bds2dEcfqKlTp6alpfn6+r65IkZXV9f58+ednJyIHaoWLlyIYVhmZma/tbjw8PDCwsLY2FiFnEpPLi4udDr9l19+IeXoQElAylFdGIbFxMRMnDiRwWBwudzFixcT34gDAgLU1dWJHZE3bNigqalJoVDq6+sRQoGBgVu3bi0rK6NQKFZWVnFxcUwm08DAYN26dcbGxkwmk8fjFRQUyNAVGsSWRX2Ji4vDMMzd3f3NqqioKBsbmxMnTly6dGmgQ9Tv7kdy3+joyZMnzc3NpqamRImlpSVCqKioqN9aHJfLdXJyio2NJWXmmJaWlqOjI9xbU3Xij99V5xXCkU3K32NYWJi6uvqpU6caGxuLioqmT5+up6dXU1OD1/r6+hoaGhKNo6OjEUL4Fj4Yhnl5eVlaWhK1fn5+mpqa9+7da2trKykpmTlzJpvNJjb/HlBX/W5ZJA5J8bqihYWFra1tj0JLS8vy8nIMw27cuEGlUseNG9fc3IxhWE5OjoeHB9FM8hBJ2P0IG/RuT2+//fbUqVPFS/BVpoiXmnEaGhr4orSSawnBwcFIum3jpRnbgTp06JCurm5nZ6d8uwXDCFzlqCihUBgTE7NkyZIVK1Zoa2tPnjz56NGj9fX14ssXDQiNRsOvBmxtbRMSEgQCQWJiogz9uLm58fn80NBQ2cLooaWlpby8HP++3ysHB4fNmzdXVFTs2LGjR5WUQ8Tj8Tgcjr6+vo+PT0tLy9OnTxFCbW1tCQkJnp6eXl5eOjo6u3btotPpsg0IAZ9+Jr7JOkKITqcLhcJ+awnW1tYIob4W6Rhqrq6uDQ0NxBUwUEGQclRUSUlJc3PzjBkziJKZM2eqq6vL5c/BjBkzWCzWgDY3GiK1tbUYhuHrDPUlKipq/Pjx8fHxeXl54uUDHSLx3Y8Gv9vTm/BnUT0e/nd0dGhoaPRbS8CH4uXLl4OJRGYTJ060sLDAN0ABqglSjorCJ8v2WGxbR0dHIBDIpX8Gg1FXVyeXrgYDX0pS8uYUTCYzMTGRQqGsWbNG/JpgMEPU0tKCENq1axfxnnJlZeWbk54HBH8ehm95jmttbW1razM2Nu63loBnIBJX2HRxcbly5QpZRwekg5SjonR0dBBCPf56NjY2jhkzZvCdd3Z2yqurQcL/wvb7CqSDg8OWLVtKS0sjIyOJwsEM0WA2OuqLubk5m82urKwkSh4/fowQmjJlSr+1hI6ODvSfYSHFnDlz/vjjj0FmXzB8QcpRUXZ2dlpaWrdu3SJKCgoKOjo63nrrLfwjjUbD7xHJIDc3F8Ow2bNnD76rQTIwMKBQKNK8eRMZGTlhwgTxFVn6HSIJhmKjIxqN5urqeu3ate7ubrwkJyeHQqHgk/Ek1xLwoTA0NJRjYAMyZ86czs7O//u//yMrAEAuSDkqislkbt26NT09PSkpic/nFxcX+/v7Gxsb+/n54Q2srKwaGhoyMjI6Ozvr6urEvz4jhHR1daurqysqKgQCAZ5Ouru7X79+3dXVVVRUFBgYaGpqumrVKhm6kmbLIumxWCwLC4uqqippBiQxMVH88Xu/QyS5t742OvLx8TE0NJRtQZ3Q0NCXL1/u3r27paUlPz8/Ojp61apV48ePl6YWhw/F5MmTZTi6XJiZmZmamvZ4bAZUiPiFP0ySHhmk/D12d3dHR0dbW1vT6XQul+vp6fnw4UOi9tWrV/PmzWMymebm5l988cX27dsRQlZWVvjU59u3b5uZmWloaMyZM6empsbPz49Op5uYmNBoNA6Hs3jx4rKyMtm6krBl0ZuQFBN5AwIC6HR6a2sr/jE9PR2fwKanp7dx48Yejbdv3y4+SVrCEEne/Qjre6MjT09PhFBYWFiv0ebn5zs6OhIPYIyMjHg83tWrV4kGV69enTVrFoPBMDY23r59e1tbm/iPS67FMMzNzc3ExATfYFAyacZWNsuXL58/f/5Q9AyUH6ScEUjxv0c/Pz9dXV1FHhEnzZ/F0tJSGo126tQpxYTUL5FINHfu3JMnTyr+0PX19Uwm89ChQ9I0HrqUk5CQwOFw8B0IgaqBG2tAPkhcpVgyKyuriIiIiIiI5uZmsmNBIpEoIyNDIBCQstFGeHi4vb19QECA4g8tbs6cOfge9uSGAUgBKQeMfMHBwd7e3j4+PqSv4Jmbm5uWlpaTkyP5VaGhEBMTU1hYmJ2dTafTFXzoHuzs7HR1dX/77TdywwCkgJQDBmvnzp2JiYlNTU3m5uZnz54lO5ze7d27NyAgYP/+/eSG4ezs/MMPPxArzilMZmZme3t7bm4ul8tV8KHfRKFQeDze9evXyQ4EkIBGdgBg2Nu3b9++ffvIjqJ/Li4uLi4uZEdBDg8PDw8PD7Kj+C9HR8d//OMfZEcBSABXOQAARcNnJ1ZUVJAdCFA0SDkAAEWbNm0alUoVf/EWqAhIOQAARdPU1LSyspLv6gxgWICUAwAgwbRp0+AqRwX1Mn3A29tb8XEAOcIXNVGR3+Phw4dTU1PJjgIMmL29fXx8PNlRAEWjYGJb0ubn58fExJAYDVAG+HfPadOmkR0IINmWLVscHByGqPMLFy4sXLjw1atXurq6Q3QIoIT+J+UAgBBaunQpQiglJYXsQMBIVllZOW7cuOvXr/N4PLJjAYoDz3IAACQwNTXV0tK6f/8+2YEAhYKUAwAgAYVCsbGxUYbdyoEiQcoBAJBjwoQJcJWjaiDlAADIMWHChEePHpEdBVAoSDkAAHKYmZk9ffqU2DkbqAJIOQAAcowbN669vb2mpobsQIDiQMoBAJDDzMwMIVRZWUl2IEBxIOUAAMgxZswYOp0O60mrFEg5AAByqKmpGRsbP3v2jOxAgOJAygEAkMbIyOjly5dkRwEUB1IOAIA0hoaGkHJUCqQcAABpIOWoGkg5AADSwI01VQMpBwBAmlGjRjU0NJAdBVAcSDkAANKw2WyBQEB2FEBxIOUAAEjD4XAEAgGseaM6IOUAAEjDZrMxDIMLHdUBKQcAQBptbW2EEJ/PJzsQoCCQcgAApGEymQih9vZ2sgMBCgIpBwBAGjqdjhDq7OwkOxCgIJByAACkodFoCFKOKoGUAwAgDVzlqBpIOQAA0kDKUTWQcgAApBGJROg/t9eAKoCUAwAgDX59g1/rAFUAKQcAQBo85cBVjuqAlAMAIA1c5agaSDkAANJ0dHQgSDmqBFIOAIA0+OpqHA6H7ECAgkDKAQCQBl9djc1mkx0IUBBIOQAA0vD5fA0NDXV1dbIDAQoCKQcAQBo+nw931VQKzE0EqLW1VXwpX/yJ7uvXr4kSBoPBYrFIiAyMdE1NTTo6OmRHARQHUg5A33///YYNG3oU6urqEv8dHx+/fv16xQYFVEJNTY2hoSHZUQDFgRtrAHl7e6upqfVVq6am5u3trch4gOqAlKNqIOUApK+v7+zs3GvWUVNTe//99/X19RUfFVAFL1++hJSjUiDlAIQQWrFiBYZhb5ZjGLZixQrFxwNUBKQcVQMpByCE0OLFi3t9A5xGo7m7uys+HqAiqqurjY2NyY4CKA6kHIAQQmw2e9GiRT2yDo1G8/DwgDmsYIjU1dW1tLSMGzeO7ECA4kDKAX/x9fXt6uoSLxGJRL6+vmTFA0a8yspKhJCZmRnZgQDFgZQD/uLq6qqlpSVeoqmpuWDBArLiASNeRUUFlUodO3Ys2YEAxYGUA/6irq7u7e1NLD1Cp9OXLVvGYDDIjQqMYE+fPjU2NoZ/YyoFUg74r+XLl+NLDyCEOjs7ly9fTm48YGQrLS21tLQkOwqgUJBywH/NmzePeAVHT0/PycmJ3HjAyHb//v2JEyeSHQVQKEg54L+oVOry5cvV1dXpdLqvr6+EJQkAGDxIOSoIUg74Hx999FFHRwfcVQNDraGhoba2dsKECWQHAhTqf5b1rKqqunHjBlmhAGWAYdioUaMQQuXl5RUVFWSHA8jE4/HGjBkzRJ3fv38fIQQpR+VgYpKTk8kOBwCgLJKTk7Ehk5CQwOFwuru7h+4QQAn1snkB1ttaW2AYSUlJWbZsmcy/x3v37iGEbG1t5RrUkKBQKMnJyUuXLiU7kBGIQqEMaf937tyZNm3aUB8FKBvYLwf0NCySDRjuCgsLHR0dyY4CKBpMHwAAKJpIJCopKbG3tyc7EKBokHIAAIr24MGD1tZWSDkqCFIOAEDR8vPzNTU14RauCoKUAwBQtLy8vNmzZ/e6RRMY2SDlAAAULS8vb86cOWRHAUgAKQf8JTs7W1tb++effyY7kKFy6dKl4ODgtLQ0CwsLCoVCoVBWrlwp3sDFxYXNZqupqU2aNOn27dtkxYkQ6u7uPnz4MI/He7MqLy/P0dGRxWIZGxsHBQW1t7dLU5uVlXXw4EGRSKSI6PtTU1NTVlYG09VUE6Qc8JeR/T7W7t274+Lidu7c6eXl9eTJE0tLy1GjRiUlJZ0/f55o88svv6Smpi5atKikpGT69OlkhVpaWvrOO+9s2bKltbW1R1VJSYmLi4uzs3NdXV16evp3333n7+8vTa27uzuTyXR2dm5sbFTcmfTh2rVrNBpt9uzZZAcCyCD+Xii++gApr6QCOVLy32Nra6uDg4NcukLSvSG/f/9+GxsboVBIlFhaWv7www9UKtXExKSxsZEoz8nJ8fDwkEtssiksLFyyZElSUpK9vf3UqVN71C5btszc3Jx4Yz86OppCody/f1+aWgzDAgICHBwcOjs7pYlEyrGVwRdffDFjxoyh6BkoP7jKAYp28uTJ2tpahR3u8ePHoaGhe/bsYTKZ4uU8Hi8wMPD58+fbtm1TWDD9mjp1alpamq+v75sbl3V1dZ0/f97JyYl4Y3/hwoUYhmVmZvZbiwsPDy8sLIyNjVXIqfQJHuSoMkg5ACGE8vLyTE1NKRTK119/jRBKSEjQ1NRksViZmZkLFy7kcDhjxow5ffo03jguLo7JZBoYGKxbt87Y2JjJZPJ4vIKCArw2ICBAXV3dyMgI/7hhwwZNTU0KhVJfX48QCgwM3Lp1a1lZGYVCsbKyQghduHCBw+Hs3bt3iE4tLi4OwzB3d/c3q6KiomxsbE6cOHHp0qVefxbDsJiYmIkTJzIYDC6Xu3jx4gcPHuBVkocIISQSicLCwkxNTTU0NKZMmTL4BQyfPHnS3NxsampKlOD7mxUVFfVbi+NyuU5OTrGxsRh5N1H5fH5RURGkHJUFKQcghNCcOXPEFxFfv3795s2bhUIhm81OTk4uKyuzsLD4/PPPOzs7EUIBAQGrVq1qbW3dtGlTRUXF7du3u7q6Pvjgg2fPniGE4uLixBc9i4+P37NnD/ExNjZ20aJFlpaWGIY9fvwYIYQ/0+7u7h6iUzt//vz48eNZLNabVRoaGt9//z2VSv38889bWlrebBAeHh4cHBwSElJbW3vt2rVnz57NnTv35cuXqL8hQgjt2LHjq6++Onz48IsXLxYtWrR8+fJbt24N5kRqamoQQmw2myhhMpkaGhp4PJJrCdOmTXv+/Pmff/45mEgGIz8/XyQS9TozAqgCSDlAEh6Px+Fw9PX1fXx8Wlpanj59SlTRaDT867+trW1CQoJAIEhMTJThEG5ubnw+PzQ0VH5R/1dLS0t5ebmE3Y4dHBw2b95cUVGxY8eOHlVCoTAmJmbJkiUrVqzQ1taePHny0aNH6+vrjx07Jt6s1yFqa2tLSEjw9PT08vLS0dHZtWsXnU6XbXwI+PSzHvvm0el0oVDYby3BcwSWrwAAIABJREFU2toaIVRcXDyYSAYjLy/P2tra2NiYrAAAuSDlAKmoq6sjhIiv8D3MmDGDxWIRN52UR21tLYZhvV7iEKKiosaPHx8fH5+XlydeXlJS0tzcPGPGDKJk5syZ6urqxC3EHsSH6OHDh62trXZ2dniVhoaGkZHRIMcHfxbV1dUlXtjR0aGhodFvLQEfih6XPop05cqVd955h6yjA9JBygHywWAw6urqyI6ip7a2NoTQm4/ixTGZzMTERAqFsmbNGvFrAnw+sZaWlnhjHR0dgUDQ73Hx23S7du2i/EdlZeWbk54HBH88xufziZLW1ta2tjb8ikFyLQHPQPiwKF5DQ0NBQcHChQtJOTpQBpBygBx0dnY2NjYO3Q6SMsP/wvb7CqSDg8OWLVtKS0sjIyOJQh0dHYRQjwQj5Wnq6+sjhA4fPiw+PTQ/P1+GUyCYm5uz2ezKykqiBH8YNmXKlH5rCR0dHeg/w6J4OTk5VCr1/fffJ+XoQBlAygFykJubi2EY8XIfjUbr6xacghkYGFAolKampn5bRkZGTpgw4c6dO0SJnZ2dlpaW+DP/goKCjo6Ot956q9/exo4dy2QyCwsLZQu7VzQazdXV9dq1a8RUi5ycHAqFgk/Gk1xLwIfC0NBQjoFJLycnZ+7cudra2qQcHSgDSDlARt3d3a9fv+7q6ioqKgoMDDQ1NV21ahVeZWVl1dDQkJGR0dnZWVdXJ/7VGyGkq6tbXV1dUVEhEAg6OztzcnKGbpI0i8WysLCoqqrqtyV+e0388TuTydy6dWt6enpSUhKfzy8uLvb39zc2Nvbz85Omt9WrV58+fTohIYHP54tEoqqqqhcvXiCEfHx8DA0NZVtQJzQ09OXLl7t3725pacnPz4+Ojl61atX48eOlqcXhQzF58mQZjj5IIpHo4sWLcFdN1Ylf+Cv5W+tASjL8Ho8cOYI/DGCxWO7u7vHx8fhzZmtr67KysmPHjnE4HISQmZnZo0ePMAzz8/Oj0+kmJiY0Go3D4SxevLisrIzo7dWrV/PmzWMymebm5l988cX27dsRQlZWVk+fPsUw7Pbt22ZmZhoaGnPmzKmpqcnOzmaz2VFRUTKcKZLiDfmAgAA6nd7a2op/TE9Pxyew6enpbdy4sUfj7du3i68+0N3dHR0dbW1tTafTuVyup6fnw4cP8ap+h6i9vT0oKMjU1JRGo+nr63t5eZWUlGAY5unpiRAKCwvrNdr8/HxHR0fiAYyRkRGPx7t69SrR4OrVq7NmzWIwGMbGxtu3b29raxP/ccm1GIa5ubmZmJgQKxRIIM3YDsj169cRQvfu3ZNjn2DYgZQzAing9+jn56erqzukh5CGNH8WS0tLaTTaqVOnFBNSv0Qi0dy5c0+ePKn4Q9fX1zOZzEOHDknTWO4pJyQkxNzcXI4dguEIbqwBGSnJssT9srKyioiIiIiIaG5uJjsWJBKJMjIyBAKBj4+P4o8eHh5ub28fEBCg+EMjhLKzs11dXUk5NFAeg005n332GZvNplAo8n1SOhgRERG2trYcDofBYFhZWX355Zc9/tZ0dnbu27fPyspKXV1dR0fHzs6uoqKi327FF73HqaurGxgYvPvuu9HR0a9fvx6q8wGDFhwc7O3t7ePjI808giGVm5ublpaWk5Mj+VWhoRATE1NYWJidnU3KxmgvXrwoLCyElAPkcGMNX1fqzp078rv2GhQnJ6f4+PhXr17x+fzk5GQ6nb5gwQLxBp6enuPHj79582ZnZ2d1dbW7u3txcbGUnVtaWmpra2MYhj88//e//71q1SoKhWJsbPz777/L/2RkMtQ31oKDg/HXHseNG5eamjp0B+oXGsjNn4sXLwYFBQ1pPEorIyNj3759XV1d0v/IgMa2XydOnNDQ0GhpaZFXh2CYGoEpx83NTfx/LXy9L/zBNYZhp0+fplAoRUVFsnVOpBxxqampVCrVwMBAfBl8EqnOMzn5/lkE4uQ7tkuWLHF1dZVXb2D4ksOzHGKxdCVx7tw58amuenp6CCHixe9vvvlm+vTp8p0k+uGHH65ataq2tvbo0aNy7BaAkaGlpeXixYuLFi0iOxBAPllSDoZh0dHR48ePZzAY2tra+BRYQq9rtve70js+uZPFYnE4nMmTJ+Prdshl+ffnz59raGiYm5sjhDo6Om7evGlvb99XY5kX0sdfScnJycE/KtsgAECic+fOtbW1LVmyhOxAgBIQv+SR8oZMSEgIhUL5+9///vr169bW1vj4eCR2Y23btm0MBuPs2bOvX7/euXMnlUrFH3KEhIQghC5fvtzU1FRbWzt37lxNTc2Ojg4Mw5qbmzkczsGDB4VCYU1NzZIlS+rq6iR0Jb2WlhY2mx0QEIB/LC8vRwjZ29u/++67RkZGDAZjwoQJX3/9NfGawrlz59hsdkRERF8d9npjDcMwPD2MHTtWGQYBbqyBwZPj2Hp5eb3//vty6QoMdwNOOa2trSwW64MPPiBKxJ/lCIVCFovl4+NDNGYwGOvXr8f+89eW2AwYT1SPHz/GMOzu3bsIoXPnzokfSEJX0gsJCbGxseHz+fhHfM32Dz744Pr1669evWpsbMSXrE9KSpKyw75SDoZhFApFR0dHGQYBUg4YPHmNbXNzs6am5rfffjv4rsAIQBvoVdHjx49bW1udnZ17rZV+zXbxld4tLCwMDAxWrFixadOmVatWjRs3bkBd9SU9PT0lJeWXX34h9q3CVxSeNGkSsUPUnj17vvnmm2PHjvn6+krf85vwqTj4++dKMgje3t6DOaPh4vDhw6mpqWRHAfqUlZXV3t6+ePFisgMBSmHAz3LwNZrwhXLfJNua7RoaGleuXJkzZ87evXstLCx8fHyEQuEgl38/c+bMgQMHcnNz8b/dOHwdEXxHZJy6urqZmVlZWZmU3fbl0aNHCKEJEyYgZRoEAEiXmpr63nvvGRgYkB0IUAoDvsrBd4LCtyB8E7Fme2Bg4IC6nTRp0s8//1xXVxcTE3PgwIFJkybhr2fL0BVC6MiRIxcvXrxy5UqPzU60tLSsra3v3bsnXtjV1TX4pW0vXLiAEMKXLFSSQVCF7/4UCmXz5s3i+14DeZHLTFSBQHDhwoW4uLjBdwVGhgFf5djZ2VGp1KtXr/ZaK9ua7dXV1Xga0NfX379///Tp0+/duydbVxiGBQUFFRcXZ2Rk9Mg3uGXLlt25c+fJkyf4x9bW1srKykHOma6pqTl8+PCYMWPWrFmDlGAQAFASWVlZXV1d+EqmACAZUg6+Ju7Zs2dPnjzJ5/OLiorEt4KXsGa7BNXV1evWrXvw4EFHR8edO3cqKytnz54tW1f37t376quvjh8/TqfTxRenOXToEN5gy5YtZmZmq1atevr06atXr4KCgoRCIbHvvTQL6WMY1tzcjE9yq6urS05OdnR0VFNTy8jIwJ/lkD4IACiJ1NRUZ2fnUaNGkR0IUBricwmknOkkEAg+++yzUaNGaWlpzZkzJywsDCE0ZsyYP//8E+tjzXbJK71XVFTweDwul6umpjZ69OiQkBB8+YC+ln+XAJ+T9qbo6GiizbNnzz766CMul8tgMGbNmpWTk0NUSVhIPysra8qUKSwWS11dnUqlIoTwKWqzZs2KiIh49eqVeGNyBwFmrIHBG/zY8vl8JpNJyprZQGlRMAwj/i6npKQsW7ZMvAQMR6rze6RQKMnJyfAsZygMfmxPnTr16aef1tTU6OrqyjEwMKzB5gUAgCGRnJz8wQcfQL4B4oZZynnw4AGlb6TsQQKGi0uXLgUHB4tvQrFy5UrxBi4uLmw2W01NbdKkSbJtFC0v3d3dhw8fJt4eE5eXl+fo6MhisYyNjYOCgnrMHe2rNisr6+DBg4rc4ujly5cXL178+OOPFXZEMDyI32VTnWcAI5vq/B6R1M8bwsLCFi1aRKxDYWlpiT/T7rHcQ05OjvhG1KR49OiRo6MjQmjq1Kk9qu7evauhoREaGtrc3Hzjxg09Pb3Vq1dLWRsbG+vk5PT69Wspw5B+bHv11VdfaWtrE/t/A4AbZlc5QEkIhcJev4OT21VfDhw4cObMmZSUFGIdCoRQXFwclUr18/Mjfd82cX/++eeOHTv8/f17XXw2MjLSyMhoz549mpqaDg4OQUFB33//PbEaheTaTZs2TZ061dXVtaurSwEncurUqY8++khDQ0MBxwLDCKQcIIuTJ0/W1tYqW1e9evz4cWho6J49e/C3mAk8Hi8wMPD58+fbtm0buqMP1NSpU9PS0nx9ffHFmcR1dXWdP3/eycmJeElz4cKFGIZlZmb2W4sLDw8vLCyMjY0d6rO4detWcXHxJ598MtQHAsMOpBzVhWFYTEzMxIkTGQwGl8tdvHgx8Y04ICBAXV3dyMgI/7hhwwZNTU0KhYKvFRQYGLh169aysjIKhWJlZRUXF8dkMg0MDNatW2dsbMxkMnk8XkFBgQxdoUHsH9GXuLg4DMPc3d3frIqKirKxsTlx4sSlS5cGOkT9bkUh910nnjx50tzcbGpqSpRYWloihIqKivqtxXG5XCcnp9jYWGyIZzP+85//tLGxefvtt4f0KGBYEr/LpjrPAEY2KX+PYWFh6urqp06damxsLCoqmj59up6eXk1NDV7r6+traGhINI6OjkYI4fspYBjm5eVlaWlJ1Pr5+Wlqat67d6+tra2kpGTmzJlsNpvYiXVAXfW7f4Q4JMXzBgsLC1tb2x6FlpaW5eXlGIbduHGDSqWOGzeuubkZe+NZjuQhkrAVBTborTfefvvtHs9y8CU/xN8wwzBMQ0PD2dm531pCcHAwkm4PX2nGtlft7e16enr79++X4WfBiAdXOSpKKBTGxMQsWbJkxYoV2trakydPPnr0aH19vfhaEgNCo9HwqwFbW9uEhASBQJCYmChDP25ubnw+PzQ0VLYwemhpaSkvL8e/7/fKwcFh8+bNFRUVxAoUBCmHiMfjcTgcfX19Hx+flpaWp0+fIoTa2toSEhI8PT29vLx0dHR27dpFp9NlGxACPv1MfMdbhBCdThcKhf3WEqytrRFCfb0xLRdZWVkNDQ2DXJodjFSQclRUSUlJc3PzjBkziJKZM2eqq6sTN8QGY8b/t3fnAU1c7cLATyAhIZCwiCCiKJsoglJXCFJUWqxSRUQKt1pFfd+iVSOiFBFBBNfiRS4qrwuW1h1ZClZFrfaCtaJv+ykiWBVQEEQEBCQsMZDM98fczs0NEGK2IfD8/jJzJmeeOcE8me08U6YwmcwPqjShInV1dRiG4ZM+9Gbnzp329vaHDx++ffu2+PIPHSLxUhSKl97oDr8WJXHxXyAQ4JfopbcS8KF48+aNIpFI9+OPP3766acjR45U3SaA5oKUM0g1NzcjhCRmPjU0NOTxeErpn06n19fXK6UrRfD5fPR3naTeMBiM1NRUCoWycuVK8WMCRYZIFVUn8OtheP1ZXHt7O5/Px0tySG8l4BkIHxZVqKuru3btGtw4AHoDKWeQMjQ0RAhJfHs2NzePGDFC8c47OzuV1ZWC8G/YPh+BdHV1DQ0NLS0tjYuLIxYqMkREAQvxs9gFBQVy7ALBysqKxWJVVlYSS8rKyhBCEyZM6LOVIBAI0N/DogqnTp1iMpk+Pj4q6h9oOkg5g5Sjo6O+vv6ff/5JLLl3755AIJg8eTL+kkql4ueI5JCXl4dhmIuLi+JdKcjU1JRCocjy5E1cXNzYsWMfPHhALOlziKRQRdUJKpU6b968W7duiUQifElubi6FQsFvxpPeSsCHwszMTImBiTt58mRgYKD0M5lgMIOUM0gxGIxNmzZlZWWdPn26paXl0aNHa9asMTc3Dw4OxlewtbVtbGzMzs7u7Oysr68X//mMEDI2Nq6pqamoqODxeHg6EYlETU1NXV1dRUVFISEhlpaWQUFBcnQlS/0I2TGZTGtra7yUbZ8DkpqaKn75vc8hkt5bb1UnAgMDzczM5JtQJyoq6s2bN9u3b29raysoKIiPjw8KCrK3t5elFYcPhYIFonrzxx9/FBUVwVk1II34gT/cJD0wyPg5ikSi+Ph4Ozs7Go1mZGTk6+v79OlTovXt27ezZs1iMBhWVlbr168PCwtDCNna2uK3Pt+/f3/UqFG6urozZsyora0NDg6m0WgWFhZUKpXNZi9cuLC8vFy+rqTUj+gOyXAjL5fLpdFoxMwrWVlZ+A1sJiYm69atk1g5LCxM/CZpKUMkvRQF1nvVCbxeWXR0dI/RFhQUuLm5ERdghg0bxuFw8vPziRXy8/OnTZtGp9PNzc3DwsL4fL7426W3Yhjm7e1tYWGBV3uSTpaxlbBq1apx48Z90FvAYAMpZwBS/+cYHBxsbGyszi3iZPlaLC0tpVKpp06dUk9IfRIKhe7u7qRUkWloaGAwGPv375dl5Q9NOc3NzXp6egcPHpQ3OjAowIk1oBzqnKX4g9ja2sbGxsbGxra2tpIdCxIKhdnZ2Twej5RZz2NiYpydnblcrio6P336tEgkgsdxgHSQcsDAFxER4e/vHxgYSPoMnnl5eZmZmbm5ueq/wJ6QkFBYWHjlyhUajaaK/lNSUvBiu6roHAwYkHKAorZu3Zqamvru3TsrK6uMjAyyw+nZrl27uFzunj17yA3D09PzzJkzxIxzapOTk/P+/fu8vDwVpYQ7d+4UFhbKcmMFGOSoZAcANN7u3bt3795NdhR98/Ly8vLyIjsKcvj4+Kj0WZmjR49OnDhx2rRpqtsEGBjgKAcAoJDm5uaMjIw1a9aQHQjQAJByAAAK+eGHH7S0tP7jP/6D7ECABoCUAwBQSEpKypIlS/DHkgCQDq7lAADkl5+fX1JScvLkSbIDAZoBjnIAAPI7evTo1KlTJ02aRHYgQDP0cJRD1E4HGm2QfI4BAQEBAQFkRzFINTQ0/PTTT4cOHSI7EKAx/k/K4XA4ildoB6BHQqEwLCxMS0tr+/btLBaL7HBA3zgcjvQVjh07xmAwSJlJAWgoCoZhZMcABovq6uqPP/7YwMDg119/hcfUNd379++trKyWL19O+gO2QIPAtRygPiNGjPjll1/q6uq8vb37w4xnQBEnT55sbGxcv3492YEATQJHOUDdnj596uHh4ejoeOnSJQaDQXY4QB4Yho0fP57D4aSkpJAdC9AkcJQD1M3e3v7atWv3798PCAggq1ooUFBOTs6TJ09CQ0PJDgRoGDjKAeS4e/fup59+OmfOnLS0NPFanEAjcDicIUOG/Pzzz2QHAjQMHOUAcri4uOTk5Fy+fHnVqlXwu0ez5OTk3L17NzIykuxAgOaBoxxApmvXrvn4+Hz99ddJSUlkxwJkIhKJJk2aZGdnl56eTnYsQPPAhDeATHPmzDl79mxAQICBgUFcXBzZ4YC+nTx5sri4+Ny5c2QHAjQSHOUA8v34448rV67ctWvXli1byI4FSCMQCMaNG+fp6Xns2DGyYwEaCY5yAPmWL1/e2tq6bt06HR0duAmqP0tOTn79+nV0dDTZgQBNBSkH9Atr164VCASbNm1isVj//Oc/yQ4H9KC2tjYmJmbjxo0jRowgOxagqSDlgP5i48aNDQ0Na9asYbFYMG1XP7R582YDA4OtW7eSHQjQYJByQD+ya9cugUCwbNkyPT29+fPnkx0O+F+3bt06e/bsTz/9pKenR3YsQIPB7QOgf8EwbPXq1T/88ENOTs5nn31GdjgAIYQEAoGzs/OoUaNyc3PJjgVoNjjKAf0LhUL517/+xePx/Pz8rl696u7uTnZEACUkJFRUVFy+fJnsQIDGg6Mc0B8JhcLAwMBffvnl5s2bkydPJjucQe2vv/6aNGlSdHR0REQE2bEAjQcpB/RTAoFg4cKFf/zxR15e3vjx48kOZ5Dq6upyc3Pr6uq6e/cujUYjOxyg8WCONdBP6ejoZGRkODg4eHp6Pn36lOxwBqndu3c/fPjw5MmTkG+AUsBRDujXWlpaPD0937x589tvv40aNYrscAaXwsLC6dOn79mzB57PBcoCKQf0dw0NDTNnzhQIBPn5+ebm5mSHM1i8f/9+2rRpBgYGeXl5WlpwOgQoB/wlgf7OxMTk119/1dbWnjNnztu3b8kOZ7AIDQ2tqKhITU2FfAOUCP6YgAYwNTW9fv06j8f75JNPmpubyQ5n4EtPT09OTk5OTraxsSE7FjCgwIk1oDHKyso+/vhjKyur69evwzPwqlNeXj558uSlS5ceOnSI7FjAQAMpB2iS4uLimTNnfvTRRz///DODwSA7nAHo/fv3HA4HvytaV1eX7HDAQAMn1oAmcXR0vHHjxv/7f/8vMDCwq6uL7HAGoA0bNpSXl2dlZUG+AaoAKQdoGGdn58uXL9+8eXPFihUikYjscAaUo0ePHjt2LDU1FS7hABWBOdaA5nF1df3pp5/mz59Po9FOnDhBoVDIjmgg+P3337lcbkxMjK+vL9mxgAELruUATXXx4sXFixevWbPmv/7rv8iOReNVVlZOmzaNw+FkZWVBCgeqox0TE0N2DADIw97e3sHB4dtvvxWJRDNnziQ7HA3W0dHx2Wef6ejoXLp0CW7KACoFJ9aABvPz80tJSVm5cqWurm54eDjZ4WgkoVD45ZdfVlZW/vvf/2az2WSHAwY4SDlAswUFBfF4vA0bNrDZ7DVr1pAdjuYJDQ29evXqL7/8YmVlRXYsYOCDlAM03vr169+9e7d27VodHZ1Vq1aRHY4m2bt376FDh9LS0mbMmEF2LGBQgJQDBoJt27a1tbUFBwfr6+sHBASQHY5mOH/+fGRkZEJCwuLFi8mOBQwWkHLAALFnzx6BQPDVV1/p6+t7e3uLN3V2djY2NpqZmZEVWz/066+/Ll++fOPGjRs2bCA7FjCIwKOgYODYv3//8uXL/f39//u//5tYyOfzFyxYEB0dTWJg5AoPD29raxNfcvfuXR8fHz8/v++++46sqMDgBM/lgAFFKBQuXbr04sWL165dmzFjRmtr6+eff37r1i0ajVZdXT106FCyA1S369evz5kzZ9asWVeuXMFvgH706NHMmTOnTp168eJFHR0dsgMEgwsc5YABRVtb++TJk7Nnz/7888/z8/M9PT3v3LmDYRiGYcnJyWRHR4LIyEhtbe3ffvvNz8+vs7OzrKzMy8vL2dk5Ozsb8g1QPzjKAQMQn8+fO3fu8+fPX79+3dnZiS80NDSsqakZVLNVXrp0af78+fi/tbW1P/nkk8ePH1tYWPzyyy/6+vrkxgYGJzjKAQNQc3NzTU2NeL5BCPF4vFOnTpEYlZphGBYVFaWtrY2/FAqFv/zyC4PBuHTpEuQbQBZIOWCgqaysdHFxefHihXi+QQiJRKK9e/cOnsmns7OzCwsLhUIhsUQkEpWXlw/mOykA6SDlgAHlyZMn06dPr6mpkcg3CCEMw168eHHp0iVSAlMzDMO2bdtGHOIQRCLRkSNHQkNDSYkKAEg5YEDJzMxsamrqrVVbW3vfvn3qjIcs6enpf/31l/ghDkEkEh04cGDXrl3qjwoASDlgQImMjKyqqtq4cSONRqPRaBKtQqHwzp079+7dIyU2tREKhZGRkb3VINDW1tbT0+PxeAKBQM2BAQApBww0pqam+/btKysrCwoK0tbWlkg8NBpt//79ZMWmHufOnSsvL5e4aqWlpUWhUCwsLHbt2lVdXb137164SRqoH9wkDQayioqKXbt2ff/991QqlfhRr6WlVVpaam1tTW5sKiIUCseMGVNRUUGkHCqV2tXV5ejoGBYW9uWXX1KpMM0VIA0c5YCBbPTo0cePHy8uLvb399fS0sK/bbW1tQ8ePEh2aKry448/EvmGRqNRKBR3d/eLFy8+evRo2bJlkG8AueAoBwwWRUVF27Ztu3TpEoZhurq6NTU1hoaGZAelZJ2dndbW1tXV1XhqWbZs2ebNm8eNG0d2XAD8D0VTTkJCQkFBgbKiAUDVmpqaiouL37x54+TkZG9vT3Y4Svb8+fP79+/TaDRbW1sbGxuoKg1IFxoa6urqSrxU9MRaQUHB3bt3FewEALUxMjJyd3efPXt2XV1dRkYG2eEok0gkqqqq+uijjz7//PPx48cT+SYjI6O6uprc2MDglJGRUVVVJb5ECSd2XVxc0tPTFe8HAHW6cOFCQEDAQPrT7ejooNPpWlqSvyMpFMrGjRu/+OILUqICg1n3O/XhWiIAA8SgmrEUaCi4Yw0AAICaQMoBAACgJpByAAAAqAmkHAAAAGoCKQeAD3DlyhUDA4Off/6Z7ECUbPXq1ZS/LV26VLzpxo0bERERmZmZ1tbW+ApfffWV+ApeXl4sFktbW3v8+PH3799Xb+D/Bz5JNofD6d50+/ZtNzc3JpNpbm4eHh7+/v17WVovXry4b9++Hifk7pOmjNvZs2enTp3KYrFGjRq1YsWK2tpafHn3fc/Ozib+SExMTOTcHqaYxYsXL168WMFOAFC/tLQ0Of7+L126xGazL168qIqQVAQhlJaWJn2d4OBgY2Pj3Nzcp0+f8vl8Ynl0dPT8+fNbWlrwlzY2NkOGDEEI4ZM4EHJzc318fJQe+Qd59uyZm5sbQmjixIkSTcXFxbq6ulFRUa2trXfu3DExMVmxYoWMrYmJiR4eHk1NTR8UjKaM2/nz5xFC+/bta25ufvDggbW1tbOzc2dnJ94qse8ikai6uvrWrVvz5s0bMmSILP13/9uDlAMGKflSjtq0t7e7uroqpSsZU46FhYXEwj179owZM6ajo4NYYmNjc+bMGS0tLQsLi+bmZmI56V+dhYWFixYtOn36tLOzc/eUExAQYGVlJRKJ8Jfx8fEUCuWvv/6SpRXDMC6X6+rqSnwR90mDxm3WrFnDhw8n9v3QoUMIodu3bxMr9LjvGzZskDvlwIk1APqjEydO1NXVkRhAWVlZVFTUjh07JGbN4XA4ISEhr1692rx5M1mxdTdx4sTMzMwlS5bQ6XSJpq6ursuXL3t4eBCPJc6dOxfDsJycnD7jVqRIAAAgAElEQVRbcTExMYWFhYmJibJEolnjVlVVZW5uTuz7yJEjEUKVlZXECh+077KAlAOArG7fvm1paUmhUPAfg8nJyXp6ekwmMycnZ+7cuWw2e8SIEefOncNXTkpKYjAYpqamq1evNjc3ZzAYHA6HqA7H5XJ1dHSGDRuGv1y7dq2enh6FQmloaEAIhYSEbNq0qby8nEKh2NraIoSuXr3KZrPVWcozKSkJw7AFCxZ0b9q5c+eYMWNSUlJu3LjR43sxDEtISBg3bhydTjcyMlq4cOGTJ0/wJumDhhASCoXR0dGWlpa6uroTJkzAD0YV8fz589bWVktLS2KJjY0NQqioqKjPVpyRkZGHh0diYiImw4yUmjVu1tbW4r9s8As54nU9PmjfZSL3ERkOTqwBDSXfiTV8wqiDBw/iLyMjIxFCN2/efPfuXV1dnbu7u56enkAgwFuDg4P19PQeP37M5/NLSkrwi7QvX77EW5csWWJmZkb0HB8fjxCqr6/HX/r5+dnY2BCtly5dYrFYsbGxcuwpkuvEmrW1tYODg8RqNjY2L168wDDszp07Wlpao0ePbm1txbqdIIqOjtbR0Tl16lRzc3NRUdGkSZNMTExqa2vxVumDtnnzZjqdnpGR0dTUtHXrVi0trT/++EP2nZ0+fbrEibX8/HyEUHx8vPhCXV1dT0/PPlsJERERCKEHDx70GYBmjVteXh6NRktKSmppaSkuLh43btycOXMk1um+73BiDQAycTgcNps9dOjQwMDAtra2ly9fEk1UKhX/0erg4JCcnMzj8VJTU+XYhLe3d0tLS1RUlPKilqatre3Fixf47/0eubq6bty4saKiYsuWLRJNHR0dCQkJixYtWrp0qYGBgZOT05EjRxoaGo4dOya+Wo+Dxufzk5OTfX19/fz8DA0Nt23bRqPR5BsxAn77mba2tvhCGo3W0dHRZyvBzs4OIfTo0SPp29K4cfPw8AgPD+dyuWw229HRkcfjpaSkSKwj477LCFIOAEqDl3bu7OzssXXKlClMJpM4VdKf1dXVYRjGZDKlrLNz5057e/vDhw/fvn1bfHlJSUlra+uUKVOIJVOnTtXR0SFOKkoQH7SnT5+2t7c7OjriTbq6usOGDVNwxPBrKl1dXeILBQIBPiWd9FYCPhRv3ryRvi2NG7fIyMhjx47dvHmztbX1+fPnHA7H1dVVYu5nGfddRpByAFAfOp1eX19PdhR94/P5CKHul+LFMRiM1NRUCoWycuVK8WOC5uZmhJC+vr74yoaGhjwer8/ttrW1IYS2bdtGPP9RWVnZ3t4u317g8AtmLS0txJL29nY+n29ubt5nKwHPQPiwSKFZ4/b69et9+/Z9/fXXs2fP1tPTs7KyOn78eE1NDX6OlyDjvssIUg4AatLZ2dnc3DxixAiyA+kb/i3T5yOQrq6uoaGhpaWlcXFxxEK81qrEF6WMOz506FCE0IEDB8TP/itYBNLKyorFYonfhVVWVoYQmjBhQp+tBIFAgGSYq1uzxq20tFQoFA4fPpxYwmazjY2NS0pKxFeTcd9lBCkHADXJy8vDMMzFxQV/SaVSezsFRzpTU1MKhfLu3bs+14yLixs7duyDBw+IJY6Ojvr6+n/++Sex5N69ewKBYPLkyX32NnLkSAaDUVhYKF/YPaJSqfPmzbt165ZIJMKX5ObmUigU/KYy6a0EfCjMzMykb0uzxg1PZq9fvyaW8Hi8xsZG/FZpgoz7LiNIOQCokEgkampq6urqKioqCgkJsbS0DAoKwptsbW0bGxuzs7M7Ozvr6+vFf2gjhIyNjWtqaioqKng8XmdnZ25urjpvkmYymdbW1rLUEsVPE4lffmcwGJs2bcrKyjp9+nRLS8ujR4/WrFljbm4eHBwsS28rVqw4d+5ccnJyS0uLUCisrq7GvxMDAwPNzMzkmxgmKirqzZs327dvb2trKygoiI+PDwoKIsqQS2/F4UPh5OQkPRLNGjcrK6tZs2YdP3781q1bHR0dVVVV+LZWrVrV274rgSw3ukkBN0kDDSXHTdIHDx7ET/0zmcwFCxYcPnwYv7JqZ2dXXl5+7NgxNpuNEBo1atSzZ88wDAsODqbRaBYWFlQqlc1mL1y4sLy8nOjt7du3s2bNYjAYVlZW69evDwsLQwjZ2trid1Hfv39/1KhRurq6M2bMqK2tvXLlCovF2rlzpxx7iuS6SZrL5dJotPb2dvxlVlYWfiOWiYnJunXrJN4eFhYmfrOvSCSKj4+3s7Oj0WhGRka+vr5Pnz7Fm/octPfv34eHh1taWlKp1KFDh/r5+ZWUlGAY5uvrixCKjo7uMf6CggI3NzfiAsywYcM4HE5+fj6xQn5+/rRp0+h0urm5eVhYmPikPn22Yhjm7e1tYWGBP6UvPRLNGreGhoaQkBBbW1s6na6vr+/m5vbTTz9J2XecIjdJQ8oBg5QaJrzBJy5T6SZkIV/KKS0tpVKpp06dUmVoH0AoFLq7u584cUL9m25oaGAwGPv375clkgE2bhL7joPncgDop+SbhJgUHR0d165dKy0txS8X29raxsbGxsbGtra2kh0aEgqF2dnZPB4vMDBQ/VuPiYlxdnbmcrmyRDLAxk183zEMq6mpuX37Nn6HhXwg5QAAEEKosbHxs88+GzNmzMqVK/ElERER/v7+gYGBslwPV6m8vLzMzMzc3Fzpj7yoQkJCQmFh4ZUrV2g0moyRDJhxk9j3nJwcCwsLd3f3y5cvyx+TfEdbBDixplxnzpyZMmWKvr4+fp359evXH/T2J0+erFu3zsHBQV9fX1tbm81m29nZzZs3786dOyoKWHOp+sRaREQE/rDe6NGj09PTVbehPiEZTqxJce3atfDwcCXGo0Gys7N3797d1dUlx3s1fdwU2XdC9789SDn9iPTaFX1KSUmh0Wgff/zx1atXm5qa+Hx+eXn5+fPnORzO0aNHVRq5JurnxQuUSMGUA4Dcuv/tDaITax0dHT2WC+w/nR89enT48OFhYWEGBgbOzs6hoaGFhYW9TXch4e7du8HBwe7u7jdv3pwzZ46hoSGdTre2tg4ICIiOjsbPzqtZ/x9wAICaDaKUo9ICJErpvM/aFVLs3LlTKBTu2bOHSqVKNM2ZM2fdunUKxiaH/j/gAAA1U1/KOXXq1JQpUxgMhp6e3ujRo/GZHjAF6kP01udvv/3m4OBgYGDAYDCcnJyuXbuGeipA0mN5iT43qkjnfZJeu0JKxRSBQHDz5s0hQ4ZMmzZN+iZgwAEAZFLwVJ2M13IOHDiAENqzZ8/bt28bGxuPHj26ZMkSTLH6EL31mZ6eHhMT09jY+PbtWxcXF+L+cYkCJL2Vl5C+UQU7l0567QopFVOePXuGEHJxcelzEzDgBLiWA4Cqdf/bU0fKEQgEhoaGs2bNIpZ0dXUlJia2t7fr6+sHBgYSy//9738jhIhvVfzLiCghfvjwYYRQWVmZlD4lNr17927094zi4l9SHR0dTCaT2HR7ezudTv/mm2+kb1Txzvu0bds24tfAiBEjqqqqZHkXPi/TJ598In01GHBxkHIAULXuf3uS5/1VoaioqLm5ec6cOcQSbW3tDRs2/Pnnn3LXh+itT4m34LeTd38cT/byElIqoCjeuYTIyMiUlJSbN29Onz69rq5uy5Ytrq6ud+7ckZhlrzt8wvM+5ypXpCDHgBxwhBBx5WxgCwgICAgIIDsKAJA6Ug5eiwKfmlucIvUheusTIXT58uX4+PiSkpKWlpbeZuolykuIH1VIVMjokeo6x2tXREREzJ49GyGE164wMjKKj49PSkqS/t7Ro0czGAz89JoUMODdDYarPgEBASEhIa6urmQHAgad7j901JFy8HoMDQ0NEssVqQ/RW58vX7709fVdtGjR999/P3z48IMHD3777bfd306UlwgJCZF9R1TauYy1K3pEp9PnzJmTk5Pz+++/u7m5SbQ2NjZ+++23KSkpMODdffHFFx/6Fo0TEBDg6uo6GPYU9DfdU4467lgbPXq0sbHx9evXJZYrUh+itz4fPXrU2dn5zTffWFtbMxiM3k6byFdeQqWdy1i7ojcxMTF0Oj00NFSiZjtCqLi4GL9zGgYcAEAudaQcOp2+devWW7ducbncV69eiUQiHo/3+PFjRepD9NanpaUlQujGjRt8Pr+0tFT8KoV4ARJtbe3eyktIodLO+6xdIb1iirOz85kzZ4qLi93d3a9cufLu3bvOzs4XL14cP3581apV+FUQGHAAAMkUvCFB9glvDh065OTkxGAwGAzGRx99dPjwYUyx+hC99RkeHm5sbGxoaOjv73/o0CGEkI2NzcuXLyUKkPRYXqLPjSrSeZ9DJL12hSwVU16+fLl582YnJyd8jjVDQ8OPPvpo1apVv//+O74CDDgB7lgDQNW6/+1R8KVy8/f3Rwilp6cr0gkA6nfhwoWAgAAF//41AoVCSUtLg2s5QP26/+0NoglvAAAAkAtSjpo8efKE0jtSCk8B0P/duHEjIiIiMzPT2toa/8/y1Vdfia/g5eXFYrG0tbXHjx9///59suJECIlEogMHDvQ42+zt27fd3NyYTKa5uXl4ePj79+/FW8+ePTt16lQWizVq1KgVK1bgM10hhC5evLhv3z4NqvInEwVP1UHxAqCh4FpO/xcdHT1//vyWlhb8pY2NzZAhQxBCly5dEl8tNzfXx8eHjAD/17Nnz/DnEyZOnCjRVFxcrKurGxUV1draeufOHRMTkxUrVhCt0kuWJCYmenh4NDU1qW9PlKr73x4c5QCgKkqssDAIizXs3bv3/PnzFy5cYLFYxMKkpCQtLa3g4GDSC26Ke/jw4ZYtW9asWePs7Ny9NS4ubtiwYTt27NDT03N1dQ0PD//hhx+I2TGklyzZsGHDxIkT582b19XVpb79USVIOQCoihIrLAy2Yg1lZWVRUVE7duxgMBjiyzkcTkhIyKtXrzZv3kxWbN1NnDgxMzNzyZIldDpdoqmrq+vy5cseHh7EU2Vz587FMCwnJwd/2WfJkpiYmMLCwsTERJXvhlpAygFAGqz3cg9cLldHR2fYsGH4y7Vr1+rp6VEoFHyOBokKC0lJSQwGw9TUdPXq1ebm5gwGg8PhED9mP6grJLWSxcCQlJSEYdiCBQu6N+3cuXPMmDEpKSk3btzo8b1SPrI+i2UovQrG8+fPW1tb8SfMcDY2NgihoqIi/KX0kiUIISMjIw8PD3wKXQWD6RcUPFUH13KAhpLxWo70cg9LliwxMzMjVo6Pj0cI1dfX4y8lKiwEBwfr6ek9fvyYz+eXlJTgV4xfvnwpR1dSKll0hzTwWo61tbWDg4PEQhsbmxcvXmAYdufOHS0trdGjR7e2tmLdruUoUqFDvrIjhOnTp0tcy8nPz0cIxcfHiy/U1dX19PTE/y29ZAkuIiICIfTgwQPZI+knuv/twVEOAL3q6OhISEhYtGjR0qVLDQwMnJycjhw50tDQcOzYMfk6pFKp+K9vBweH5ORkHo+XmpoqRz/e3t4tLS1RUVHyhdHPtbW1vXjxAj8a6JGrq+vGjRsrKiq2bNki0STjR8bhcNhs9tChQwMDA9va2l6+fIkQ4vP5ycnJvr6+fn5+hoaG27Zto9Fo8n1ABPzmNG1tbfGFNBqNmJjKw8MjPDycy+Wy2WxHR0cej5eSkiLRiZ2dHULo0aNHikTST0DKAaBXH1ru4YNMmTKFyWTKWGRhUMFLIuHTUvRm586d9vb2hw8fvn37tvhyRSp0KFIFozf4tSiJi/8CgUBXVxf/d2Rk5LFjx27evNna2vr8+XMOh+Pq6lpVVSW+Pj4Ub968USSSfgJSDgC9UqTcgyzodHp9fb1SuhpI+Hw+Qqj7pXhxDAYjNTWVQqGsXLlSfCpbRT4yogoG8cBcZWVln2WopMOvz+G1P3Dt7e18Ph8vroGXLPn6669nz56tp6eHlyypqanBT6sS8PyED4umg5QDQK8UKffQp87OTmV1NcDg37B9PgLp6uoaGhpaWloaFxdHLFTkIyOqYIhfeygoKJBjFwhWVlYsFkv8DrSysjKE0IQJE5DMJUsEAgH6e1g0HaQcAHrVZ7kHKpXaW+G4PuXl5WEY5uLionhXA4ypqSmFQpHlyZu4uLixY8c+ePCAWKJIhQ5VVMGgUqnz5s27deuWSCTCl+Tm5lIoFPxmPBlLluBDYWZmpsTAyAIpB4Be9VnuwdbWtrGxMTs7u7Ozs76+XvzHLPq/FRbwdCISiZqamrq6uoqKikJCQiwtLYOCguToSnolC03HZDKtra2rq6v7XBM/vSZ+cV6RCh0MBqO3KhiBgYFmZmbyTagTFRX15s2b7du3t7W1FRQUxMfHBwUF2dvbIxlKluDwoXBycpJj6/2OgvfAwU3SQEPJeJO0lHIPGIa9fft21qxZDAbDyspq/fr1YWFhCCFbW1v81meJCgvBwcE0Gs3CwoJKpbLZ7IULF5aXl8vXlSyVLAhIA2+S5nK5NBqtvb0df5mVlYXfwGZiYrJu3TqJlcPCwsRvklakQkdvVTB8fX0RQtHR0T1GW1BQ4ObmRtQ+HzZsGIfDyc/PJ1bIz8+fNm0anU43NzcPCwvj8/lEk/SSJThvb28LCwuRSCTfYJKo+98epBwwSKl/jrXg4GBjY2N1bhGniSmntLSUSqWeOnWK7ED+h1AodHd3P3HihPo33dDQwGAw9u/fr/5NK6773x6cWANAfQbarMAqY2trGxsbGxsb29raSnYsSCgUZmdn83g8UmZ8j4mJcXZ25nK56t+0KkDKAQD0RxEREf7+/oGBgaTP4JmXl5eZmZmbmyv9USFVSEhIKCwsvHLlCl5LfgCAlAOAOmzdujU1NfXdu3dWVlYZGRlkh6MZdu3axeVy9+zZQ24Ynp6eZ86cIWbAU5ucnJz379/n5eUZGRmpedOqQyU7AAAGhd27d+/evZvsKDSPl5eXl5cX2VGQw8fHx8fHh+wolAyOcgAAAKgJpBwAAABqAikHAACAmkDKAQAAoCZKuH2gurr6woULivcDgDrh0zUOkj9dBeemBEBpFHy4dPHixWTvAQAAgH5KYvYBCjYw6mkD0A9cuHAhICAA/k8B0Bu4lgMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAMAAEBNqGQHAIAGq66uXr58uVAoxF82NTWxWKyZM2cSK9jb2x89epSc4ADofyDlACC/ESNGVFZWlpeXiy/Mz88n/v3xxx+rPSgA+i84sQaAQpYtW0aj0XprDQwMVGcwAPRzFAzDyI4BAA1WXl5uZ2fX4/+j8ePHFxcXqz8kAPotOMoBQCE2NjYTJkygUCgSy2k02vLly0kJCYB+C1IOAIpatmyZtra2xMKuri5/f39S4gGg34ITawAo6vXr1yNGjBCJRMQSLS2t6dOn37lzh8SoAOiH4CgHAEWZm5u7ublpaf3v/yYtLa1ly5aRGBIA/ROkHACU4KuvvhJ/iWHYokWLyAoGgH4LUg4ASrB48WLico62tvYnn3xiampKbkgA9EOQcgBQAiMjo08//RTPOhiGLV26lOyIAOiPIOUAoBxLly7F7yCg0WgLFy4kOxwA+iNIOQAox4IFC+h0OkJo/vz5+vr6ZIcDQH8EKQcA5dDT08MPbuCsGgC9gedylKb78+cAgMFp8eLF6enpZEfRH8FM0soUEhLi6upKdhRAIQEBAXJ/jkKhMC0t7csvv1R6VEp34MABhNDGjRvJDmQAwscW9AiOcpSGQqGkpaV98cUXZAcCFKLg58jn8xkMhnJDUgV8Mh74Ja4KMLZSwLUcAJRJI/INAGSBlAMAAEBNIOUAAABQE0g5AAAA1ARSDgAAADWBlAOAEly5csXAwODnn38mOxBVuXHjRkRERGZmprW1NYVCoVAoEpNne3l5sVgsbW3t8ePH379/n6w4EUIikejAgQMcDqd70+3bt93c3JhMprm5eXh4+Pv378Vbz549O3XqVBaLNWrUqBUrVtTW1uLLL168uG/fPqFQqI7oBzpIOQAowcB+2GD79u1JSUlbt2718/N7/vy5jY3NkCFDTp8+ffnyZWKd69evp6enz58/v6SkZNKkSWSFWlpa+vHHH4eGhra3t0s0lZSUeHl5eXp61tfXZ2Vlff/992vWrCFa09LSlixZ4u/vX11dnZOTc+vWrblz53Z1dSGEFixYwGAwPD09m5ub1bozAxGkHACUwNvb+927d/Pnz1f1hjo6Onr8/a46e/fuPX/+/IULF1gsFrEwKSlJS0srODj43bt36gxGuocPH27ZsmXNmjXOzs7dW+Pi4oYNG7Zjxw49PT1XV9fw8PAffvjhyZMneOvRo0eHDx8eFhZmYGDg7OwcGhpaWFh47949vHXDhg0TJ06cN28enoSA3CDlAKBJTpw4UVdXp7bNlZWVRUVF7dixQ+J5Iw6HExIS8urVq82bN6stmD5NnDgxMzNzyZIl+Pyq4rq6ui5fvuzh4UFMTDV37lwMw3JycvCXVVVV5ubmROvIkSMRQpWVlUQPMTExhYWFiYmJKt+NAQ1SDgCKun37tqWlJYVCOXToEEIoOTlZT0+PyWTm5OTMnTuXzWaPGDHi3Llz+MpJSUkMBsPU1HT16tXm5uYMBoPD4RC/prlcro6OzrBhw/CXa9eu1dPTo1AoDQ0NCKGQkJBNmzaVl5dTKBRbW1uE0NWrV9ls9q5du1S0a0lJSRiGLViwoHvTzp07x4wZk5KScuPGjR7fi2FYQkLCuHHj6HS6kZHRwoULiUMK6UOEEBIKhdHR0ZaWlrq6uhMmTEhLS1NwR54/f97a2mppaUkssbGxQQgVFRXhL62trcVzOX4hx9ramlhiZGTk4eGRmJg4sE+iqhqkHAAUNWPGjDt37hAvv/nmm40bN3Z0dLBYrLS0tPLycmtr63/+85+dnZ0IIS6XGxQU1N7evmHDhoqKivv373d1dX366adVVVUIoaSkJPG5dg4fPrxjxw7iZWJi4vz5821sbDAMKysrQwjh17TxOj2qcPnyZXt7eyaT2b1JV1f3hx9+0NLS+uc//9nW1tZ9hZiYmIiIiMjIyLq6ulu3blVVVbm7u7958wb1NUQIoS1btnz33XcHDhx4/fr1/Pnzv/zyyz///FORHcFTiPi5QQaDoauri8eDENq6dWttbe3Bgwd5PF5JSUliYuKcOXNcXFzEO/noo49evXr18OFDRSIZ5CDlAKAqHA6HzWYPHTo0MDCwra3t5cuXRBOVSsV//js4OCQnJ/N4vNTUVDk24e3t3dLSEhUVpbyo/1dbW9uLFy/wo4Eeubq6bty4saKiYsuWLRJNHR0dCQkJixYtWrp0qYGBgZOT05EjRxoaGo4dOya+Wo9DxOfzk5OTfX19/fz8DA0Nt23bRqPR5BsfAn5zGlEsHEej0To6OvB/e3h4hIeHc7lcNpvt6OjI4/FSUlIkOrGzs0MIPXr0SJFIBjlIOQConI6ODkKI+AkvYcqUKUwmkzjp1H/U1dVhGNbjIQ5h586d9vb2hw8fvn37tvjykpKS1tbWKVOmEEumTp2qo6NDnEKUID5ET58+bW9vd3R0xJt0dXWHDRum4Pjg16IkLv4LBAJdXV3835GRkceOHbt582Zra+vz5885HI6rqyt+6EnAh4I4MAJygJQDAPnodHp9fT3ZUUji8/kIoe6X4sUxGIzU1FQKhbJy5UriiAEhhN9PLFEd1dDQkMfj9bld/DTdtm3bKH+rrKzsftPzB8Evj7W0tBBL2tvb+Xy+ubk5Quj169f79u37+uuvZ8+eraenZ2Vldfz48Zqamvj4ePFO8PyEDwuQD6QcAEjW2dnZ3Nw8YsQIsgORhH/D9vkIpKura2hoaGlpaVxcHLHQ0NAQISSRYGTczaFDhyKEDhw4gIkpKCiQYxcIVlZWLBZL/A40/GLYhAkTEEKlpaVCoXD48OFEK5vNNjY2LikpEe9EIBCgv4cFyAdSDgAky8vLwzCMuFJNpVJ7OwWnZqamphQKRZYnb+Li4saOHfvgwQNiiaOjo76+vvg1/3v37gkEgsmTJ/fZ28iRIxkMRmFhoXxh94hKpc6bN+/WrVvErRa5ubkUCgW/GQ9PhK9fvybW5/F4jY2N+K3SBHwozMzMlBjYYAMpBwASiESipqamrq6uoqKikJAQS0vLoKAgvMnW1raxsTE7O7uzs7O+vl78hzlCyNjYuKampqKigsfjdXZ25ubmqu4maSaTaW1tXV1d3eea+Ok18YvzDAZj06ZNWVlZp0+fbmlpefTo0Zo1a8zNzYODg2XpbcWKFefOnUtOTm5paREKhdXV1Xg+CAwMNDMzk29CnaioqDdv3mzfvr2tra2goCA+Pj4oKMje3h4hZGVlNWvWrOPHj9+6daujo6OqqgqPc9WqVeI94EPh5OQkx9bB/8CAkiCE0tLSyI4CKEqOz/HgwYP4pQImk7lgwYLDhw/j15nt7OzKy8uPHTvGZrMRQqNGjXr27BmGYcHBwTQazcLCgkqlstnshQsXlpeXE729fft21qxZDAbDyspq/fr1YWFhCCFbW9uXL19iGHb//v1Ro0bp6urOmDGjtrb2ypUrLBZr586dH7qbixcvXrx4cZ+rcblcGo3W3t6Ov8zKysJvYDMxMVm3bp3EymFhYT4+PsRLkUgUHx9vZ2dHo9GMjIx8fX2fPn2KN/U5RO/fvw8PD7e0tKRSqUOHDvXz8yspKcEwzBq15tQAAA3ISURBVNfXFyEUHR3dY7QFBQVubm745RmE0LBhwzgcTn5+PrFCfn7+tGnT6HS6ubl5WFgYn88nmhoaGkJCQmxtbel0ur6+vpub208//STRv7e3t4WFhUgkkj5oMo7t4AQpR2kg5QwMavgcg4ODjY2NVbqJPsn4tVhaWkqlUk+dOqWGkGQhFArd3d1PnDih/k03NDQwGIz9+/f3uSakHCngxBoAJNCUaYltbW1jY2NjY2NbW1vJjgUJhcLs7GwejxcYGKj+rcfExDg7O3O5XPVveiCBlEOaf/zjHywWi0KhKPcyqSJiY2MdHBzYbDadTre1tf3222/Fv2hmzpxJ6UbiLtgeic94j9PR0TE1NZ05c2Z8fHxTU5Mq9wkoKiIiwt/fPzAwkPQZPPPy8jIzM3Nzc6U/KqQKCQkJhYWFV65codFoat70AAMphzQpKSnHjx8nO4r/49dff123bl1FRUVDQ8Pu3bsTExP9/f2lv2XGjBl9dkvMeG9gYIBhmEgkqquru3DhgpWVVXh4+Pjx4xWcy0SzbN26NTU19d27d1ZWVhkZGWSHI5Ndu3Zxudw9e/aQG4anp+eZM2eICejUJicn5/3793l5eUZGRmre9MBDJTsA0I/o6+sHBwfj9x198cUXmZmZFy5cqKqqwm8VZTAYLS0t4rNUrV69WnxCMBlRKBRDQ8OZM2fOnDnT29s7ICDA29v72bNnBgYGStyXfmv37t27d+8mO4oP5uXl5eXlRXYU5PDx8fHx8SE7igECjnLIRMyU3k9cunRJ/D5XExMThBDx1PfVq1fF801VVVVxcfHs2bMV2eLixYuDgoLq6uqOHDmiSD8AAI0AKUetMAyLj4+3t7en0+kGBgb4/a+EHids73Oad/y+TyaTyWaznZyc8Ck9lDL3+6tXr3R1da2srHps3bt374YNG4iXcs+ijz+Pkpubi7/sb4MAAFAmsm+ZGziQDDfXRkZGUiiU//zP/2xqampvbz98+DBC6MGDB3jr5s2b6XR6RkZGU1PT1q1btbS0/vjjD/xdCKGbN2++e/eurq7O3d1dT09PIBBgGNba2spms/ft29fR0VFbW7to0aL6+nopXcmura2NxWJxudweW6urqx0cHIRCIbHk0qVLLBYrNja2tw6JazkS8PQwcuTIfjIIsnyOAwDcyKs6MLZSQMpRmj6/qtrb25lM5qeffkoswX+n4ymno6ODyWQGBgYSK9Pp9G+++Qb7+9u2o6MDb8ITVVlZGYZhxcXFCKFLly6Jb0hKV7KLjIwcM2ZMS0tLj63r1q3717/+9UEd9pZyMAzDr+5g/WMQIOUABcHYSgG3D6hPWVlZe3u7p6dnj62yT9guPs27tbW1qanp0qVLN2zYEBQUNHr06A/qqjdZWVkXLly4fv26+MUbQk1NzcWLFyUm2ZVbW1sbhmH4w+f9ZBAUnEFSI+Bzt1y4cIHsQAag6urqfjhJa39Bds4bOFBfv46vXLmCEBJ/cFr8KOf333/v/um4uLhg3X7g47dW//XXX/jL4uLizz//nEqlUiiUgICA9vZ2KV3J4ty5c1OnTn316lVvK3C53Li4OBl7I/R2lIPPl+Xl5YX1j0GQ//8SAH+Do5zewO0D6oMXicKrE3Yn94Tt48eP//nnn2tqasLDw9PS0vbv36/I3O8HDx48ffr0r7/+Kj6Ru7ja2tqzZ89+8803svQmi6tXryKE5s6di/rNIMCJNaCIxYsXK/RfYkCDlKM+jo6OWlpa+fn5PbbKN2F7TU3N48ePEUJDhw7ds2fPpEmTHj9+LF9XGIaFh4c/evQoOztbypwC+/btW7p0qbGx8Qd13pva2toDBw6MGDFi5cqVqB8MAgBApSDlqA8+IW5GRsaJEydaWlqKiorE68BLmbBdipqamtWrVz958kQgEDx48KCystLFxUW+rh4/fvzdd98dP36cRqOJT06zf/9+Yp03b958//33Gzdu7P52WWbRxzCstbUVn4i3vr4+LS3Nzc1NW1s7Ozsbv5ZD+iAAAFSL5EPQAQTJcEKGx+P94x//GDJkiL6+/owZM6KjoxFCI0aMePjwIdbLhO3Sp3mvqKjgcDhGRkba2trDhw+PjIzs6urqrSvpsT169KjHv5D4+HhindDQ0KVLl/b4dimz6F+8eHHChAlMJlNHR0dLSwv9PQHBtGnTYmNj3759K74yuYOAwR1rQGEwtlJQMLheqiQUCiUtLU2OCWBAvzJIPkd89rz09HSyAxmAYGylgBNrAAAA1ARSzmDx5MmT7qUHCKQUIAEADDaQcgaLsWPHSjnBev78ebIDBBrsxo0bERER4oWRvvrqK/EVvLy8WCyWtrb2+PHj8SexyCISiQ4cOMDhcMQXXrx4cd++fZpSN0+jQcoBAChk+/btSUlJW7duJQojDRky5PTp05cvXybWuX79enp6+vz580tKSiZNmkRWqKWlpR9//HFoaCgxPzpuwYIFDAbD09OzubmZrNgGCUg5AKhVR0eHxE/s/tCV3Pbu3Xv+/PkLFy6Iz42UlJSkpaUVHBxMeiFRcQ8fPtyyZcuaNWucnZ27t27YsGHixInz5s3r6upSf2yDB6QcANTqxIkTdXV1/a0r+ZSVlUVFRe3YsQOfWYPA4XBCQkJevXq1efNmsmLrbuLEiZmZmUuWLKHT6T2uEBMTU1hYmJiYqObABhVIOQB8MAzDEhISxo0bR6fTjYyMFi5cSEwYyuVydXR0iGLJa9eu1dPTo1AoDQ0NCKGQkJBNmzaVl5dTKBRbW9ukpCQGg2Fqarp69Wpzc3MGg8HhcO7duydHV0iBkkVyS0pKwjBswYIF3Zt27tw5ZsyYlJSUGzdu9PheKWPYZ3kkFVVCMjIy8vDwSExMhEdHVEgdD/8MDmhwPEI44MnyOUZHR+vo6Jw6daq5ubmoqGjSpEkmJia1tbV465IlS8zMzIiV8Sm38RI+GIb5+fnZ2NgQrcHBwXp6eo8fP+bz+SUlJVOnTmWxWC9fvpSjqz5LFolTyuOK1tbWDg4OEgttbGxevHiBYdidO3e0tLRGjx7d2tqKYVhubq6Pjw+xmvQxlFIeCVO4HNT06dMnTpzYY1NERAQSK2ElH3gUVAo4ygHgw3R0dCQkJCxatGjp0qUGBgZOTk5HjhxpaGgQn77og1CpVPzHvoODQ3JyMo/HS01NlaMfb2/vlpaWqKgo+cL4UG1tbS9evLCxseltBVdX140bN1ZUVGzZskWiScYx5HA4bDZ76NChgYGBbW1tL1++RAjx+fzk5GRfX18/Pz9DQ8Nt27bRaDT5Rqw7Ozs7hFBvM3EAxUHKAeDDlJSUtLa2TpkyhVgydepUHR0d4oSYIqZMmcJkMj+ouBFZ6urqMAzDJyLqzc6dO+3t7Q8fPnz79m3x5R86huLlkRQvByUFvjtv3rxRSm+gO0g5AHwY/D5aicm2DQ0NeTyeUvqn0+n19fVK6Uql+Hw+Qqi3S/E4BoORmppKoVBWrlzZ0dFBLFdkDNva2hBC27ZtIx5krqyslLjpWW66urro710DqgApB4APY2hoiBCS+HJsbm5WSiHIzs5OZXWlavi3c5+PT7q6uoaGhpaWlsbFxRELFRlDRSoh9UkgEKC/dw2oAqQcAD6Mo6Ojvr7+n3/+SSy5d++eQCCYPHky/pJKpeKngOSQl5eHYZiLi4viXamaqakphUKR5cmbuLi4sWPHPnjwgFjS5xhKodJKSPjumJmZqaJzgCDlAPChGAzGpk2bsrKyTp8+3dLS8ujRozVr1pibmwcHB+Mr2NraNjY2Zmdnd3Z21tfXV1ZWir/d2Ni4pqamoqKCx+Ph6UQkEjU1NXV1dRUVFYWEhFhaWgYFBcnRlSwli5SIyWRaW1tXV1f3uSZ+ek1bW1t8ifQxlN5bb5WQAgMDzczMFJlQB98dJycnuXsAfSDnRrmBCMFN0gOCLJ+jSCSKj4+3s7Oj0WhGRka+vr5Pnz4lWt++fTtr1iwGg2FlZbV+/fqwsDCEkK2tLX7r8/3790eNGqWrqztjxoza2trg4GAajWZhYUGlUtls9sKFC8vLy+XrSkrJou6UciMvl8ul0Wjt7e34y6ysLPwGNhMTk3Xr1kmsHBYWJn6TtJQxlF4eCeu9EpKvry9CKDo6usdoCwoK3NzczM3N8a++YcOGcTic/Px88XW8vb0tLCzwKoJyg5ukpYCUozSQcgYGNX+OwcHBxsbGatscQSlfi6WlpVQq9dSpU0oJSXFCodDd3f3EiRPyvb2hoYHBYOzfv1/BMCDlSAEn1gAgmeZOYGxraxsbGxsbG9va2kp2LEgoFGZnZ/N4PLkrccTExDg7O3O5XOUGBsRBygEAyC8iIsLf3z8wMJD0GTzz8vIyMzNzc3OlPyrUm4SEhMLCwitXrtBoNKXHBgiQcgAgzdatW1NTU9+9e2dlZZWRkUF2OHLatWsXl8vds2cPuWF4enqeOXOGmJLug+Tk5Lx//z4vL8/IyEjpgQFxVLIDAGDw2r179+7du8mOQgm8vLy8vLzIjkJ+Pj4+Pj4+ZEcxKMBRDgAAADWBlAMAAEBNIOUAAABQE0g5AAAA1ISCQf07JaFQKC4uLhoxISOQIiMjYzB8jnfv3kUIEZO5ASW6e/eui4tLeno62YH0R5BylMbf35/sEAAA/QI+fzbZUfRHkHIAAACoCVzLAQAAoCaQcgAAAKgJpBwAAABqAikHAACAmvx/BY2qLfciv0cAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7X1WpoaJ8vZ",
        "outputId": "5cf50d05-5574-4e91-dad9-5e0ae71c6bc8"
      },
      "source": [
        "# 3.5 Compile model\r\n",
        "model.compile(loss = \"mean_squared_error\")\r\n",
        "history = model.fit(\r\n",
        "                    X_train,\r\n",
        "                    y_train,\r\n",
        "                    epochs = 10,\r\n",
        "                    verbose = 1\r\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5214\n",
            "Epoch 2/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4380\n",
            "Epoch 3/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5411\n",
            "Epoch 4/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4837\n",
            "Epoch 5/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4958\n",
            "Epoch 6/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4681\n",
            "Epoch 7/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4758\n",
            "Epoch 8/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4888\n",
            "Epoch 9/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4339\n",
            "Epoch 10/10\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QQ8xAgGLTOo",
        "outputId": "e958e9a0-cc59-45e4-9cd2-0388954fbfec"
      },
      "source": [
        "3.6 model.evaluate(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129/129 [==============================] - 0s 863us/step - loss: 2.4378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4377660751342773"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYuF2BagAAYt"
      },
      "source": [
        "# Wide and Deep Network--IInd version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYrB04GJNwrN"
      },
      "source": [
        "# 4.0 We have two inputs\r\n",
        "inputsA = tf.keras.Input(shape = X_train[:,:4].shape[1:])\r\n",
        "inputsB = tf.keras.Input(shape = X_train[:,1:8].shape[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1iVJhhcRTIi"
      },
      "source": [
        "# 4.1 One arm of network\r\n",
        "x = layers.Dense(100, activation = 'relu')(inputsB)\r\n",
        "x = layers.Dense(100,activation= 'relu')(x)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6iLjl8gSa69"
      },
      "source": [
        "# 4.2 Concatenate one input with output of another arm\r\n",
        "concat = layers.concatenate([x,inputsA])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG3SQHycS1Tu"
      },
      "source": [
        "# 4.3 Output layer\r\n",
        "out = layers.Dense(1,activation = 'sigmoid')(concat)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x90xNBumS_bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d320e37-778a-4ed9-aebf-19ce933f06eb"
      },
      "source": [
        "# 4.4 Create model and show summary\r\n",
        "model2 = Model(inputs = [inputsA,inputsB], outputs = [out])\r\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_18 (InputLayer)           [(None, 7)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 100)          800         input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 100)          10100       dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_17 (InputLayer)           [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 104)          0           dense_29[0][0]                   \n",
            "                                                                 input_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_31 (Dense)                (None, 1)            105         concatenate_9[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 11,005\n",
            "Trainable params: 11,005\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWwLnTskTPDi",
        "outputId": "5f8a2cf2-8fd6-492b-c513-1514f58fd354"
      },
      "source": [
        "# 4.5 Compile model\r\n",
        "model2.compile(\r\n",
        "               optimizer='rmsprop', \r\n",
        "               loss = 'mean_squared_error'\r\n",
        "               )\r\n",
        "\r\n",
        "# 4.6 Train the model now.\r\n",
        "#     Note the two train inputs\r\n",
        "model2.fit(\r\n",
        "            [X_train[:,:4], X_train[:,1:8]],\r\n",
        "            y_train,\r\n",
        "            epochs = 100\r\n",
        "           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.6578\n",
            "Epoch 2/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4948\n",
            "Epoch 3/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4901\n",
            "Epoch 4/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4797\n",
            "Epoch 5/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4541\n",
            "Epoch 6/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4691\n",
            "Epoch 7/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4731\n",
            "Epoch 8/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4998\n",
            "Epoch 9/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5091\n",
            "Epoch 10/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4542\n",
            "Epoch 11/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4512\n",
            "Epoch 12/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4789\n",
            "Epoch 13/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4811\n",
            "Epoch 14/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4869\n",
            "Epoch 15/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5363\n",
            "Epoch 16/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4781\n",
            "Epoch 17/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5037\n",
            "Epoch 18/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4610\n",
            "Epoch 19/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4809\n",
            "Epoch 20/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5035\n",
            "Epoch 21/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4914\n",
            "Epoch 22/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4429\n",
            "Epoch 23/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4965\n",
            "Epoch 24/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4645\n",
            "Epoch 25/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4459\n",
            "Epoch 26/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4714\n",
            "Epoch 27/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4697\n",
            "Epoch 28/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5143\n",
            "Epoch 29/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4575\n",
            "Epoch 30/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5729\n",
            "Epoch 31/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4698\n",
            "Epoch 32/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5025\n",
            "Epoch 33/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5378\n",
            "Epoch 34/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4445\n",
            "Epoch 35/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4885\n",
            "Epoch 36/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4371\n",
            "Epoch 37/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4203\n",
            "Epoch 38/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4827\n",
            "Epoch 39/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5119\n",
            "Epoch 40/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4790\n",
            "Epoch 41/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4559\n",
            "Epoch 42/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4504\n",
            "Epoch 43/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5163\n",
            "Epoch 44/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4440\n",
            "Epoch 45/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4931\n",
            "Epoch 46/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5242\n",
            "Epoch 47/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5359\n",
            "Epoch 48/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4938\n",
            "Epoch 49/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5103\n",
            "Epoch 50/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4931\n",
            "Epoch 51/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5341\n",
            "Epoch 52/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5141\n",
            "Epoch 53/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5196\n",
            "Epoch 54/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4987\n",
            "Epoch 55/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5157\n",
            "Epoch 56/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5070\n",
            "Epoch 57/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4943\n",
            "Epoch 58/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4640\n",
            "Epoch 59/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5108\n",
            "Epoch 60/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5197\n",
            "Epoch 61/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4418\n",
            "Epoch 62/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4828\n",
            "Epoch 63/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4737\n",
            "Epoch 64/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4872\n",
            "Epoch 65/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4442\n",
            "Epoch 66/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4393\n",
            "Epoch 67/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4949\n",
            "Epoch 68/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4189\n",
            "Epoch 69/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4954\n",
            "Epoch 70/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4745\n",
            "Epoch 71/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4598\n",
            "Epoch 72/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4241\n",
            "Epoch 73/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5375\n",
            "Epoch 74/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5019\n",
            "Epoch 75/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4823\n",
            "Epoch 76/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5538\n",
            "Epoch 77/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4443\n",
            "Epoch 78/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4503\n",
            "Epoch 79/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4703\n",
            "Epoch 80/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5018\n",
            "Epoch 81/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4757\n",
            "Epoch 82/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4117\n",
            "Epoch 83/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4730\n",
            "Epoch 84/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5220\n",
            "Epoch 85/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4587\n",
            "Epoch 86/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4905\n",
            "Epoch 87/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5263\n",
            "Epoch 88/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4712\n",
            "Epoch 89/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4886\n",
            "Epoch 90/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4464\n",
            "Epoch 91/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4934\n",
            "Epoch 92/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5552\n",
            "Epoch 93/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4752\n",
            "Epoch 94/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5213\n",
            "Epoch 95/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4983\n",
            "Epoch 96/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4724\n",
            "Epoch 97/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4810\n",
            "Epoch 98/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4708\n",
            "Epoch 99/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4802\n",
            "Epoch 100/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe6993e0668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "F5MvbvlUTYd3",
        "outputId": "e3d494d5-8388-4ab8-8504-8e0cc280d043"
      },
      "source": [
        "# 4.7 Plot our model\r\n",
        "plot_model(model2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqjA96pAERaO"
      },
      "source": [
        "# Two inputs and two outputs model--IIIrd ver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Bg8Lm2Ei0y",
        "outputId": "c24e307a-963c-4eed-94e5-830193b74428"
      },
      "source": [
        "# 5.0 We have two inputs.\r\n",
        "#     To distiguish them, we give names to each\r\n",
        "inputsA = tf.keras.Input(\r\n",
        "                          shape = X_train[:,:4].shape[1:],\r\n",
        "                          name = \"in_a\"\r\n",
        "                         )\r\n",
        "\r\n",
        "# 5.1\r\n",
        "inputsB = tf.keras.Input(\r\n",
        "                          shape = X_train[:,1:8].shape[1:],\r\n",
        "                          name = \"in_b\"\r\n",
        "                        )\r\n",
        "\r\n",
        "\r\n",
        "# 5.2 One arm of network\r\n",
        "x = layers.Dense(100, activation = 'relu')(inputsB)\r\n",
        "x = layers.Dense(100,activation= 'relu')(x)\r\n",
        "\r\n",
        "# 5.3 Concatenate an input with output of one arm\r\n",
        "concat = layers.concatenate([x,inputsA])\r\n",
        "\r\n",
        "# 5.4 Output layers\r\n",
        "#     We have two output layers. To distiguish them, we give names to each\r\n",
        "out_x = layers.Dense(1,activation = 'sigmoid' , name = \"out_a\")(concat)\r\n",
        "out_y = layers.Dense(1,activation = 'sigmoid', name = \"out_b\")(x)\r\n",
        "\r\n",
        "# 5.5 Create model and show summary\r\n",
        "#     While outputs are two, model is one\r\n",
        "main_model = Model(inputs = [inputsA,inputsB], outputs = [out_x, out_y])\r\n",
        "main_model.summary()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "in_b (InputLayer)               [(None, 7)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100)          800         in_b[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          10100       dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "in_a (InputLayer)               [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 104)          0           dense_1[0][0]                    \n",
            "                                                                 in_a[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "out_a (Dense)                   (None, 1)            105         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "out_b (Dense)                   (None, 1)            101         dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 11,106\n",
            "Trainable params: 11,106\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "BOmnKemAlzJ6",
        "outputId": "c8cc27f1-acfa-4f2c-cb8e-be25ed6690ea"
      },
      "source": [
        "# 5.6 Plot the model now\r\n",
        "plot_model(main_model, show_shapes = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAIECAIAAAB6zUNKAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzde1RTV74H8H0gQBJMeCgIA0YFBF/go9oLVEYtlatyARUsVHEWOvXycOQhtQi+EJGK9gKLSsZlRbqmdhQRB2wB22VvqcNUXVpFLR0RUFBEBUR5JbzCuX+c6bkZQAivJJDv57/svbPPbx+SzS/nsQ9F0zQBAAAAAA2jpeoAAAAAAEAFkAUCAAAAaCJkgQAAAACaCFkgAAAAgCbiqDoAAAAY2NWrV5OSklQdBQAo1Y4dO5ycnEavfxwLBAAYA548eXL+/HlVRwHDVV1drSF/x/Pnz1dXV6s6irHt/PnzT548GdVN4FggAMCYkZWVpeoQYFjOnTvn6+urCX9HiqIiIiLef/99VQcyhlEUNdqbwLFAAAAAAE2ELBAAAABAEyELBAAAANBEyAIBAAAANBGyQAAAAABNhCwQAABAreXn5xsYGHz99deqDmSEBQUFUb/x9/eXr7p8+XJ0dHR2draVlRXTYNOmTfIN3NzcBAKBtrb2nDlzbt26pdzA/2XZsmVULxMmTCCEXLx4MTExUSaTsY1zcnLYNpMmTVJJwL0hCwQAAFBrNE2rOoTRYmxsXFBQUFpamp6ezhbu378/NTU1JibG29v74cOH1tbWEydOPH36dF5eHtvmu+++y8rK8vDwKCkpWbhwoSpi79uSJUsIIZ6enlwu19XV9fXr10y5l5dXdXX1lStXVq9erdIA/w2yQAAAALXm7u7e2Njo4eEx2huSSqXOzs6jvRV5PB5v5cqVtra2enp6TMnhw4fPnj177tw5gUDANktNTdXS0goMDGxsbFRmeP3jcrlNTU20nMDAwI8//pipDQsLmzdv3urVq7u6ugghFEVZWFi4uLjMmDFDpVH/G2SBAAAAQAgh6enptbW1KgygvLx87969Bw4c4HK58uXOzs7h4eFPnz796KOPVBVbb5cuXZJPVZ88efLLL7+8++67bElsbGxxcXFKSooqolMIskAAAAD1VVRUJBKJKIo6duwYIUQsFuvr6/P5/Nzc3FWrVgmFQktLyzNnzjCNU1NTuVyuqalpUFCQubk5l8t1dna+fv06UxsaGqqrq2tmZsa83LZtm76+PkVR9fX1hJDw8PDIyMiKigqKomxsbAghly5dEgqFhw4dUtpgU1NTaZr29PTsXRUfH29ra3vy5MnLly/3+V6appOSkmbNmqWnp2dkZLRmzZr79+8zVf3vNEKITCbbt2+fSCTi8XgODg6ZmZlDCP7w4cNhYWHyJUZGRkuXLk1JSVHbc/rIAgEAANTXkiVLfvrpJ/ZlSEhIRESEVCoVCASZmZkVFRVWVlZbt27t7OwkhISGhgYEBEgkkrCwsMrKylu3bnV1da1YsYJ5HG1qaqr8I93S0tIOHDjAvkxJSfHw8LC2tqZpury8nBDC3NzQ3d2ttMHm5eXZ2dnx+fzeVTwe74svvtDS0tq6dWtra2vvBrGxsdHR0bt3766trb1y5cqTJ09cXFxevHhBBtpphJBdu3YdOXIkOTn52bNnHh4eGzZsuHnz5qAif/r0aWFhobe3d4/yBQsWPH369M6dO4PqTWmQBQIAAIw9zs7OQqHQxMTEz8+vtbX18ePHbBWHw2EOic2ePVssFjc3N2dkZAxhE+7u7k1NTXv37h25qPvT2tr66NEja2vrNzVwcnKKiIiorKzctWtXjyqpVJqUlLRu3Tp/f38DAwN7e/vjx4/X19efOHFCvlmfO62trU0sFq9du9bb29vQ0HDPnj06OjqD3WOHDx/evn27llbPtIq5CvDevXuD6k1pkAUCAACMYbq6uoQQ9rBWD4sWLeLz+ey5UXVWW1tL03SfBwJZ8fHxdnZ2aWlpRUVF8uUlJSUtLS2LFi1iSxYvXqyrq8ueDe9BfqeVlpZKJJK5c+cyVTwez8zMbFB7rKam5uLFiwEBAb2rmOEwhyTVELJAAACA8UxPT6+urk7VUQysra2NEMLeLNwnLpebkZFBUdSWLVukUilbzizIwqzVxzI0NGxubh5wu8z55T179rDr+VVVVUkkEsUjT0xM3Lp1a487Whg8Ho/8NjQ1hCwQAABg3Ors7Hz9+rWlpaWqAxkYkzDJr7TcJycnpx07dpSVlR08eJAtNDQ0JIT0yPkUHLiJiQkhJDk5WX7Nl6tXryoY9vPnz//617+GhIT0WdvR0UF+G5oaQhYIAAAwbhUWFtI07ejoyLzkcDhvOnescqamphRFKbIi4MGDB2fOnHn79m22ZO7cuRMmTJC/peP69esdHR1vvfXWgL1NmTKFy+UWFxcPLezExER/f39jY+M+a5nhTJ48eWidjzZkgQAAAONKd3f3q1evurq67t69Gx4eLhKJ2EvWbGxsGhoacnJyOjs76+rqqqqq5N9obGxcU1NTWVnZ3Nzc2dlZUFCgzJVi+Hy+lZVVdXX1gC2Z88La2tryJZGRkRcuXDh9+nRTU9O9e/eCg4PNzc0DAwMV6W3z5s1nzpwRi8VNTU0ymay6uvrZs2eEED8/v8mTJ/fzhLoXL16cOnUqIiLiTQ2Y4djb2w8YhkogCwQAAFBfx44dW7x4MSEkKirKy8tLLBYnJycTQhwcHB4+fPj5559HRkYSQlauXFlWVsa8pa2tzd7ensfjubi42Nra/vDDD+zFdiEhIcuXL//ggw/s7OwOHjzInKl0cnJilpIJDg42NTWdPXv26tWrGxoalD9Yd3f3kpIS9oK/v/3tbzY2NhUVFYsXL96+fbt8S0dHxx07dsiX7N+/PyEhIS4ubtKkSUuXLp02bVphYaG+vj4hZMCdlpKSEhERkZiYOHHiRHNz8/Dw8FevXhFCOjo6amtrc3Nz3xTwkSNHPD09RSLRmxrcuHHDwsLCwcFhyPtkdNEAAKD2mGVsVR0FDJcS/o6BgYHGxsajuglFEEIyMzP7bxMYGGhhYSFfUlZWxuFwvvzyy9EMbRBkMpmLi0t6evrQ3l5fX8/lcj/99FP5wrCwsIkTJyrydkX24TDhWCAAAMC4MuANFupDKpV+++23ZWVlzF0UNjY2cXFxcXFxLS0tqg6NyGSynJyc5uZmPz+/ofUQGxs7f/780NBQQghN0zU1NUVFRcyK3GoCWSAAAACoRkNDw8qVK21tbbds2cKUREdHr1+/3s/PT5HbREZVYWFhdnZ2QUFB/0sYvklSUlJxcXF+fr6Ojg4hJDc318LCwsXFJS8vb6QjHTpkgQAA40d+fr6BgcHXX389zH4WL16sra09f/78Ibz32rVrs2bN0tLSoihq8uTJ8fHxwwxGcdnZ2VZWVsySb2ZmZv7+/krbtJqIiYnJyMhobGycPn36+fPnVR3OAI4fP86emjx9+jRbfujQodDQ0E8++USFsRFCXF1dv/rqK/axy4OSm5vb3t5eWFhoZGTElKxZs4YdLPPgZnXAUXUAAAAwYugRemj9jRs33nvvvaH9r3J0dPznP/+5cuXKb7/9trS0lFnITTm8vb29vb1tbGzq6+ufP3+utO2qj4SEhISEBFVHMQLc3Nzc3NxUHcXQeXl5eXl5qTqKgSELBAAYP9zd3UfwPBpFUSPV1eiRSqWurq4//fSTqgMBGHtwRhgAAPrGXM+k5tLT02tra1UdBcCYhCwQAGCcKCoqEolEFEUdO3aMECIWi/X19fl8fm5u7qpVq4RCoaWl5ZkzZxTvsLy8fObMmfr6+szKc0VFRWzVpUuXFF9PuP9IUlNTuVyuqalpUFCQubk5l8t1dna+fv06UxsaGqqrq8tem7Vt2zZ9fX2Kopiz1eHh4ZGRkRUVFRRF2djYKDiuv//977NnzzYwMOByufb29t9++y0h5MMPP2QuKLS2tmYeSrF582Y+n29gYHDx4kVCiEwm27dvn0gk4vF4Dg4OzJovR44c4fP5AoGgtrY2MjLSwsKitLRUwTAAVA5ZIADAOLFkyRL5E6MhISERERFSqVQgEGRmZlZUVFhZWW3dulXxB4gZGRldunSpsbHx5s2bnZ2dK1asYNclZtYi6e7uVqSf/iMJDQ0NCAiQSCRhYWGVlZW3bt3q6upasWIFs45xamrq+++/z3aVlpZ24MAB9mVKSoqHh4e1tTVN04ovwPHixQtfX9/KysqampoJEyZs3LiREHLy5Elvb29tbe2///3vCxYsIIRkZGSsXbv29OnTnp6ehJBdu3YdOXIkOTn52bNnHh4eGzZsuHnz5scff7xjx46WlpaEhITp06c7OjqO1KWZAEqALBAAYJxzdnYWCoUmJiZ+fn6tra2PHz9W8I0CgWDatGkcDmfOnDmff/55W1vbiRMnmCp3d/empqa9e/eOVCQcDmfWrFl6enqzZ88Wi8XNzc0ZGRmD6lxxPj4++/fvNzIyMjY29vT0fPnyZV1dHSEkODhYJpOx221qarpx48bq1asJIW1tbWKxeO3atd7e3oaGhnv27NHR0ZGP8PDhw3/605+ys7Nnzpw5SmEDjDhkgQAAmkJXV5cQovixQHn29vYGBgZ3795VQiSLFi3i8/n3798fkW31j7n2kTm0+e6779ra2p46dYo5nnf27Fk/Pz/mYbWlpaUSiWTu3LnMu3g8npmZ2ZAjpDQAIcTX11fVUYxtI/IJ7x/uEQYAAIXo6OgMLYMcAj09Peb43GjIy8s7evRoSUlJU1OT/IgoigoKCtqxY8f333//3nvv/eUvf/nqq6+YqtbWVkLInj179uzZw7Y3NzcfWgDMNYXjm6+vb3h4uJOTk6oDGcN8fX1HexPIAgEAYGBdXV0NDQ0ikUgJ2+rs7Hz9+rWlpeUI9nnlypWff/45IiLi8ePHa9euXbdu3alTp373u9999tlnH3/8MdssICAgJibm5MmTU6ZMEQqFU6dOZcpNTEwIIcnJyeHh4cMPRv5Kx/HK19fXyclJE0Y6epAFAgCAWvjhhx+6u7sXLlyohG0VFhbSNO3o6Mi85HA4wz8G+fPPP+vr6xNC7t2719nZGRISYmVlRXqtiWhkZOTr63v27FmBQLB161a2fMqUKVwut7i4eJhhAKgVXBcIAAB96+joaGxs7OrqunXrVmho6NSpUwMCApiqgoICxVeKUUR3d/erV6+6urru3r0bHh4uEonYbdnY2DQ0NOTk5HR2dtbV1VVVVcm/0djYuKamprKysrm5uc9ksbOz88WLF4WFhUwWyBzOvHz5cltbW1lZGbskDSs4OLi9vf2bb77x8PBgC7lc7ubNm8+cOSMWi5uammQyWXV19bNnz0Zq+ACqQQMAgNpjriTrv81nn33GrKvH5/M9PT3T0tL4fD4hZMaMGRUVFSdOnBAKhYSQqVOnPnjwYMAtZmRkLF++3NTUlMPhTJw48YMPPqiqqmJr8/PzBQJBfHx87zdeu3Ztzpw5WlpahBAzM7NDhw4NGElgYKCOjo6FhQWHwxEKhWvWrKmoqGA7fPny5fLly7lc7vTp07dv375z505CiI2NzePHj2mavnXr1tSpU3k83pIlS/785z9bW1u/6f/dhQsXmA6joqKMjY0NDQ3Xr1/PrK1obW3N9MZYsGBBdHR0j3G1t7dHRUWJRCIOh2NiYuLt7V1SUpKYmMjj8QghU6ZM+fLLLwfcq4r8HccHQkhmZqaqoxjblLAPKRorGwEAqL1z5875+vqO1xk7KCgoKyvr5cuXqg7kX9zd3Y8dOzZ9+vQR73l8/x3lURSVmZmJ6wKHQwn7EGeEAQBA9Zi1WlSIPZt89+5d5rijauMBUAJkgQAAGuf+/fv9rFLm5+en6gBVICoqqqys7MGDB5s3bz548KCqw9EIQUFB7KfO399fvury5cvR0dHZ2dlWVlZMg02bNsk3cHNzEwgE2trac+bMuXXrlnID/5dly5b1/vpMmDCBEHLx4sXExET53zY5OTlsm0mTJqkk4N6QBQIAaJyZM2f2c6nQ2bNnlRlMTExMRkZGY2Pj9OnTz58/r8xNy+Pz+TNnznzvvfdiY2Nnz56tqjA0jbGxcUFBQWlpaXp6Olu4f//+1NTUmJgYb2/vhw8fWltbT5w48fTp03l5eWyb7777Lisry8PDo6SkRDm3ritoyZIlhBBPT08ul+vq6vr69Wum3MvLq7q6+sqVK8zTaNQEskAAAFClhISE9vZ2mqYfPXrk4+OjqjDi4+NlMtnjx4/lbw0ei6RSqbOzs7p19SY8Hm/lypW2trZ6enpMyeHDh8+ePXvu3DmBQMA2S01N1dLSCgwMbGxsHNV4BoXL5TY1Ncn/ggoMDGSXnwwLC5s3b97q1au7uroIIRRFWVhYuLi4zJgxQ6VR/xtkgQAAAONHenp6bW2tunWloPLy8r179x44cIDL5cqXOzs7h4eHP3369KOPPlJmPP27dOmSfKr65MmTX3755d1332VLYmNji4uLU1JSVBGdQpAFAgAAqBeappOSkmbNmqWnp2dkZLRmzRr2mcWhoaG6urrMkkCEkG3btunr61MUVV9fTwgJDw+PjIysqKigKMrGxiY1NZXL5ZqamgYFBZmbm3O5XGdnZ3aJxEF1RQi5dOnSyC4S2VtqaipN056enr2r4uPjbW1tT548efny5T7f289OE4vF+vr6fD4/Nzd31apVQqHQ0tLyzJkz7HtlMtm+fftEIhGPx3NwcBjaI/4OHz4cFhYmX2JkZLR06dKUlBT1vSt8lFagAQCAEaQ568yNbwr+Hfft26erq/vll1++fv367t27CxcunDRp0vPnz5najRs3Tp48mW189OhRQkhdXR3z0tvb29ramq0NDAzU19f/9ddf29raSkpKFi9eLBAI2MURB9XVN998IxAI4uLiFBkpUWCtu8DAQAsLC/kSKyur2bNn92hmbW396NEjmqZ/+uknLS2tadOmtbS00DRdUFDg5eXFNut/p+3evZsQ8v333zc2NtbW1rq4uOjr63d0dDC1H330kZ6e3vnz51+9ehUTE6OlpXXjxg1Fhsmqrq6ePXu2TCbrUR4dHU0IuX37NlsSFhY2ceJERfpUZB8OE44FAgAAqBGpVJqUlLRu3Tp/f38DAwN7e/vjx4/X19efOHFiaB1yOBzmCNns2bPFYnFzc3NGRsYQ+nF3d29qatq7d+/QwhhQa2vro0eP+ln328nJKSIiorKycteuXT2qFNxpzs7OQqHQxMTEz8+vtbX18ePHhJC2tjaxWLx27Vpvb29DQ8M9e/bo6OgMdhcdPnx4+/btzGLp8pirAO/duzeo3pQGWSAAAIAaKSkpaWlpWbRoEVuyePFiXV3d3g+7G4JFixbx+Xz2VKlaqa2tpWmaeczMm8THx9vZ2aWlpRUVFcmXD3an6erqkt8WiSwtLZVIJHPnzmWqeDyemZnZoHZRTU3NxYsX2WceymOG8+LFC8V7UyZkgQAAAGqEWVuEWXaOZWho2NzcPCL96+np1dXVjUhXI6utrY0Qwt4s3Ccul5uRkUFR1JYtW6RSKVs+nJ3W2tpKCNmzZw+7nl9VVZVEIlE88sTExK1bt/a4o4XBPGCQGZoaQhYIAACgRgwNDQkhPdKX169fW1paDr/zzs7OkepqxDEJ04BPkXFyctqxY0dZWZn84t7D2WkmJiaEkOTkZPkL5q5evapg2M+fP//rX/8aEhLSZ21HRwf5bWhqCFkgAACAGpk7d+6ECRNu3rzJlly/fr2jo+Ott95iXnI4HPZ5d4NVWFhI07Sjo+PwuxpxpqamFEUpsiLgwYMHZ86cefv2bbZkwJ3WjylTpnC53OLi4qGFnZiY6O/vb2xs3GctM5zJkycPrfPRhiwQAABAjXC53MjIyAsXLpw+fbqpqenevXvBwcHm5uaBgYFMAxsbm4aGhpycnM7Ozrq6uqqqKvm3Gxsb19TUVFZWNjc3Mxled3f3q1evurq67t69Gx4eLhKJ2CvYBtVVQUHBqK4Uw+fzraysqqurB2zJnBfW1taWL+l/p/Xf2+bNm8+cOSMWi5uammQyWXV19bNnzwghfn5+kydP7ucJdS9evDh16lRERMSbGjDDsbe3HzAMlUAWCAAAoF7279+fkJAQFxc3adKkpUuXTps2rbCwUF9fn6kNCQlZvnz5Bx98YGdnd/DgQeZso5OT05MnTwghwcHBpqams2fPXr16dUNDAyGkra3N3t6ex+O5uLjY2tr+8MMP7LV3g+1qtLm7u5eUlLAX/P3tb3+zsbGpqKhYvHjx9u3b5Vs6Ojru2LFDvqSfnSYWi5OTkwkhDg4ODx8+/PzzzyMjIwkhK1euLCsrI4SkpKREREQkJiZOnDjR3Nw8PDz81atXhJCOjo7a2trc3Nw3BXzkyBFPT0+RSPSmBjdu3LCwsHBwcBjyPhldo7oODQAAjAisFzg+KP/vGBgYaGxsrMwtMsiQ1gssKyvjcDhffvnlaIY2CDKZzMXFJT09fWhvr6+v53K5n376qXwh1gsEAAAAJRnwfgsVkkql3377bVlZGXMXhY2NTVxcXFxcXEtLi6pDIzKZLCcnp7m52c/Pb2g9xMbGzp8/PzQ0lBBC03RNTU1RUVF5efmIhjksyAIBAABANRoaGlauXGlra7tlyxamJDo6ev369X5+forcJjKqCgsLs7OzCwoK+l/C8E2SkpKKi4vz8/N1dHQIIbm5uRYWFi4uLnl5eSMd6dAhCwQAABifYmJiMjIyGhsbp0+ffv78eVWH09Px48fZU5OnT59myw8dOhQaGvrJJ5+oMDZCiKur61dffcU+Z3lQcnNz29vbCwsLjYyMmJI1a9awg2We1KwOOKoOAAAAAEZFQkJCQkKCqqMYCjc3Nzc3N1VHMXReXl5eXl6qjmJgOBYIAAAAoImQBQIAAABoImSBAAAAAJoIWSAAAACAJsLdIQAAY8a5c+dUHQIMy9WrV4nG/B2ZwYI6o5jFqQEAQJ2dO3fO19dX1VEAgFJlZma+//77o9c/skAAAAD1xSQBGnL4EJQM1wUCAAAAaCJkgQAAAACaCFkgAAAAgCZCFggAAACgiZAFAgAAAGgiZIEAAAAAmghZIAAAAIAmQhYIAAAAoImQBQIAAABoImSBAAAAAJoIWSAAAACAJkIWCAAAAKCJkAUCAAAAaCJkgQAAAACaCFkgAAAAgCZCFggAAACgiZAFAgAAAGgiZIEAAAAAmghZIAAAAIAmQhYIAAAAoImQBQIAAABoImSBAAAAAJoIWSAAAACAJkIWCAAAAKCJkAUCAAAAaCJkgQAAAACaCFkgAAAAgCZCFggAAACgiZAFAgAAAGgiZIEAAAAAmghZIAAAAIAmQhYIAAAAoImQBQIAAABoIoqmaVXHAAAAAP/y1Vdfpaend3d3My8fPXpECJk+fTrzUktL649//OPGjRtVFh+MI8gCAQAA1Mjdu3fnzZvXT4M7d+44ODgoLR4Yx5AFAgAAqJeZM2eWlpb2WWVjY1NWVqbkeGC8wnWBAAAA6mXTpk06Ojq9y3V0dDZv3qz8eGC8wrFAAAAA9fLw4UMbG5s+/0GXlZXZ2NgoPyQYl3AsEAAAQL1YWVktXLiQoij5QoqiFi1ahBQQRhCyQAAAALXzhz/8QVtbW75EW1v7D3/4g6rigXEJZ4QBAADUTm1trbm5ObteDCFES0urpqZm8uTJKowKxhkcCwQAAFA7pqamS5cuZQ8HamtrL1u2DCkgjCxkgQAAAOpo06ZN8ufrNm3apMJgYFzCGWEAAAB11NTUZGJi0tHRQQjR0dGpra01NDRUdVAwruBYIAAAgDoSCoUrV67kcDgcDmf16tVIAWHEIQsEAABQU/7+/jKZTCaT4cHBMBpwRhgAAEBNtbW1TZo0iabp+vp6Ho+n6nBgvEEWCAAwHqxfv/78+fOqjgIABsHHxycrK0uFAXBUuG0AABhBjo6OERERqo5CU/j6+oaHhzs5OY32hoqLiymKmjdv3mhvqE/JycmEEHyuRgOzb1ULWSAAwDhhaWn5/vvvqzoKTeHr6+vk5KSEHb5u3TpCCIejmv/XzJEqfK5Gg2qPAjKQBQIAAKgvVeV/oAlwjzAAAACAJkIWCAAAAKCJkAUCAAAAaCJkgQAAAACaCFkgAACAkuTn5xsYGHz99deqDkQ1Ll++HB0dnZ2dbWVlRVEURVGbNm2Sb+Dm5iYQCLS1tefMmXPr1i2VBLls2TKqlwkTJhBCLl68mJiYKJPJVBLYaEAWCAAAoCSa/KSG/fv3p6amxsTEeHt7P3z40NraeuLEiadPn87Ly2PbfPfdd1lZWR4eHiUlJQsXLlRhtD0sWbKEEOLp6cnlcl1dXV+/fq3qiEYGskAAAAAlcXd3b2xs9PDwGO0NSaVSZ2fn0d6K4g4fPnz27Nlz584JBAK2MDU1VUtLKzAwsLGxUYWx9cDlcpuammg5gYGBH3/8MVMbFhY2b9681atXd3V1qTbOEYEsEAAAYLxJT0+vra1VdRT/Ul5evnfv3gMHDnC5XPlyZ2fn8PDwp0+ffvTRR6qKrbdLly7Jp6pPnjz55Zdf3n33XbYkNja2uLg4JSVFFdGNMGSBAAAAylBUVCQSiSiKOnbsGCFELBbr6+vz+fzc3NxVq1YJhUJLS8szZ84wjVNTU7lcrqmpaVBQkLm5OZfLdXZ2vn79OlMbGhqqq6trZmbGvNy2bZu+vj5FUfX19YSQ8PDwyMjIiooKiqJsbGwIIZcuXRIKhYcOHVLBsAlJTU2ladrT07N3VXx8vK2t7cmTJy9fvtzne2maTkpKmjVrlp6enpGR0Zo1a+7fv89U9b8DCSEymWzfvn0ikYjH4zk4OGRmZg4h+MOHD4eFhcmXGBkZLV26NCUlZRyc30cWCAAAoAxLliz56aef2JchISERERFSqVQgEGRmZlZUVFhZWW3durWzs5MQEhoaGhAQIJFIwsLCKisrb9261dXVtWLFiidPnhBCUlNT5Z/qlpaWduDAAfZlSkqKh4eHtbU1TdPl5eWEEOaGhu7ubqUNVl5eXp6dnR2fz+9dxePxvvjiCy0tra1bt7a2tuIL86YAACAASURBVPZuEBsbGx0dvXv37tra2itXrjx58sTFxeXFixdkoB1ICNm1a9eRI0eSk5OfPXvm4eGxYcOGmzdvDiryp0+fFhYWent79yhfsGDB06dP79y5M6je1BCyQAAAAFVydnYWCoUmJiZ+fn6tra2PHz9mqzgcDnMYbPbs2WKxuLm5OSMjYwibcHd3b2pq2rt378hFrajW1tZHjx5ZW1u/qYGTk1NERERlZeWuXbt6VEml0qSkpHXr1vn7+xsYGNjb2x8/fry+vv7EiRPyzfrcgW1tbWKxeO3atd7e3oaGhnv27NHR0Rns3jt8+PD27du1tHomSzNmzCCE3Lt3b1C9qSFkgQAAAGpBV1eXEMIeyuph0aJFfD6fPR86VtTW1tI03eeBQFZ8fLydnV1aWlpRUZF8eUlJSUtLy6JFi9iSxYsX6+rqsmfGe5DfgaWlpRKJZO7cuUwVj8czMzMb1N6rqam5ePFiQEBA7ypmOMwhyTENWSAAAMDYoKenV1dXp+ooBqetrY0Qoqen108bLpebkZFBUdSWLVukUilbzizIwqzVxzI0NGxubh5wu8z55T179rBr/lVVVUkkEsUjT0xM3Lp1a487Whg8Ho/8NrQxDVkgAADAGNDZ2fn69WtLS0tVBzI4TMI04ErLTk5OO3bsKCsrO3jwIFtoaGhICOmR8ym4E0xMTAghycnJ8mu+XL16VcGwnz9//te//jUkJKTP2o6ODvLb0MY0ZIEAAABjQGFhIU3Tjo6OzEsOh/Omc8dqxdTUlKIoRVYEPHjw4MyZM2/fvs2WzJ07d8KECfK3dFy/fr2jo+Ott94asLcpU6Zwudzi4uKhhZ2YmOjv729sbNxnLTOcyZMnD61z9YEsEAAAQE11d3e/evWqq6vr7t274eHhIpGIvUzNxsamoaEhJyens7Ozrq6uqqpK/o3GxsY1NTWVlZXNzc2dnZ0FBQWqWimGz+dbWVlVV1cP2JI5L6ytrS1fEhkZeeHChdOnTzc1Nd27dy84ONjc3DwwMFCR3jZv3nzmzBmxWNzU1CSTyaqrq589e0YI8fPzmzx5cj9PqHvx4sWpU6ciIiLe1IAZjr29/YBhqDlkgQAAAMpw7NixxYsXE0KioqK8vLzEYnFycjIhxMHB4eHDh59//nlkZCQhZOXKlWVlZcxb2tra7O3teTyei4uLra3tDz/8wF5gFxISsnz58g8++MDOzu7gwYPM2UknJydmKZng4GBTU9PZs2evXr26oaFBJeNlubu7l5SUsBf8/e1vf7OxsamoqFi8ePH27dvlWzo6Ou7YsUO+ZP/+/QkJCXFxcZMmTVq6dOm0adMKCwv19fUJIQPuwJSUlIiIiMTExIkTJ5qbm4eHh7969YoQ0tHRUVtbm5ub+6aAjxw54unpKRKJ3tTgxo0bFhYWDg4OQ94naoIaB2seAgDA+vXrCSFZWVmqDkRTUBSVmZkpv2jfiAsKCsrKynr58uXobWJAI/K5Ki8vnzVrVkZGhr+//wjFNSzd3d3Lli0LCAjYsmXLEN7+8uVLS0vL+Ph4JukcMnX4zuJYIAAAgJoa8KaKMcHGxiYuLi4uLq6lpUXVsRCZTJaTk9Pc3Ozn5ze0HmJjY+fPnx8aGjqygakEskAAAA314YcfCgQCiqKGfAX9yMrOzraysqLk6OrqmpqaLlu27OjRo8y5PBijoqOj169f7+fnp8htIqOqsLAwOzu7oKCg/yUM3yQpKam4uDg/P19HR2fEY1M+ZIEAABrq5MmTn3/+uaqj+H/e3t4PHz60trY2MDCgabq7u7u2tvbcuXPTp0+PioqaM2fOYB//NabFxMRkZGQ0NjZOnz79/Pnzqg5nBBw6dCg0NPSTTz5RbRiurq5fffUV+wjmQcnNzW1vby8sLDQyMhrxwFSCo+oAAAAA+kBRlKGh4bJly5YtW+bu7u7r6+vu7v7gwQMDAwNVh6YMCQkJCQkJqo5ihLm5ubm5uak6iqHz8vLy8vJSdRQjCccCAQA0F0VRqg5BIT4+PgEBAbW1tcePH1d1LADjB7JAAAANQtP00aNH7ezs9PT0DAwMdu7cKV8rk8n27dsnEol4PJ6Dg0NmZiYhRCwW6+vr8/n83NzcVatWCYVCS0vLM2fOsO/68ccf3377bT6fLxQK7e3tm5qa3tQVIeTSpUtDW7iOWSevoKBAaaECjHvIAgEANMjevXujoqICAwNfvHjx/PnzXbt2ydfu2rXryJEjycnJz5498/Dw2LBhw82bN0NCQiIiIqRSqUAgyMzMrKiosLKy2rp1K/PgitbWVk9PTx8fn4aGhrKyMltbW+bhWn12RX676bW7u3uwkc+fP58Q8vDhQ6WFCjD+0QAAMPb5+Pj4+Pj030YikfD5/BUrVrAlzHGy27dv0zQtlUr5fL6fnx/bWE9PLyQkhKbp3bt3E0KkUilTlZaWRggpLy+nafqXX34hhHzzzTfyG+qnqwGxd4f0xlwpqCahEkIyMzMVGdGYpsjnCoZGHfYt7g4BANAU5eXlEonE1dW1z9rS0lKJRDJ37lzmJY/HMzMzu3//fu+Wurq6hBDmAJuVlZWpqam/v39YWFhAQMC0adMG1ZXiWltbaZoWCoXqE+rVq1eHM6IxgXlU2rlz51QdyDhUXV1taWmp4iBUm4QCAMCIUOS4Qn5+PiEkPT2dLZE/FviPf/yj9/8IR0dHutcBNmZ9mX/+85/My19++eW//uu/OBwORVG+vr4SiaSfrgb0pmOBzFNf3dzc1CTUYfzjBfgXlR8LxHWBAACagsvlEkLa29v7rDUxMSGEJCcny/+TUORw15w5c77++uuampqoqKjMzMxPP/10yF3149KlS4SQVatWqU+oOCMMw+Hj4zOcb8SIQBYIAKAp5s6dq6Wl9eOPP/ZZO2XKFC6XO9jniNTU1Pz666+EEBMTk08++WThwoW//vrr0Lrqx/Pnz5OTky0tLZkHv6pzqABjCLJAAABNYWJi4u3tff78+fT09Kamprt37544cYKt5XK5mzdvPnPmjFgsbmpqkslk1dXVz54967/PmpqaoKCg+/fvd3R03L59u6qqytHRsZ+uCgoKBlwphqbplpaW7u5umqbr6uoyMzPfeecdbW3tnJwc5rpA5YQKMP6p+oAoAACMAAXP3DU3N3/44YcTJ06cMGHCkiVL9u3bRwixtLS8c+cOTdPt7e1RUVEikYjD4TApY0lJSVpaGvPE1RkzZlRUVJw4cYJJxaZOnfrgwYPKykpnZ2cjIyNtbe3f/e53u3fv7urqelNXNE3n5+cLBIL4+PjesV28eNHBwYHP5+vq6mppaZHfHh/y9ttvx8XFvXz5Ur6xEkLtH8EZYRgeddi3FI1LXAEAxr7169cTQrKyslQdiKagKCozM/P9999XdSCjC5+r0aMO+xZnhAEAAAA0EbJAAAAAAE2ELBAAAABGxuXLl6Ojo7Ozs62srCiKoihq06ZN8g3c3NwEAoG2tvacOXOYNSBVpbu7Ozk52dnZuXdVUVHRO++8w+fzzc3No6Kieiyu9KbaixcvJiYmMs9IHCuQBQIAAMAI2L9/f2pqakxMjLe398OHD62trSdOnHj69Om8vDy2zXfffZeVleXh4VFSUrJw4UJVhVpWVvb73/9+x44dEomkR1VJSYmbm5urq2tdXd2FCxdOnToVHBysSK2npyeXy3V1dX39+rXyRjI8yAIBAADUjlQq7fMwlWq76sfhw4fPnj177tw5gUDAFqampmppaQUGBjY2No52AIq7c+fOrl27goOD58+f37v24MGDZmZmBw4c0NfXd3JyioqK+uKLL9iHCvZfGxYWNm/evNWrV3d1dSlvPMOALBAAAEDtpKen19bWqltXb1JeXr53794DBw4wz6dhOTs7h4eHP3369KOPPhrVAAZl3rx52dnZGzdu1NPT61HV1dWVl5e3dOlSiqKYklWrVtE0nZubO2AtIzY2tri4OCUlRSlDGS5kgQAAAKOCpumkpKRZs2bp6ekZGRmtWbOGPWgUGhqqq6trZmbGvNy2bZu+vj5FUfX19YSQ8PDwyMjIiooKiqJsbGxSU1O5XK6pqWlQUJC5uTmXy3V2dr5+/foQuiKEXLp0acCFuwcrNTWVpmlPT8/eVfHx8ba2tidPnrx8+fJg95JYLNbX1+fz+bm5uatWrRIKhZaWlszDrxkymWzfvn0ikYjH4zk4OGRmZg5zIA8fPmxpaRGJRGyJtbU1IeTu3bsD1jKMjIyWLl2akpIyJlbiQxYIAAAwKmJjY6Ojo3fv3l1bW3vlypUnT564uLi8ePGCEJKamiq/1mBaWtqBAwfYlykpKR4eHtbW1jRNl5eXh4aGBgQESCSSsLCwysrKW7dudXV1rVix4smTJ4PtihDC3L7Q3d09giPNy8uzs7NjluzugcfjffHFF1paWlu3bm1tbe3doJ+9FBISEhERIZVKBQJBZmZmRUWFlZXV1q1bOzs7mffu2rXryJEjycnJz5498/Dw2LBhw82bN4czkOfPnxNC5E9qc7lcHo/HxNN/LWvBggVPnz69c+fOcCJRDmSBAAAAI08qlSYlJa1bt87f39/AwMDe3v748eP19fXyT+0bFA6Hwxwwmz17tlgsbm5uzsjIGEI/7u7uTU1Ne/fuHVoYvbW2tj569Ig5KtYnJyeniIiIysrKXbt29ahScC85OzsLhUITExM/P7/W1tbHjx8TQtra2sRi8dq1a729vQ0NDffs2aOjozO0fcJibvjV1taWL9TR0ZFKpQPWsmbMmEEIuXfv3nAiUQ5kgQAAACOvpKSkpaVl0aJFbMnixYt1dXXZM7nDsWjRIj6fz545Va3a2lqapvs8EMiKj4+3s7NLS0srKiqSLx/sXtLV1SWEMMcCS0tLJRLJ3LlzmSoej2dmZjbMfcJc19jj3o6Ojg4ejzdgLYvZFT0OEKonZIEAAAAjj1kuZMKECfKFhoaGzc3NI9K/np5eXV3diHQ1TG1tbYSQ3ndayONyuRkZGRRFbdmyRf7I2XD2EnN+ec+ePdRvqqqqeq/8MijM5ZVNTU1siUQiaWtrMzc3H7CWxSSFzG5Rc8gCAQAARp6hoSEhpEc28/r1a0tLy+F33tnZOVJdDR+T9Ay4WrKTk9OOHTvKysoOHjzIFg5nL5mYmBBCkpOTaTlXr14dwhBY06dPFwgEVVVVbAlzMaWDg8OAtayOjg7y225Rc8gCAQAARt7cuXMnTJggf7PC9evXOzo63nrrLeYlh8Nh73IYrMLCQpqmHR0dh9/V8JmamlIUpciKgAcPHpw5c+bt27fZkgH3Uj+mTJnC5XKLi4uHFnafOBzO6tWrr1y5wt49U1BQQFEUc/tz/7UsZldMnjx5BAMbJcgCAQAARh6Xy42MjLxw4cLp06ebmpru3bsXHBxsbm4eGBjINLCxsWloaMjJyens7Kyrq5M/wkQIMTY2rqmpqaysbG5uZjK87u7uV69edXV13b17Nzw8XCQSBQQEDKGrgoKCkV0phs/nW1lZVVdXD9iSOS8sf3fFgHup/942b9585swZsVjc1NQkk8mqq6ufPXtGCPHz85s8efLQnlC3d+/eFy9e7N+/v7W19erVq0ePHg0ICLCzs1OklsHsCnt7+yFsXdloAAAY+3x8fHx8fFQdhQYhhGRmZvbfpru7++jRozNmzNDR0TEyMlq7dm1paSlb+/Lly+XLl3O53OnTp2/fvn3nzp2EEBsbm8ePH9M0fevWralTp/J4vCVLljx//jwwMFBHR8fCwoLD4QiFwjVr1lRUVAytq/z8fIFAEB8fr8gwFfxchYaG6ujoSCQS5uWFCxeYW4YnTZr0pz/9qUfjnTt3enl5KbKX0tLSmDstZsyYUVFRceLECaFQSAiZOnXqgwcPaJpub2+PiooSiUQcDsfExMTb27ukpISm6bVr1xJC9u3b12e0V69efeedd9iL+czMzJydnX/88Ue2wY8//vj222/r6emZm5vv3Lmzra1N/u3919I07e7ubmFh0d3d3f9OU4fvLEWPhVUNAQCgf+vXryeEZGVlqToQTUFRVGZmpvxCfaMqKCgoKyvr5cuXytkcS8HPVXl5+axZszIyMvz9/ZUS1wC6u7uXLVsWEBCwZcsWJW/65cuXlpaW8fHxkZGR/bdUh+8szggDAACMAQPefqFCNjY2cXFxcXFxLS0tqo6FyGSynJyc5uZmPz8/5W89NjZ2/vz5oaGhyt/0ECALBAAAgOGKjo5ev369n5+fIreJjKrCwsLs7OyCgoL+lzAcDUlJScXFxfn5+To6Okre9NAgCwQAAFBrMTExGRkZjY2N06dPP3/+vKrDeaNDhw6FhoZ+8sknqg3D1dX1q6++Yh+srDS5ubnt7e2FhYVGRkZK3vSQcVQdAAAAAPQnISEhISFB1VEoxM3Nzc3NTdVRqIaXl5eXl5eqoxgcHAsEAAAA0ETIAgEAAAA0EbJAAAAAAE2ELBAAAABAE+HuEACAceLatWvMOrSgHMnJyeN+me5r166R39Y3hpF17do19knQqoIsEABgPHByclJ1CJrFx8dHORu6ffs2IWTBggXK2VwPKk9TxjFHR0eVf23xBDkAAAD1xTyk7ty5c6oOBMYhXBcIAAAAoImQBQIAAABoImSBAAAAAJoIWSAAAACAJkIWCAAAAKCJkAUCAAAAaCJkgQAAAACaCFkgAAAAgCZCFggAAACgiZAFAgAAAGgiZIEAAAAAmghZIAAAAIAmQhYIAAAAoImQBQIAAABoImSBAAAAAJoIWSAAAACAJkIWCAAAAKCJkAUCAAAAaCJkgQAAAACaCFkgAAAAgCZCFggAAACgiZAFAgAAAGgiZIEAAAAAmghZIAAAAIAmQhYIAAAAoImQBQIAAABoImSBAAAAAJoIWSAAAACAJkIWCAAAAKCJkAUCAAAAaCJkgQAAAACaCFkgAAAAgCbiqDoAAAAA+H8SiaS9vZ192dHRQQh59eoVW6Knp8fn81UQGYw7FE3Tqo4BAAAA/kUsFm/btq2fBmlpaSEhIUqLB8YxZIEAAABqpK6uztzcXCaT9Vmrra397NkzExMTJUcF4xKuCwQAAFAjJiYmrq6u2travau0tbXfe+89pIAwUpAFAgAAqBd/f/8+z9TRNO3v76/8eGC8whlhAAAA9dLc3GxiYiJ/jwhDV1e3rq5OKBSqJCoYf3AsEAAAQL0IBAIPDw8dHR35Qg6H4+XlhRQQRhCyQAAAALWzcePGrq4u+RKZTLZx40ZVxQPjEs4IAwAAqJ2Ojo5JkyY1NzezJRMmTKivr9fT01NhVDDO4FggAACA2tHV1V2/fr2uri7zUkdHx9fXFykgjCxkgQAAAOpow4YNzINDCCGdnZ0bNmxQbTww/uCMMAAAgDrq7u42MzOrq6sjhEyaNOn58+d9LiIIMGQ4FggAAKCOtLS0NmzYoKurq6Ojs3HjRqSAMOKQBQIAAKipDz74oKOjA6eDYZRwVB2Aujh37pyqQwAA5XF2dra0tFR1FINz9erVJ0+eqDoKUCqapidOnEgIefToUWVlparDAaWaMmWKk5PT6G6DBpqmcXEkgIbJzMxU9awzaD4+PqrebQCgPD4+PqM9q+BY4P/LzMx8//33VR0FDNG5c+d8fX1pDUjoKYrCZ3WYKIpSdQhD5OPjk5WVpeooYGAj+D399ddfCSGzZ88eflcjbv369YQQfCZHA7NvRxuyQAAAAPWlnvkfjA+4OwQAAABAEyELBAAAANBEyAIBAAAANBGyQAAAAABNhCwQAAAAQBMhCwSNlp+fb2Bg8PXXX6s6kBEWFBRE/cbf31++6vLly9HR0dnZ2VZWVkyDTZs2yTdwc3MTCATa2tpz5sy5deuWcgPvQ1tb28yZM/fs2cO8vHjxYmJiokwmYxvk5OSwg500aZKKwgQYAeN1RmKNofmnu7s7OTnZ2dm5d1VRUdE777zD5/PNzc2joqLa29sVqe09d6kDZIGg0cbx+oLGxsYFBQWlpaXp6els4f79+1NTU2NiYry9vR8+fGhtbT1x4sTTp0/n5eWxbb777rusrCwPD4+SkpKFCxeqIvZ/s3v37tLSUvalp6cnl8t1dXV9/fo1U+Ll5VVdXX3lypXVq1erKEaAkTGOZyQypuafsrKy3//+9zt27JBIJD2qSkpK3NzcXF1d6+rqLly4cOrUqeDgYEVqe89d6gBZIGg0d3f3xsZGDw+P0d6QVCrt8zfl6OHxeCtXrrS1tdXT02NKDh8+fPbs2XPnzgkEArZZamqqlpZWYGBgY2OjMsNT0E8//fTLL7/0KAwLC5s3b97q1au7uroIIRRFWVhYuLi4zJgxQxUxAoyYcTwjjaH5586dO7t27QoODp4/f37v2oMHD5qZmR04cEBfX9/JySkqKuqLL764f/++IrU95i51gCwQQBnS09Nra2tVGEB5efnevXsPHDjA5XLly52dncPDw58+ffrRRx+pKrY3kUqlO3fuTElJ6V0VGxtbXFzcZxUADEjJM9LYmn/mzZuXnZ29ceNG9ic0q6urKy8vb+nSpezzh1atWkXTdG5u7oC1DHWbu5AFguYqKioSiUQURR07dowQIhaL9fX1+Xx+bm7uqlWrhEKhpaXlmTNnmMapqalcLtfU1DQoKMjc3JzL5To7O1+/fp2pDQ0N1dXVNTMzY15u27ZNX1+foqj6+npCSHh4eGRkZEVFBUVRNjY2hJBLly4JhcJDhw4pbbCpqak0TXt6evauio+Pt7W1PXny5OXLl/t8L03TSUlJs2bN0tPTMzIyWrNmDfvTtv+dRgiRyWT79u0TiUQ8Hs/BwSEzM1PxmHfv3r1t2zYTE5PeVUZGRkuXLk1JSRnfZ9BAo4zjGWkszj99evjwYUtLi0gkYkusra0JIXfv3h2wlqFucxeyQNBcS5Ys+emnn9iXISEhERERUqlUIBBkZmZWVFRYWVlt3bq1s7OTEBIaGhoQECCRSMLCwiorK2/dutXV1bVixYonT54QQlJTU+UfGJqWlnbgwAH2ZUpKioeHh7W1NU3T5eXlhBDmAuHu7m6lDTYvL8/Ozo7P5/eu4vF4X3zxhZaW1tatW1tbW3s3iI2NjY6O3r17d21t7ZUrV548eeLi4vLixQsy0E4jhOzatevIkSPJycnPnj3z8PDYsGHDzZs3FQn4H//4R0VFxYYNG97UYMGCBU+fPr1z545C4wdQe+N4Rhpz88+bPH/+nBAif1Kby+XyeDwmnv5rWWo1dyELBOjJ2dlZKBSamJj4+fm1trY+fvyYreJwOMxP0tmzZ4vF4ubm5oyMjCFswt3dvampae/evSMXdX9aW1sfPXrE/Crtk5OTU0RERGVl5a5du3pUSaXSpKSkdevW+fv7GxgY2NvbHz9+vL6+/sSJE/LN+txpbW1tYrF47dq13t7ehoaGe/bs0dHRUWSPSaXS8PBwsVjcTxvmKsB79+4N2BvAmDbWZ6QxN//0g7nhV1tbW75QR0dHKpUOWMtSq7kLWSDAG+nq6hJC2J+VPSxatIjP57PnJtRZbW0tTdN9/hBnxcfH29nZpaWlFRUVyZeXlJS0tLQsWrSILVm8eLGuri577qkH+Z1WWloqkUjmzp3LVPF4PDMzM0X2WExMzH//939bWFj004YZTo8f2QDj2Bidkcbc/NMP5rrGHvd2dHR08Hi8AWtZajV3IQsEGDo9Pb26ujpVRzGwtrY2QkjvK53lcbncjIwMiqK2bNki/8uVWdRgwoQJ8o0NDQ2bm5sH3C5zfmfPnj3sen5VVVW9V17ooaio6N69ex9++GH/zZiJlRkaABB1nZHG1vzTP+ZSy6amJrZEIpG0tbWZm5sPWMtSq7kLWSDAEHV2dr5+/drS0lLVgQyMmXQGXK3Uyclpx44dZWVlBw8eZAsNDQ0JIT3mXAUHztzYkZycTMu5evVq/+9KT0///vvvtbS0mImb6eTQoUMURclf09PR0cEODQDUdkYaW/NP/6ZPny4QCKqqqtgS5sJKBweHAWtZajV3IQsEGKLCwkKaph0dHZmXHA7nTWdqVM7U1JSiKEVW5Dp48ODMmTNv377NlsydO3fChAny6df169c7OjreeuutAXubMmUKl8stLi4eVLQZGRnyszZzbGP37t00TcufGGKGM3ny5EF1DjBeqe2MNLbmn/5xOJzVq1dfuXKFvZOmoKCAoijm9uf+a1lqNXchCwQYhO7u7levXnV1dd29ezc8PFwkEgUEBDBVNjY2DQ0NOTk5nZ2ddXV18j8HCSHGxsY1NTWVlZXNzc2dnZ0FBQXKXCmGz+dbWVlVV1cP2JI5LyN/dTOXy42MjLxw4cLp06ebmpru3bsXHBxsbm4eGBioSG+bN28+c+aMWCxuamqSyWTV1dXPnj0jhPj5+U2ePHk4T4hihmNvbz/kHgDGujExI42z+Wfv3r0vXrzYv39/a2vr1atXjx49GhAQYGdnp0gtQ73mLhpomqZpQkhmZqaqo4ChYxaCGtRbPvvsM+YyDj6f7+npmZaWxly0O2PGjIqKihMnTgiFQkLI1KlTHzx4QNN0YGCgjo6OhYUFh8MRCoVr1qypqKhge3v58uXy5cu5XO706dO3b9++c+dOQoiNjc3jx49pmr5169bUqVN5PN6SJUueP3+en58vEAji4+OHMFJFPquBgYEWFhbyJaGhoTo6OhKJhHl54cIF5pa9SZMm/elPf+rx9p07d3p5ebEvu7u7jx49OmPGDB0dHSMjo7Vr15aWljJVA+609vb2qKgokUjE4XBMTEy8vb1LSkpoml67di0hZN++fQOOV/5YoDx3d3cLC4vu7m62JCwsbOLEiQN2SI/Z77uPj4+Pj4+qowCFDOEzNhZnJAU/k2Nr/rl69eo777zDXsxnZmbm7Oz8448/sg1+/PHHt99+W09Pz9zcfOfOnW1tbfJv77+W7mvu6pNyvu/IAv9lMfs+jAAAIABJREFUjP5XANYQssDBCgwMNDY2HtVNKGJoWWBZWRmHw/nyyy9HM7RBkMlkLi4u6enpQ3t7fX09l8v99NNP5QuRBYL6UMJnTB1mJAU/k+Ns/hmOPueuPinn+44zwgCDMOAFzupDKpV+++23ZWVlzJXINjY2cXFxcXFxLS0tqg6NyGSynJyc5uZmPz+/ofUQGxs7f/780NBQQghN0zU1NUVFRcyF2ACaY6zMSONs/hkO+blLHSALHIT8/HwDA4Ovv/5aJVu/du3arFmzmBsnJ0+eHB8fr7RNZ2dnW1lZMTdsmpmZ+fv7K23TMGQNDQ0rV660tbXdsmULUxIdHb1+/Xo/Pz+VP7i9sLAwOzu7oKCg/yXE3iQpKam4uDg/P19HR4cQkpuba2Fh4eLikpeXN9KRjnkffvihQCCgKGpkr5Efvu7u7uTkZGdnZ8XfIj8RMXR1dU1NTZctW3b06NFXr16NXrQwfONm/hmOHnOXWhjtg41jBVHg6P0333wjFAovXryonJD69J//+Z+EkFevXil/09bW1gYGBsrfroJG+4xwdHQ0sx7ptGnTsrKyRm9DA1Lks9qPb7/9NioqagTjUbKcnJyEhISurq7hdDLMfagqQztDxDxW9fbt26MR0tA8ePDgnXfeIYTMmzdvsO9lJyLmxogffvghICCAoihzc/MbN26MQrBDNNqfMTWZkQb7mRzr889wDHbuUs4ZYY6qss+xyN3dXeU/YpRGKpW6urrKP9RSwyUkJCQkJKg6ihHg5ubm5uam6iiGzsvLy8vLS9VRwNDduXMnLi4uODi4tbWVpukh90NRlKGh4bJly5YtW+bu7u7r6+vu7v7gwQMDA4MRjFZtjdEZaazPP8OhnnMXzghD39LT02tra1UdBQAMF0VRqg7h38ybNy87O3vjxo39P0xiUHx8fAICAmpra48fPz5SfQJoAmSBiioqKhKJRBRFHTt2jBAiFov19fX5fH5ubu6qVauEQqGlpSVz5kURf//732fPnm1gYMDlcu3t7b/99lum/NKlS4ov2tR/DKmpqVwu19TUNCgoyNzcnMvlOjs7s89eDA0N1dXVZVYlIIRs27ZNX1+foqj6+npCSHh4eGRkZEVFBUVRNjY2wxnUhx9+yFzBY21tzawFunnzZj6fb2BgcPHiRUKITCbbt2+fSCTi8XgODg7Mid0jR47w+XyBQFBbWxsZGWlhYVFaWqpgGAAajqbpo0eP2tnZ6enpGRgYMEuEsPr8xg04oTGLX/D5fKFQaG9vzzwjq8+uhmlQc6A8Zp28goKCMTFMAHUx2qecxwqiwDUcT548IYR89tlnzMvdu3cTQr7//vvGxsba2loXFxd9ff2Ojg5FNpeVlRUbG9vQ0PDy5UtHR0d2eYtvvvlGIBDExcW96Y09rgvsP4bAwEB9ff1ff/21ra2tpKRk8eLFAoGAWSyKpumNGzdOnjyZ7fno0aOEkLq6Oualt7e3tbW1/KYHvC7wTYPy9vbW1tZ++vQp23LDhg3s5ZUfffSRnp7e+fPnX716FRMTo6WlxVzcwwwtLCzss88+W7du3T//+c/+d6kSVopRE4p8VqF/Y3QfKnid0O7duymK+p//+Z9Xr15JJJK0tDQid11g/9+4PieTlpYWoVCYmJgolUqfP3++bt06ZqJ4U1cK+o//+I/e1wUOOAe+aSJiMrYpU6aoyTDH6GdssLB60ejBeoFKNeQsUCqVMi+Zqba8vHywm2au7aitrVWkcZ9Z4JtiCAwMlJ8ub9y4QQg5cOAA83LEs8A3Dery5cuEEHY90sbGxhkzZjBXyEqlUj6f7+fnx1RJJBI9Pb2QkJDeQxsQskBQ3Bjdh4r8V5BIJHw+f8WKFWyJ/N0hin/j5CeTX375hRDyzTffyG+on64U1GcWOKB+JiLmSsH+Y1PaMMfoZ2ywkAWOHtwdMsYwt2sN4bmNzB3jI7LsU/8xLFq0iM/n379/f/gbGpD8oN59911bW9tTp07FxMRQFHX27Fk/Pz/mGUGlpaUSiWTu3LnMu3g8npmZ2XAiXL9+/UiEr+6Sk5OzsrJUHQWoo/LycolE4urq2met4t84+cnEysrK1NTU398/LCwsICBg2rRpg+pKOZh7TZhHR6jJMDXhe3rt2jWiMROvkl27do19KvTowXWBqpGXl7ds2TITExM9Pb2PP/5YadvV09Njnsc1Gt40KIqigoKCHj58+P333xNC/vKXv/zxj39kqlpbWwkhe/bsYRcAq6qqkkgkoxQhwLjHPKLUxMSkz9qhfeN4PN7//u//Llmy5NChQ1ZWVn5+flKpVN2+vA8ePCCEzJw5k4zrYQKMLBwLVIHHjx+vXbt23bp1p06d+t3vfvfZZ58pJxHs7Ox8/fq1paXlCPZ55cqVn3/+OSIiov9BBQQExMTEnDx5csqUKUKhcOrUqUw5878qOTk5PDx8ROIZ97+8CSEURUVERLz//vuqDmQMU7fbZkcQl8slhLS3t/dZO+Rv3Jw5c77++uu6urqkpKTDhw/PmTOHee7CCH55h+nSpUuEkFWrVhG1GaYmfE+Zo4CaMPEqn3KOsOJYoArcu3evs7MzJCTEysqKy+Uq7R9SYWEhTdPsEWYOhzOE89c9/Pzzz/r6+mSgQRkZGfn6+ubk5Hz66adbt25ly6dMmcLlctXtqQYAY9fcuXO1tLR+/PHHPmuH9o2rqan59ddfCSEmJiaffPLJwoULf/31V7X68j5//jw5OdnS0pJ5Us54HSbAiEMWqAIikYgQcvny5ba2trKyMnb1FkJIQUHB0FZJeBNmef2urq67d++Gh4eLRCJmPQVCiI2NTUNDQ05OTmdnZ11dXVVVlfwbjY2Na2pqKisrm5ub+0wWOzs7X7x4UVhYyGSB/QyKERwc3N7e/s0333h4eLCFXC538+bNZ86cEYvFTU1NMpmsurr62bNnIzV8AE1jYmLi7e19/vz59PT0pqamu3fvnjhxgq0d2jeupqYmKCjo/v37HR0dt2/frqqqcnR0HKUvryJzIE3TLS0t3d3dNE3X1dVlZma+88472traOTk5zHWB6j9MAHUx2refjBVkoPu5PvvsM2Z1PT6f7+npmZaWxjyCcMaMGRUVFSdOnGBmn6lTpz548GDAzUVFRRkbGxsaGq5fv55ZgNDa2vrx48f5+fkCgYC9nVbetWvX5syZo6WlRQgxMzM7dOjQgDEEBgbq6OhYWFhwOByhULhmzZqKigq2w5cvXy5fvpzL5U6fPn379u3MomI2NjbMUjK3bt2aOnUqj8dbsmTJn//8Z2tr6zd9hC5cuND/oNgtLliwIDo6use42tvbo6KiRCIRh8Nh/oGVlJQkJibyeDxC/o+9O49r6sr/x38uJJAEgoCAUBBkc19wrURxGRSrFhRRxK2DrR20VsSFQURQEZcOfQCllbGOls5HOwqKI25oix11qNRvLaIUKyKKiKggyJ6w5f7+uDP3lwEMYctCXs8/+mjuvTn3fQ/J23fucg4ZOHDgsWPHOuxPGs8IQ2doaB8q+MxgTU3NmjVr+vfvb2hoOHXq1IiICEKIjY3N3bt36bd84+Qnk8LCQpFIZGJioqur+84774SFhTHP+LfbVIfhZWZmTpkyxcrKikkglpaWIpHo+vXrzFo5OfDcuXOjR48WCAR6enpMJmQeCp40aVJkZGR5ebnsxio/TA39jHUWnhHuPcrpW4ruxgQ+fQlFUUlJSX3sHo61a9eeOnWqvLxc1YH8x/z587/66it7e/veaDw5OXnp0qXa8Hnuk59VJdPQPsQ9WBpEQz9jnYXPZO9RTt/iinAf1yMD0HQHezX53r17zHlH1cYDAAAADFSBPe/BgwfU2zFPnGmPkJCQ/Pz8hw8frl69es+ePaoOR1usXbuW/citXLlSdlV6enpoaGhKSoqDgwOzwapVq2Q38PDwEAqFurq6I0aMyMrKUm7g7ZBIJEOHDt2xYwfz8ty5c5999pnsz5uzZ8+yB2tmZqaiMIEQZD/oiAblH6lUGhsbKxKJ2q7KyMiYMmWKQCCwsrIKCQlp9Uj+29a2zV1qobcvOWsK0ufu4QgNDWUGRB00aNCpU6dUFUZYWJiOjs7AgQPZKeN6Ce4LlBUQEGBqapqWlpaXlyeRSNjlERERnp6e1dXVzEtHR8f+/fuTNvMlpKWlLViwoMcj75rNmzcTQsLCwtglcXFx06dPZ2fQkUqlxcXFN27cmDdvHjtvoXwa+n3HPVgaREM/Y53Vqc+kBuWfhw8fTpkyhRDSdnqb3377jc/nh4eH19bW3rx508zMbPXq1QqubZW75FPO9x3nAvusffv2NTQ00DT95MmTxYsXqyqMqKiolpaWoqIi2UeDNZRYLG73d6Fqm3obPp//3nvvDR48WF9fn1ly4MCBkydPJicnC4VCdrP4+HgdHZ2AgICqqqpejadrbt68yczrJWvjxo1jxoyZN29ec3MzIYSiKGtrazc3N2dnZ1XECKACmpWOiEbln7t3727btm3dunUuLi5t1+7Zs8fS0nL37t0GBgaurq4hISHffvstO52M/LWtcpc6QBUIoKijR4+WlpaqW1MKevToUXh4+O7du5lRhVkikSgoKOj58+dbt25VZjyKEIvFwcHBcXFxbVft2rUrOzu73VUA2kCz0pFm5Z8xY8akpKSsWLGC/QnNam5uvnjx4vTp09kxcefOnUvTdGpqaodrGeqWu1AFgnahaTomJmbYsGH6+vomJiYLFy5kf6UFBgbq6ekx4wERQtavX29gYEBR1OvXrwkhQUFBW7ZsKSgooCjKyckpPj6ex+NZWFisXbvWysqKx+OJRCJ2lMRONUUIuXz5cs+OE9lWfHw8TdNeXl5tV0VFRQ0ePPjIkSPp6entvldOpyUkJBgYGAgEgtTU1Llz5xoZGdnY2Jw4cYJ9b0tLS0REhK2tLZ/PHz16NHPhXkFhYWHr169vdzI0ExOT6dOnx8XF0VrwVDj0VdqTjjQx/7Tr8ePHtbW1zPi4DGYYtXv37nW4lqF2uau3LzlrCqId93D0YQreFxgREaGnp3fs2LHKysp79+6NGzfOzMzs5cuXzNoVK1YMGDCA3Tg6OpoQUlZWxrz08fFxdHRk1wYEBBgYGNy/f18ikeTm5k6cOFEoFLLjI3aqqQsXLgiFwsjISEWOVJHPakBAgLW1tewSBweH4cOHt9rM0dHxyZMnNE3fvHlTR0dn0KBBtbW1dJv7cuR3WlhYGCHk6tWrVVVVpaWlbm5uBgYGjY2NzNqtW7fq6+ufPn36zZs327dv19HR+eWXXxQ5zIyMDC8vL5qmmZmvZe8LZISGhhJC7ty5wy7ZuHEj7gsENaHIZ6wPpCMFP5Mal38Y7777bqv7AplZeaKjo2UX8vl8d3f3Dtey2uauduG+QIAeJhaLY2JiFi1atHLlyn79+o0aNerQoUOvX7+WnVyhUzgcDvMLdfjw4QkJCTU1NYmJiV1oZ/78+dXV1eHh4V0Lo0N1dXVPnjyRM/S3q6vrpk2bCgsLt23b1mqVgp0mEomMjIzMzc39/Pzq6uqKiooIIRKJJCEhwdvb28fHx9jYeMeOHVwuV5EuEovFQUFBCQkJcrZh7gLMycnpsDUANaQ96Ujj8o8czAO/urq6sgu5XK5YLO5wLUutcheqQNAiubm5tbW1EyZMYJdMnDhRT0+v7Xx3XTBhwgSBQMBeqlArpaWlNE0zsya8TVRU1JAhQw4ePJiRkSG7vLOdxjyZzowTmZeXV19fP3LkSGYVn8+3tLRUpIu2b9/+pz/9ydraWs42zOG8evWqw9YA1JD2pCONyz9yMPc1tnq2o7GxkZnsSv5allrlLlSBoEUqKysJIYaGhrILjY2Na2pqeqR9fX195vKlupFIJISQtnc6y+LxeImJiRRFffjhh7K/XLvTaXV1dYSQHTt2sCPGPX36tL6+Xv67MjIycnJy1qxZI38zJrEyhwagcbQnHWlW/pGPub2yurqaXVJfXy+RSJgZEeWvZalV7kIVCFrE2NiYENIqfVRWVtrY2HS/8aampp5qqscxSafD0UpdXV03b96cn58vO753dzqNebAjNjZW9jaUzMxM+e86evTo1atXdXR0mMTNNLJ3716Kom7fvs1u1tjYyB4agMbRnnSkWflHPnt7e6FQ+PTpU3bJo0ePCCGjR4/ucC1LrXIXqkDQIiNHjjQ0NJStJG7dutXY2Dh+/HjmJYfDYae866xr167RND158uTuN9XjLCwsKIpSZESuPXv2DB069M6dO+ySDjtNjoEDB/J4vOzs7E5Fm5iYKJu1ZZ8Okb0wxBzOgAEDOtU4gJrQnnSkWflHPg6HM2/evBs3bkilUmZJWloaRVHM48/y17LUKnehCgQtwuPxtmzZcubMmePHj1dXV+fk5Kxbt87KyiogIIDZwMnJqaKi4uzZs01NTWVlZbI/6QghpqamJSUlhYWFNTU1TEqVSqVv3rxpbm6+d+9eUFCQra2tv79/F5pKS0vr1ZFiBAKBg4NDcXFxh1sy12Vk727usNPkt7Z69eoTJ04kJCRUV1e3tLQUFxe/ePGCEOLn5zdgwIDuzBDFHM6oUaO63AKACmlPOupj+Sc8PPzVq1c7d+6sq6vLzMyMjo729/cfMmSIImsZ6pW7evcRZM1BNHPkCGApOFKMVCqNjo52dnbmcrkmJibe3t55eXns2vLy8pkzZ/J4PHt7+w0bNgQHBxNCnJycmAEXsrKy7Ozs+Hz+1KlTX758GRAQwOVyra2tORyOkZHRwoULCwoKutbUpUuXhEJhVFSUIkeqyGe17UgxgYGBXC63vr6eeXnmzBnmkT0zM7NPP/201duDg4NlR2qQ02kHDx5k7nR2dnYuKCg4fPiwkZERIcTOzu7hw4c0TTc0NISEhNja2nI4HHNzcx8fn9zcXJqmvb29CSEREREdHu/bRoqZP3++tbW1VCpll2CkGFAfinzG+kA6UvAzqVn5JzMzc8qUKezNfJaWliKR6Pr16+wG169fnzRpkr6+vpWVVXBwsOwsnR2updvLXe1SzvcdVeB/aOi/CsBS/jzCzFy9ytwjo2tVYH5+PofDOXbsWG+G1gktLS1ubm5Hjx7t2ttfv37N4/E+//xz2YWoAkF9KPkzpqp0pOBnso/ln+5oN3e1C+MFAqi7Du93ViGxWHzlypX8/HzmTmQnJ6fIyMjIyMja2lpVh0ZaWlrOnj1bU1Pj5+fXtRZ27drl4uISGBhICKFpuqSkJCMjg7kRG0A7qXM66mP5pztkc5c6QBUI0DdVVFS89957gwcP/vDDD5kloaGhS5Ys8fPzU/nE7deuXUtJSUlLS5M/hNjbxMTEZGdnX7p0icvlEkJSU1Otra3d3NwuXrzY05ECQM/oM/mnO1rlLnWAKhCgK7Zv356YmFhVVWVvb3/69GlVh9PaoUOH2BP+x48fZ5fv3bs3MDBw//79KoyNEOLu7v7dd9+xE5t2SmpqakNDw7Vr10xMTJglCxcuZA+WmRoVQKuoeTpi9YH80x1tc5c64Kg6AACNtG/fvn379qk6iq7w8PDw8PBQdRRdt2DBggULFqg6CgA1okHpSNPzT3eoZ+7CuUAAAAAAbYQqEAAAAEAboQoEAAAA0EaoAgEAAAC0EapAAAAAAK3U28NSawpV/x0AQKk0dO4QVXcbACiPEuYOoWgUQIQQQpKTk1UdAqjA0qVLg4KCXF1dVR0IKJtIJLKxsVF1FJ2TmZn57NkzVUfRjpaWlocPH969e/fu3btPnjzR1dUdMmSIi4uLl5eXqkPr+6qqqtauXbtx48bJkyerOhboYQMHDuztf55QBYJWoygqKSnJ19dX1YEAaJ4nT5788MMP6enpP/zwQ2VlpYODw6xZs2bNmuXh4dGvXz9VR6dF3N3d+/fvj3MZ0AUYNRoAABQlFot/+umn9PT09PT0X3/9VSAQiESibdu2eXp6Dh8+XNXRaaklS5Zs2bKltrbW0NBQ1bGAhkEVCAAAHXj8+HF6evr58+fT09MlEomDg8P7779/4MABNzc3fX19VUen7RYvXrxhw4YLFy74+fmpOhbQMKgCAQCgHeXl5T/++GN6enpaWtqzZ8/MzMxmzpz5xRdfzJs3T+NuqezbmD/NqVOnUAVCZ6EKBACA/2hpacnOzmYu+F67do2maRcXl+XLl7///vsikUhHB4OLqSlfX98NGzZUV1cbGRmpOhbQJKgCAQC03atXr65cuXLhwoX09PQ3b95YWlrOnj37H//4x+zZs42NjVUdHXTMx8dn/fr158+fX7FihapjAU2CKhAAQBvJPueRlZXF4/GmTJkSEhIya9as8ePHqzo66BwTE5M//OEPp06dQhUInYIqEABAizDPeaSnp1++fLmmpoYZ3mXnzp2zZ8/m8Xiqjg66ztfXd926dVVVVRimBxSHmzwAAPq42tra8+fPBwQEDBo0yNHRcfv27YSQzz//vKioqKCg4Ouvv/b09EQJqOm8vb1pmk5NTVV1IKBJcC4QAKAPkkqld+7cYU77Xb9+XSqVuri4+Pn5zZo1a/r06VwuV9UBQg8zNjaePXt2cnLyBx98oOpYQGOgCgQA6DtKS0uvX7/OjO334sWLAQMGTJs27ciRI56eniYmJqqODnqXr6/vmjVrKioqTE1NVR0LaAZUgQAAmq25ufnnn39mnvDNysrS19efOnXqxo0bZ82aNW7cOIqiVB0gKMnChQsDAgJSU1NXr16t6lhAM6AKBADQSOxzHleuXKmurmae8wgJCXnvvfeEQqGqowMVMDIy8vDwSE5ORhUICkIVCACgMerq6jIzM8+fP3/u3LnCwkJDQ8MZM2ZER0fPmTPHzs5O1dGB6vn6+vr7+5eVlZmbm6s6FtAAqAIBANRdbm4uc8H3xo0bzc3NY8eOXbp0KZ7zgLa8vLy4XG5qauqaNWtUHQtoAFSBAADqqKys7Nq1a+np6RcuXCgpKbGwsJg+ffqXX37p6elpZWWl6uhATQmFwvfeey85ORlVICgCVSAAgLpgpvE9f/78hQsX7ty5o6Oj8+677wYGBuI5D1Ccr6/vypUrS0tLLSwsVB0LqDtUgQAAKsY+5/H9999XVVWxz3nMmTPHyMhI1dGBhvHy8uLxeP/85z8DAgJUHQuoO1SBAAAqUF9ff/PmTab4+/XXXw0MDFxdXUNDQ728vIYNG6bq6ECDCQSCefPmJScnowqEDqEKBABQnsePHzMXfP/97383NDQMHz7c09PzwIED06ZN09PTU3V00EcsWbLEz8+vpKTknXfeUXUsoNZQBQIA9K7Xr1//61//Sk9Pv3TpUnFxsbm5+YwZM+Lj499//338Iw29Yf78+QKB4J///Of69etVHQuoNVSBAAA9j3nOg5nJLTMzk6IoFxeXjz76yNPTc+zYsTo6OqoOEPoygUDw/vvvnzp1ClUgyIcqEACgx7x8+fL777+/cOHCDz/8UFlZyTznsXHjRg8Pj379+qk6OtAivr6+ixcvfv78ubW1tapjAfWFKhAAoFvEYvFPP/3EPuchEAhEItG2bdtmzZo1fvx4VUcHWmru3LmGhoYpKSmBgYGqjgXUF6pAAICuYIZ3OX/+fHp6ukQicXBweP/99w8cOODm5qavr6/q6EDb8Xg8T0/PU6dOoQoEOVAFAgAoqqKi4urVq+np6Wlpac+ePevfv/8f/vCHL774Yt68eTY2NqqODuB/+Pr6Lly4sKioyNbWVtWxgJpCFQgAIA/7nEd6evr169elUqmLi8vy5ctnzZo1Y8YMDgdZFNQUM+p4SkrKpk2bVB0LqCnkLwCAdrx69erGjRvnz5+/ePFiRUWFpaXl7Nmzv/vuu1mzZpmYmKg6OoCO6evrL1iwIDk5GVUgvA2qQACA/5BIJBkZGcxpv6ysLB6PN2XKlD//+c94zgM0lK+vr6enZ2Fh4aBBg1QdC6gjVIEAoO3YaXwvX75cU1PDDO+yc+fO2bNn83g8VUcH0HUeHh4mJianT5/eunWrqmMBdYQqEAC0UV1dXWZm5vnz51NTU58+fWpoaDhjxozPP//8vffew6300GdwuVzmojCqQGgXqkAA0BZSqfTOnTvMab8bN260tLS4uLj4+fnNmjVr+vTpXC5X1QEC9DxfX9/ExMSCggJHR0dVxwJqB1UgAPRxpaWl169fZ8b2e/HixYABA6ZNm/a3v/3t/fffNzU1VXV0AL1r1qxZ5ubmp0+fDgkJYZZUVVX9/vvvkydPVm1goA5QBYJ2OXHiRE1NjeyS9PT0yspK9qW3t7e5ubnS44Ie1tzc/PPPP1+4cIF5zkNfX3/q1KkbN26cNWvWuHHjKIpSdYAASsLhcJiLwp988sm5c+dOnDjx/fffz507NzU1VdWhgeqhCgTtcuXKlb///e/stT8Oh5OYmPjtt98SQlpaWgwNDf/4xz+qMj7oHvY5jytXrlRXVzPPeYSEhLz33ntCoVDV0QGoQG1trYWFRU5OTv/+/VtaWnR0dJqbmzHOJTDwOQDtsmzZsr///e9NTU1tV3G53CVLlmDuL43DPOeRnp5+7ty533//3cDAwNXVNTIycuHChXZ2dqqODkA1JBLJDz/8cPLkyTNnzjQ0NOjo6LS0tBBCpFIpIQRVIDDwOQDt4u7ubmpqWlFR0XZVU1PT8uXLlR8SdE1ubi5zwffGjRvNzc1jx4718vKKj4+fNm2anp6eqqMDULH169d/8803urq6TPHH/JdBUZSOjo7qQgM1gioQtAuHw1m2bNnhw4fbng40MzObPn26SqICBZWVlV27di09Pf3ixYvPnz+3sLCYPn36l19+6enpaWVlperoANTIF198kZGR8fjx47arKIrS1dVVfkighlAFgtZZtmzZwYMHWy3kcrmrVq1CZuxVYrFYT0+vs53MTuN7/vz5zMxMHR2dMWPGrFmzxtPTE895ALyNoaHhhQsXxo4dK5Wpi2XqAAAgAElEQVRKmavALIqicEUYGDgnDFpHJBLZ2Ni0WtjU1LRs2TKVxKMl7ty5M378+F9++UXB7Z88eXL48GFfX9/+/ftPmDDh8OHDI0aMOHnyZHl5+e3bt3ft2jV+/HiUgAByODs7/+Mf/6Bpuu0qXBEGBj4HoHUoilq5cmWrIYIHDhw4YcIEVYXUt7W0tOzfv3/SpEm///77lStX5GxZX1+fnp6+bdu2CRMmODg4bNq06c2bN6Ghoffv3y8oKPj666+XLFliZGSktMgBNJ2Xl1dwcHCrE/C4IgwsVIGgjZYtWyZ7XyCXy/X398eJpd5QWFg4bdq08PDw5uZmQsjFixfbbvP48eMvvvhi9uzZpqams2fPPn/+/KxZs3744YeKiooffvghJCRk2LBhSg8coI/Yt29fq6lxUAUCC3cGgDYaPXr0kCFD8vLymJdNTU1Lly5VbUh90v/93/+tW7euqamJfT7x119/raioMDU1LS8v//HHH9PT0y9dulRcXGxmZjZz5sz4+Pj58+dbW1urNmyAvkRXV/fEiROjR49+/fo1+01EFQgMVIGgpVatWrV7927mjODw4cNHjBih6oj6lLKyso8++uj8+fMURcnelkTT9Nq1awsLC3/99VddXV2RSPTJJ5/MmTPHxcUFNyoB9BILC4vU1FQ3NzemCsS5QGAh7YKWWrlyJXONksvlYr6QnnX58uXhw4dfvnyZENLqznQOh3P79u3x48enpKSUl5dfu3YtNDR03LhxKAEBetW7774bGxvL3veCKhAYyLygpezs7MaNG0cIaW5u9vPzU3U4fUR9fX1gYOC8efMqKiranaClqampvr7+r3/968KFCzGlG4AyrV+/ftmyZRwORyqVogoEBqpA0F4ffPABIeTdd9+1tbVVdSx9wa1bt0aMGPHXv/6VpulW45PJevXq1f3795UZGAAwjhw5MnTo0ObmZlSBwPif+wIzMzNjYmJUFQqAkkkkEoqiGhoalixZoupYNJtUKv39998fPHjQ7shkrejo6Cxbtmzw4MFKCAzacnV13bx5c/fbwbdGQ9nY2Dx48ODcuXPtTisCfV6rDPA/5wKfPXt2+vRppYcE8B+nT58uLi5W2u54PN6AAQPajiDd237++eeff/5ZyTvtVa9evWpubrazs3vnnXf69+8vFAp5PF67Jxt0dHRomn758qXygwRCyM8//5yZmdkjTSn52wqkh/rc0NDw3XffVeeBsfpehlQfbTNAO88Inzp1SlnxAPwPiqI2bdrk6+urtD0+evTIyclJabtjMCdRtOGL1tLS8ubNmzdv3lRWVsr+j0Qi2bVrl6qj00Y9ewJPyd9W6MEM+ezZs4EDB3a/nd6gPRlS+dpmAIwUA1pN+SWgVtHV1TUzMzMzM1N1IADwP9S2BAQlw9MhAAAAANoIVSAAAACANkIVCAAAAKCNUAUCAAAAaCNUgaDxLl261K9fv/Pnz6s6kB62du1a6r9Wrlwpuyo9PT00NDQlJcXBwYHZYNWqVbIbeHh4CIVCXV3dESNGZGVlKTfw/yGVSmNjY0UiUdtVGRkZU6ZMEQgEVlZWISEhDQ0Niqw9d+7cZ599xkyH2lka1G8MiUQydOjQHTt2MC/bHvvZs2fZDwmewoF29dUMydKg77Va5cP/oGUkJSW1WgKgTISQpKSkzr7rwoULRkZG586d642QesPixYsXL17c4WYBAQGmpqZpaWl5eXkSiYRdHhER4enpWV1dzbx0dHTs378/IeTChQuyb09LS1uwYEHPRt5ZDx8+nDJlCiFkzJgxrVb99ttvfD4/PDy8trb25s2bZmZmq1evVnBtXFzc9OnT37x506lgNKjfWMzgrmFhYeySVsculUqLi4tv3Lgxb968/v37d9iggp89RXTt2wrdgQzZlgZ9r9UhH7btW1SBoEbU/N+V+vp6V1fX7rejeBVobW3dauH+/fsHDx4sFovZJY6Ojt99952Ojo61tXVlZSW7XOVZLzs7e9GiRcePH3dxcWmb9ZYuXWpvby+VSpmX0dHRFEX9/vvviqylaTowMNDV1bWpqUnBYDSo31g//fSTh4dHqyqQfsuxb9y4EVVgn6fmfa7kDElr1PdaTfJh277FFWEARR09erS0tFSFATx69Cg8PHz37t08Hk92uUgkCgoKev78+datW1UVW1tjxoxJSUlZsWKFvr5+q1XNzc0XL16cPn06O4HB3LlzaZpOTU3tcC1j165d2dnZcXFxikSiWf3GEIvFwcHB7R5gp44dQGmUnCE163utPvmwFVSBoNkyMjJsbW0pivrqq68IIQkJCQYGBgKBIDU1de7cuUZGRjY2NidOnGA2jo+P5/F4FhYWa9eutbKy4vF4IpHo1q1bzNrAwEA9PT1LS0vm5fr16w0MDCiKev36NSEkKChoy5YtBQUFFEUxY01fvnzZyMho7969SjvY+Ph4mqa9vLzaroqKiho8ePCRI0fS09PbfS9N0zExMcOGDdPX1zcxMVm4cOGDBw+YVfI7jRDS0tISERFha2vL5/NHjx7NXDTojsePH9fW1tra2rJLHB0dCSH37t3rcC3DxMRk+vTpcXFxtAIzF2tiv4WFha1fv97c3Lztqk4dO2i5PpwhNfF73S4l58NWUAWCZps6derNmzfZl5988smmTZvEYrFQKExKSiooKHBwcPj444+bmpoIIYGBgf7+/vX19Rs3biwsLMzKympubp49e/azZ88IIfHx8bJTMx08eHD37t3sy7i4OE9PT0dHR5qmHz16RAhhbsiVSqVKO9iLFy8OGTJEIBC0XcXn87/99lsdHZ2PP/64rq6u7Qa7du0KDQ0NCwsrLS29cePGs2fP3NzcXr16RTrqNELItm3b/vKXv8TGxr548cLT03P58uW3b9/uzoEw8wgLhUJ2CY/H4/P5TDzy17LGjh37/Pnzu3fvdrg7jeu3n376qaCgYPny5W/bQPFjBy3XhzOkxn2v30bJ+bAVVIHQN4lEIiMjI3Nzcz8/v7q6uqKiInYVh8NhfgIOHz48ISGhpqYmMTGxC7uYP39+dXV1eHh4z0UtT11d3ZMnT5hfge1ydXXdtGlTYWHhtm3bWq0Si8UxMTGLFi1auXJlv379Ro0adejQodevXx8+fFh2s3Y7TSKRJCQkeHt7+/j4GBsb79ixg8vldq3HWMwDbrq6urILuVyuWCzucC3L2dmZEJKTkyN/XxrXb2KxOCgoKCEhQc42Ch47wNtoeobUuO+1HMrMh22hCoQ+Tk9PjxDC/oxrZcKECQKBgL0WoM5KS0tpmm73hy8rKipqyJAhBw8ezMjIkF2em5tbW1s7YcIEdsnEiRP19PTYaz2tyHZaXl5efX39yJEjmVV8Pt/S0rKbPcbcx9Pc3Cy7sLGxkc/nd7iWxXRFqx/EbWlcv23fvv1Pf/qTtbW1nG0UPHaADmlohtS477UcysyHbaEKBG2nr69fVlam6ig6JpFICCFt7yyWxePxEhMTKYr68MMPZX8pVlZWEkIMDQ1lNzY2Nq6pqelwv8z1lB07drDj0j19+rS+vr5rR8Fgbi2qrq5ml9TX10skEisrqw7XspgkyHSLHJrVbxkZGTk5OWvWrJG/mYLHDtB96pkhNet7LZ8y82FbqAJBqzU1NVVWVtrY2Kg6kI4xX/IORwd1dXXdvHlzfn7+nj172IXGxsaEkFY5TsEDZx5QiI2NlR1cIDMzswuHwLK3txcKhU+fPmWXMDcSjR49usO1rMbGRvLfbpFDs/rt6NGjV69e1dHRYf6BYRrZu3cvRVGy9x4peOwA3aS2GVKzvtfyKTMftoUqELTatWvXaJqePHky85LD4bztyojKWVhYUBRVVVXV4ZZ79uwZOnTonTt32CUjR440NDSULSNu3brV2Ng4fvz4DlsbOHAgj8fLzs7uWtjt4nA48+bNu3HjBnvneFpaGkVRzON+8teymK4YMGCA/H1pVr8lJibK/uvCnINhxguUvYCl4LEDdJPaZkjN+l7Lp8x82BaqQNA6Uqn0zZs3zc3N9+7dCwoKsrW19ff3Z1Y5OTlVVFScPXu2qamprKxM9ucXIcTU1LSkpKSwsLCmpqapqSktLU2ZI8UIBAIHB4fi4uIOt2Sug8jeTczj8bZs2XLmzJnjx49XV1fn5OSsW7fOysoqICBAkdZWr1594sSJhISE6urqlpaW4uLiFy9eEEL8/PwGDBjQtRmZwsPDX716tXPnzrq6uszMzOjoaH9//yFDhiiylsF0xahRo+RH0sf6jSF77AA9SyMyZB/7XvdsPuwc2d+dmDsEVIt0fmT8L7/8krltQiAQeHl5HTx4kLlJ1tnZuaCg4PDhw0ZGRoQQOzu7hw8f0jQdEBDA5XKtra05HI6RkdHChQsLCgrY1srLy2fOnMnj8ezt7Tds2BAcHEwIcXJyKioqomk6KyvLzs6Oz+dPnTr15cuXly5dEgqFUVFRnT3MLs8dEhgYyOVy6+vrmZdnzpxhHpEzMzP79NNPW709ODhYdqx8qVQaHR3t7OzM5XJNTEy8vb3z8vKYVR12WkNDQ0hIiK2tLYfDMTc39/Hxyc3NpWna29ubEBIREdFu/JmZmVOmTGFvXrG0tBSJRNevX2c3uH79+qRJk/T19a2srIKDg2VnyetwLU3T8+fPt7a2ZsbTlx+JZvWbLNlzgW87dgbmDtEGyJCyNOt7rcx8KAdmkAO1poR/V5jJeXt1Fx3qchWYn5/P4XCOHTvWa6F1TktLi5ub29GjR5W/69evX/N4vM8//1yRSPpYv7U6dgaqQG2ADCmrj32vu6PdnNAuzCAH0PENxepDLBZfuXIlPz+fufPXyckpMjIyMjKytrZW1aGRlpaWs2fP1tTU+Pn5KX/vu3btcnFxCQwMVCSSPtZvssdO03RJSUlGRgZzwzhA92lKhuxj3+vukM0JnYUqEEB9VVRUvPfee4MHD/7www+ZJaGhoUuWLPHz81Pktuhede3atZSUlLS0NPlDdvWGmJiY7OzsS5cucblcBSPpM/3W6thTU1Otra3d3NwuXrzY05ECqLs+873ujlY5obNQBfauvLy8DRs2jBgxQigUcjicfv36DR48eP78+d18sBy6Zvv27YmJiVVVVfb29qdPn1Z1OB04dOgQe9L++PHj7PK9e/cGBgbu379fhbERQtzd3b/77jt2UlGlSU1NbWhouHbtmomJSaci6QP91vbYFy5cKHtVqEcj1RgTJ07U1dV1cXHpwntTUlIcHBwoGXp6ehYWFjNmzIiOjn7z5k2PR6vONCtDMvrA97o72uaETpO9PIz7AnvWkSNHuFzutGnTLl++/ObNG4lEUlBQcPLkSZFI9PXXX6s6OnVEtONOox68NwugU/rqfYHu7u5jxozp8tsdHR379etH0zTzeOy//vUvf39/iqKsrKx++eWXnguzu9Sqz3sPMmTv6Tv3BYrFYpFIpM6N//zzzwEBAW5ublevXp0zZ46xsbG+vr6Dg8PSpUsjIiKY27yUTP07DQCA0dmUQlFU93dKUZSxsfGMGTMSExOTk5NfvXo1f/58lV9tBOg9mloFHj16tLS0VJ0bj4qKamlp2b9/P4fDabVqzpw5n376aTfb7wL17zQAAEZnU0rXboqSY/Hixf7+/qWlpYcOHerZlgHURxerwGPHjk2YMIHH4xkYGAwaNIiZm4Wm6ZiYmGHDhunr65uYmCxcuJCdYjkhIcHAwEAgEKSmps6dO9fIyMjGxubEiRMdtvnvf/97+PDh/fr14/F4o0aNunLlCiEkKChoy5YtBQUFFEU5OTkRQlpaWiIiImxtbfl8/ujRo5lL2x3utDuNE0IuX778tiExGxsbr1692r9//0mTJsnvSW3rNADQHnLyW2BgoJ6eHnsf1fr16w0MDCiKYu5ubJtSOvTo0aOhQ4caGBjw+Xw3N7eMjAx2lZxcLR8zWnJaWhrzsms5kxnpTSAQGBkZjRo1ipkQFhkS1IXs5WEF7wuMjY0lhOzfv7+8vLyiouLrr79esWIFTdMRERF6enrHjh2rrKy8d+/euHHjzMzMXr58ybwrLCyMEHL16tWqqqrS0lI3NzcDA4PGxkb5bZ46dWrXrl0VFRXl5eWTJ09mB8Ty8fFxdHRkQ9q6dau+vv7p06ffvHmzfft2HR0d5mYO+TvtZuMXLlwQCoWRkZFtu+jhw4eEkMmTJ3fYmdrWafIR3PUC0JuUfF+g/Py2YsWKAQMGsBtHR0cTQsrKypiXrVKKfO7u7g4ODk+ePGlqavrtt9/effddHo/HjPFLy83VDPa+wFaYim3gwIHMyy7kzNraWiMjo88++0wsFr98+XLRokXMASJDyoEM2Xt6YNToxsZGY2PjmTNnskuam5vj4uLq6+sNDQ39/PzY5f/v//0/Qgj7xWO+J2KxmHl58OBBQsijR4/ktNlq1/v27SOElJaW0v+bIMRisUAgYHddX1+vr6//ySefyN9p9xuXg5mgcNasWfI3Q6e1ghwH0KuUWQV2mN96tgqUfTrk3r17hJCtW7cq+Pa3VYE0TTN3CtJdzZm//fYbIeTChQuybSJDyocM2Xva9m3rW9Y6dO/evcrKyjlz5rBLdHV1N27cePv27draWtn5zidOnKinp3fr1q1229HT0yOEMPNSv63NVm9hbvtoO6BlXl5efX39yJEjmZd8Pt/S0pK97vC2nfZ447IMDQ0JIfX19fI3y83NRae1snTp0qVLlyqypabrkZvZATpr8eLFytlRZ/NbDxo1alS/fv2YWrA76urqaJpmJhDrWs50cHCwsLBYuXLlxo0b/f39Bw0a1Kmm2kKGhG5qlQE6XQUyZ8iNjY1bLa+srCT/rX5YxsbGNTU1XW6TEHLx4sXo6Ojc3Nzq6up2CxFCSF1dHSFkx44dO3bsYBeys/XJ0XuNDxo0iLkeIX8zdFpbQUFBrq6uimypuZhL+Zs2bVJ1IKB1mM+ecnQnv3Ufl8t9W4JSHJPDhw4dSrqa1vh8/o8//rht27a9e/dGRkb6+vomJiYiQ8qHDNl72maATleB77zzDiGk7fCkTDnS6utdWVlpY2PT5TaLioq8vb0XLVr0zTffvPPOO19++eWf//zntm83NzcnhMTGxgYFBSl+IL3auL6+/pw5c1JTU3/66acpU6a0WltRUfHnP//5yJEj6LS2XF1dfX19O/suzXLq1ClCSJ8/TFBDzGdPObqT37qpubm5oqLC1ta2m+1cvnyZEDJ37lzSjbQ2YsSI8+fPl5WVxcTEHDhwYMSIEcwkY8iQb4MM2XvaZoBOPyM8aNAgU1PT77//vtXykSNHGhoaMvfDMW7dutXY2Dh+/Pgut5mTk9PU1PTJJ584ODjweLy3nR8eOHAgj8fLzs7u1IH0auOEkF27dunr62/evFksFrda9dtvvzHDx6DTAKCv6jC/cTic7p+ua9e//vUvqVQ6bty47jTy8uXL2NhYGxsbZv7GrqW1kpKS+/fvE0LMzc33798/bty4+/fvI0OC+uh0Faivr799+/YbN24EBgY+f/5cKpXW1NTcv3+fx+Nt2bLlzJkzx48fr66uzsnJWbdunZWVVUBAQJfbZH7JpaenSySS/Px82btJTE1NS0pKCgsLa2pqdHV1V69efeLEiYSEhOrq6paWluLi4hcvXsjfafcbT0tLkzP6gIuLy3fffffbb7+5ubldunSpqqqqqanpyZMnf/vb3z766CPmjjot7DQA0BId5jcnJ6eKioqzZ882NTWVlZU9ffpU9u2yKUWRYrGxsbGqqqq5uTkrKyswMNDOzo4Z54V0lKsZNE3X1tZKpVKapsvKypKSkqZMmaKrq3v27FnmvkAej9eFtFZSUrJ27doHDx40NjbeuXPn6dOnkydP7lpTAL1C9lERxWeQ++qrr0aNGsXj8Xg83tixYw8ePEjTtFQqjY6OdnZ25nK5JiYm3t7eeXl5zPYHDx5kplh2dnYuKCg4fPgw872ys7NjH+Zvt82QkBBTU1NjY+MlS5Z89dVXhBBHR8eioqKsrCw7Ozs+nz916tSXL182NDSEhITY2tpyOBxzc3MfH5/c3NwOd9qdxmmavnTpklAojIqKktNRRUVFW7duHTVqlKGhoa6urrGx8dixYz/66KOffvqJ2UDbOk0+gifgAHqTkkeKkZPfaJouLy+fOXMmj8ezt7ffsGFDcHAwIcTJyamoqIim6VYpRf6OEhMTZ86caWFhweFw+vfvv2zZsqdPn7Jr5eTqc+fOjR49WiAQ6Onp6ejokP9OHzJp0qTIyMjy8nLZjbuQMwsLC0UikYmJia6u7jvvvBMWFtbc3Py2pjrqcmRI6K62fUvRNM1WhMnJyUuXLpVdAqBMFEUlJSX1+dtBlixZQpR7hxYAowc/e1rybVUrWtLnyJC9p23fauoMcgAAAADQHagCAaBz0tPTQ0NDU1JSHBwcKIqiKGrVqlWyG3h4eAiFQl1d3REjRmRlZakqTkKIVCqNjY0ViURtV2VkZEyZMkUgEFhZWYWEhDQ0NLTbgkQiGTp0KDuix7lz5z777LO2Y2RC73nw4AH1dszztgBvo+n5qrdzDqpAAOiEnTt3xsfHb9++3cfH5/Hjx46Ojv379z9+/PjFixfZbb7//vtTp055enrm5uZ28znN7sjPz582bdrmzZvbjt+em5vr4eHh7u5eVlZ25syZb775Zt26de02EhYWlpeXx7708vLi8Xju7u7MYHigBEOHDpVzn9PJkydVHSCorz6Qr3o756AKBC0iFovbPS2k2qY0yIEDB06ePJmcnCwUCtmF8fHxOjo6AQEBVVVVKoytlbt3727btm3dunUuLi5t1+7Zs8fS0nL37t0GBgaurq4hISHffvtt28kbbt68ycwAJmvjxo1jxoyZN29ec3Nzb0UPoAp9LEP2mXzVqzkHVSBokaNHj5aWlqpbU5ri0aNH4eHhu3fv5vF4sstFIlFQUNDz58+3bt2qqtjaGjNmTEpKyooVK/T19Vutam5uvnjx4vTp09kBL+fOnUvTdGpqquxmYrE4ODg4Li6ubeO7du3Kzs5udxWA5upLGbLP5CtG7+UcVIGgYWiajomJGTZsmL6+vomJycKFC9lTOIGBgXp6epaWlszL9evXGxgYUBTFzLASFBS0ZcuWgoICiqKcnJzi4+N5PJ6FhcXatWutrKx4PJ5IJGJHQOxUU4SQy5cvdzggmaaLj4+nadrLy6vtqqioqMGDBx85ciQ9Pb3d98r5qyUkJBgYGAgEgtTU1Llz5xoZGdnY2Jw4cYJ9b0tLS0REhK2tLZ/PHz16NDOgVXc8fvy4trZWdmIJR0dHQkiraWfDwsLWr1/PzBjRiomJyfTp0+Pi4jCiAqgbZEhGn8lXjF7MObI3WCg+XiBAbyAKjIYVERGhp6d37NixysrKe/fujRs3zszMjB1ObMWKFQMGDGA3jo6OJoSUlZUxL318fBwdHdm1AQEBBgYG9+/fl0gkubm5EydOFAqFzFhlnW3qwoULQqEwMjJSkcPU0NGwHBwchg8f3mqho6PjkydPaJq+efOmjo7OoEGDamtraZpOS0tbsGABu5n8v1pYWBgh5OrVq1VVVaWlpW5ubgYGBo2NjczarVu36uvrnz59+s2bN9u3b9fR0fnll18UD/vdd98dM2aM7JLr168TQqKjo2UX8vl8d3d39mVGRoaXlxdN02VlZYSQsLCwVs2GhoYSQu7cuaN4JOpAyeMFQs9ChlRcn8lXrB7JOW37FucCQZOIxeKYmJhFixatXLmyX79+o0aNOnTo0OvXrw8fPty1BjkcDvODb/jw4QkJCTU1NYmJiV1oZ/78+dXV1eHh4V0LQ/3V1dU9efKEOWfWLldX102bNhUWFm7btq3VKgX/aiKRyMjIyNzc3M/Pr66urqioiBAikUgSEhK8vb19fHyMjY137NjB5XK79jdiMY8D6+rqyi7kcrnsZI9isTgoKCghIUFOI87OzoSQnJyc7kQC0LOQIRl9KV+xeinnoAoETZKbm1tbWzthwgR2ycSJE/X09GTnsuuyCRMmCASCto8IACGktLSUpmlmjoS3iYqKGjJkyMGDBzMyMmSXd/avpqenRwhhJg3Ly8urr68fOXIks4rP51taWnbzb8TcJ9TqPuvGxkY+n8/8//bt2//0pz9ZW1vLaYTpilevXnUnEoCehQzJ6Ev5itVLOQdVIGgS5lF5Q0ND2YXGxsY1NTU90r6+vj5zBRBakUgkhJC33bnM4PF4iYmJFEV9+OGH7Hk10r2/Wl1dHSFkx44d7PhwT58+bTvyS6cwNzNVV1ezS+rr6yUSiZWVFSEkIyMjJydnzZo18hthSkamWwDUBDIkoy/lK1Yv5RxUgaBJjI2NCSGtvo2VlZU2Njbdb7ypqamnmup7mATU4cilrq6umzdvzs/P37NnD7uwO3815uGM2NhY2RtZMjMzu3AILHt7e6FQ+PTpU3bJo0ePCCGjR48mhBw9evTq1as6OjpMEmcC2Lt3L0VRt2/fZt/S2NhI/tstAGoCGZLRl/IVq5dyDqpA0CQjR440NDSU/cf41q1bjY2N48ePZ15yOBzmzHwXXLt2jabpyZMnd7+pvsfCwoKiKEVG2NqzZ8/QoUPv3LnDLunwrybHwIEDeTxednZ218JuF4fDmTdv3o0bN6RSKbMkLS2NoijmccLExETZDC77dIjsRSKmKwYMGNCDgQF0EzIkoy/lK1Yv5RxUgaBJeDzeli1bzpw5c/z48erq6pycnHXr1llZWQUEBDAbODk5VVRUnD17tqmpqaysTPZ8DyHE1NS0pKSksLCwpqaGyV9SqfTNmzfNzc337t0LCgqytbX19/fvQlNpaWl9e6QYgUDg4OBQXFzc4ZbMdRbZZy86/KvJb2316tUnTpxISEiorq5uaWkpLi5+8eIFIcTPz2/AgAFdm/EpPDz81atXO3furKury8zMjLODqWgAAB45SURBVI6O9vf3HzJkiOItMF0xatSoLuwdoJcgQzL6WL5i9FbOkf3Vi5FiQLWIAuMgSKXS6OhoZ2dnLpdrYmLi7e2dl5fHri0vL585cyaPx7O3t9+wYUNwcDAhxMnJiRndICsry87Ojs/nT5069eXLlwEBAVwu19ramsPhGBkZLVy4sKCgoGtNXbp0SSgURkVFKXKYGjpSTGBgIJfLra+vZ16eOXOGeQTPzMzs008/bbVxcHCw7MgLcv5qBw8eZO56dnZ2LigoOHz4sJGRESHEzs7u4cOHNE03NDSEhITY2tpyOBxzc3MfH5/c3Fyapr29vQkhERER7UabmZk5ZcoU5lY/QoilpaVIJLp+/Tq7wfXr1ydNmqSvr29lZRUcHCyRSNpt520jxcyfP9/a2loqlSregeoAI8VoNGRIxfWxfEX3UM5p27eoAkGNKPnflYCAAFNTU6XtjqWhVWB+fj6Hwzl27JiqA/mPlpYWNze3o0ePKn/Xr1+/5vF4n3/+ufJ33U2oAjUaMqTi+li+6qmcg/ECAf5Hh7cPA8vJySkyMjIyMrK2tlbVsZCWlpazZ8/W1NT4+fkpf++7du1ycXEJDAxU/q4BlElzM2Qfy1e9l3NQBQKAokJDQ5csWeLn56fyidivXbuWkpKSlpYmf0iw3hATE5OdnX3p0iUul6vkXQOA4vpMvurVnIMqELTU9u3bExMTq6qq7O3tT58+repwNMbevXsDAwP379+v2jDc3d2/++47dhpTpUlNTW1oaLh27ZqJiYmSdw2gTH0jQ/aBfNXbOYfTG40CqL99+/bt27dP1VFoJA8PDw8PD1VHoRoLFixYsGCBqqMA6HV9JkNqer7q7ZyDc4EAAAAA2ghVIAAAAIA2QhUIAAAAoI1QBQIAAABoo3aeDklOTlZ+HACMnpp4W50xEwHhiwbKV1xcbGNj01OtacO3Vd1oQ58jQ/aedjKA7BDSzNwhAADQV/Xg3CEAoHFaZQAKX2bQZhRFJSUl+fr6qjoQAAAAZcN9gQAAAADaCFUgAAAAgDZCFQgAAACgjVAFAgAAAGgjVIEAAAAA2ghVIAAAAIA2QhUIAAAAoI1QBQIAAABoI1SBAAAAANoIVSAAAACANkIVCAAAAKCNUAUCAAAAaCNUgQAAAADaCFUgAAAAgDZCFQgAAACgjVAFAgAAAGgjVIEAAAAA2ghVIAAAAIA2QhUIAAAAoI1QBQIAAABoI1SBAAAAANoIVSAAAACANkIVCAAAAKCNUAUCAAAAaCNUgQAAAADaCFUgAAAAgDZCFQgAAACgjVAFAgAAAGgjVIEAAAAA2ghVIAAAAIA2QhUIAAAAoI1QBQIAAABoI1SBAAAAANqIomla1TEAKE9AQEBeXh77Misry97e3sTEhHmpq6v797//3cbGRkXRAQAAKA9H1QEAKNWAAQMOHz4su+TevXvs/zs4OKAEBAAALYErwqBdli9f/rZVenp6/v7+SowFAABAlXBFGLTOyJEj79+/3+4nPy8vb/DgwcoPCQAAQPlwLhC0zgcffKCrq9tqIUVRY8aMQQkIAADaA1UgaJ1ly5a1tLS0Wqirq/vHP/5RJfEAAACoBK4IgzYSiUS3bt2SSqXsEoqinj17Zm1trcKoAAAAlAnnAkEbrVq1iqIo9qWOjs7UqVNRAgIAgFZBFQjaaMmSJbIvKYr64IMPVBUMAACASqAKBG1kZmbm7u7OPiNCUZS3t7dqQwIAAFAyVIGgpVauXMncFKurqztnzpz+/furOiIAAAClQhUIWmrRokV6enqEEJqmV65cqepwAAAAlA1VIGgpAwOD999/nxCip6fn6emp6nAAAACUDVUgaK8VK1YQQry9vQ0MDFQdCwAAgLJhvECFyI4qAgAaISkpydfXV9VRAACoL46qA9AYQUFBrq6uqo5CK2RmZsbFxSUlJSlhX8ePH/fz8+NwVPNFWLp0KT5XvWTp0qWqDgEAQN3hXKBCKIrCeQWlSU5OXrp0qXI+mRKJhMfjKWFH7cLnqvegbwEAOoT7AkGrqbAEBAAAUC1UgQAAAADaCFUgAAAAgDZCFQgAAACgjVAFAgAAAGgjVIHQR1y6dKlfv37nz59XdSC9JT09PTQ0NCUlxcHBgaIoiqJWrVolu4GHh4dQKNTV1R0xYkRWVpaq4iSESKXS2NhYkUjUdlVGRsaUKVMEAoGVlVVISEhDQ0O7LUgkkqFDh+7YsYN5ee7cuc8++6ylpaUXgwYA0D6oAqGP6NtjHu3cuTM+Pn779u0+Pj6PHz92dHTs37//8ePHL168yG7z/fffnzp1ytPTMzc3d9y4caoKNT8/f9q0aZs3b66vr2+1Kjc318PDw93dvays7MyZM9988826devabSQsLCwvL4996eXlxePx3N3dKysrezF0AAAtgyoQ+oj58+dXVVUpYUZgsVjc7lmu3nPgwIGTJ08mJycLhUJ2YXx8vI6OTkBAQFVVlTKDke/u3bvbtm1bt26di4tL27V79uyxtLTcvXu3gYGBq6trSEjIt99+++DBg1ab3bx587fffmu1cOPGjWPGjJk3b15zc3NvRQ8AoGVQBQJ0ztGjR0tLS5W2u0ePHoWHh+/evbvV0IYikSgoKOj58+dbt25VWjAdGjNmTEpKyooVK/T19Vutam5uvnjx4vTp09n5GOfOnUvTdGpqquxmYrE4ODg4Li6ubeO7du3Kzs5udxUAAHQBqkDoCzIyMmxtbSmK+uqrrwghCQkJBgYGAoEgNTV17ty5RkZGNjY2J06cYDaOj4/n8XgWFhZr1661srLi8XgikejWrVvM2sDAQD09PUtLS+bl+vXrDQwMKIp6/fo1ISQoKGjLli0FBQUURTk5ORFCLl++bGRktHfv3l46tPj4eJqmvby82q6KiooaPHjwkSNH0tPT230vTdMxMTHDhg3T19c3MTFZuHAhe+JNfhcRQlpaWiIiImxtbfl8/ujRo7s/od/jx49ra2ttbW3ZJY6OjoSQe/fuyW4WFha2fv16c3Pzti2YmJhMnz49Li6ub1/9BwBQGlSB0BdMnTr15s2b7MtPPvlk06ZNYrFYKBQmJSUVFBQ4ODh8/PHHTU1NhJDAwEB/f//6+vqNGzcWFhZmZWU1NzfPnj372bNnhJD4+HjZaccOHjy4e/du9mVcXJynp6ejoyNN048ePSKEMI8sSKXSXjq0ixcvDhkyRCAQtF3F5/O//fZbHR2djz/+uK6uru0Gu3btCg0NDQsLKy0tvXHjxrNnz9zc3F69ekU66iJCyLZt2/7yl7/Exsa+ePHC09Nz+fLlt2/f7s6BvHz5khAie1Gbx+Px+XwmHsZPP/1UUFCwfPnytzUyduzY58+f3717tzuRAAAAA1Ug9GUikcjIyMjc3NzPz6+urq6oqIhdxeFwmJNkw4cPT0hIqKmpSUxM7MIu5s+fX11dHR4e3nNR///q6uqePHnCnDNrl6ur66ZNmwoLC7dt29ZqlVgsjomJWbRo0cqVK/v16zdq1KhDhw69fv368OHDspu120USiSQhIcHb29vHx8fY2HjHjh1cLrdr/cNiHgfW1dWVXcjlcsViMRtwUFBQQkKCnEacnZ0JITk5Od2JBAAAGKgCQSvo6ekRQtgTXa1MmDBBIBC0fUxB5UpLS2mabvdEICsqKmrIkCEHDx7MyMiQXZ6bm1tbWzthwgR2ycSJE/X09Nhr363IdlFeXl59ff3IkSOZVXw+39LSspv9w9zX2OrZjsbGRj6fz/z/9u3b//SnP1lbW8tphOkK2dOHAADQZagCAQghRF9fv6ysTNVRtCaRSAghbZ+0kMXj8RITEymK+vDDD9nzaoQQZlAVQ0ND2Y2NjY1ramo63C9zfXnHjh3Ufz19+rTtyC+dwtxqWV1dzS6pr6+XSCRWVlaEkIyMjJycnDVr1shvhCkZmW4BAIBuQhUIQJqamiorK21sbFQdSGtM0dPhaMmurq6bN2/Oz8/fs2cPu9DY2JgQ0qrmU/AwmYczYmNjaRmZmZldOASWvb29UCh8+vQpu4S5sXL06NGEkKNHj169elVHR4cpOpkA9u7dS1GU7P2IjY2N5L/dAgAA3YQqEIBcu3aNpunJkyczLzkcztuuHSuZhYUFRVGKjAi4Z8+eoUOH3rlzh10ycuRIQ0ND2RLq1q1bjY2N48eP77C1gQMH8ni87OzsroXdLg6HM2/evBs3brBP0qSlpVEUxTz+nJiYKFtxMudlw8LCaJqWvajNdMWAAQN6MDAAAK2FKhC0lFQqffPmTXNz871794KCgmxtbf39/ZlVTk5OFRUVZ8+ebWpqKisrkz19RQgxNTUtKSkpLCysqalpampKS0vrvZFiBAKBg4NDcXFxh1sy14Vln73g8Xhbtmw5c+bM8ePHq6urc3Jy1q1bZ2VlFRAQoEhrq1evPnHiREJCQnV1dUtLS3Fx8YsXLwghfn5+AwYM6NoMdeHh4a9evdq5c2ddXV1mZmZ0dLS/v/+QIUMUb4HpilGjRnVh7wAA0BoNCiCEJCUlqToKbcEMTdept3z55ZfMbWcCgcDLy+vgwYPMYwTOzs4FBQWHDx82MjIihNjZ2T18+JCm6YCAAC6Xa21tzeFwjIyMFi5cWFBQwLZWXl4+c+ZMHo9nb2+/YcOG4OBgQoiTk1NRURFN01lZWXZ2dnw+f+rUqS9fvrx06ZJQKIyKiurCkSryuQoMDORyufX19czLM2fOMI8Mm5mZffrpp602Dg4OXrBgAftSKpVGR0c7OztzuVwTExNvb++8vDxmVYdd1NDQEBISYmtry+FwzM3NfXx8cnNzaZr29vYmhERERLQbbWZm5pQpU5hb/QghlpaWIpHo+vXr7AbXr1+fNGmSvr6+lZVVcHCwRCJptx3Zc4Gy5s+fb21tLZVK5Xcaje8sAIACKBrjryqAoqikpCTZYeSg9yQnJy9durRXP5lr1649depUeXl57+1CEYp8rh49ejRs2LDExMSVK1cqLTA5pFLpjBkz/P39P/zwQyXvury83MbGJioqasuWLR1ujO8sAECHcEUYtFSHj1yoCScnp8jIyMjIyNraWlXHQlpaWs6ePVtTU+Pn56f8ve/atcvFxSUwMFD5uwYA6JNQBWqFlJQUBwcHSoaenp6FhcWMGTOio6PfvHmj6gBBntDQ0CVLlvj5+SnymEivunbtWkpKSlpamvwhDHtDTExMdnb2pUuXuFyukncNANBXoQrUCj4+Po8fP3Z0dOzXrx9N01KptLS0NDk52d7ePiQkZMSIEd2cHEyzbN++PTExsaqqyt7e/vTp06oORyF79+4NDAzcv3+/asNwd3f/7rvv2EmWlSY1NbWhoeHatWsmJiZK3jUAQB+GKlAtiMVikUiktN1RFGVsbDxjxozExMTk5ORXr17Nnz9f5eeZlGbfvn0NDQ00TT958mTx4sWqDkdRHh4eBw4cUHUUqrFgwYLQ0NBWs88BAEA3oQpUC0ePHi0tLVXJrhcvXuzv719aWnro0CGVBAAAAAAqgSqwx9A0HRMTM2zYMH19fRMTk4ULF7LzrgYGBurp6bHX0davX29gYEBR1OvXrwkhQUFBW7ZsKSgooCjKycmpwx39+9//Hj58eL9+/Xg83qhRo65cucIsv3z5ctcGrmPGyUtLS2NetrS0RERE2Nra8vn80aNHM+O2JCQkGBgYCASC1NTUuXPnGhkZ2djYnDhxgm2EGQFEIBAYGRmNGjWKmSis3aYAAABAHaAK7DG7du0KDQ0NCwsrLS29cePGs2fP3NzcmGnv4+PjZUesOHjw4O7du9mXcXFxnp6ejo6ONE0zc2rJ9+rVq6VLlxYWFpaUlBgaGq5YsYJZzjz0yk7MoDgXFxdCyOPHj5mX27Zt+8tf/hIbG/vixQtPT8/ly5ffvn37k08+2bRpk1gsFgqFSUlJBQUFDg4OH3/8MTPHRl1dnZeX1+LFiysqKvLz8wcPHszM9NVuU50NDwAAAHoDqsCeIRaLY2JiFi1atHLlyn79+o0aNerQoUOvX78+fPhwj+9r8eLFO3fuNDExMTU19fLyKi8vZ4bYnT9/fnV1dXh4eGcbFAqFFEUxE85KJJKEhARvb28fHx9jY+MdO3ZwudzExER2Y5FIZGRkZG5u7ufnV1dXV1RURAgpLCysrq4eMWIEj8cbMGBASkqKmZlZh00BAACACnFUHUAfkZubW1tbKzvh6cSJE/X09G7dutWr+2VGzejm0Hd1dXU0TTNTR+Tl5dXX148cOZJZxefzLS0t2UvbsvT09AghzLlABwcHCwuLlStXbty40d/ff9CgQZ1qql3JycndOShNkZmZqeoQAABAS6EK7BmVlZWEEENDQ9mFxsbGzAm2nnXx4sXo6Ojc3Nzq6mqmCOumhw8fEkKGDh1KCKmrqyOE7NixY8eOHewG7IRgb8Pn83/88cdt27bt3bs3MjLS19c3MTGxa02xli5d2snj0EhxcXFxcXGqjgIAALQRrgj3DGNjY0JIq5qvsrLSxsamZ3dUVFTk7e1taWl569atqqqqzz77rPttXr58mRAyd+5cQoi5uTkhJDY2VnaeQUXOV40YMeL8+fMlJSUhISFJSUmff/55l5tiKGkORZUimOu213T56wAAoD1QBfaMkSNHGhoayj76cOvWrcbGxvHjxzMvORxOj5y3y8nJaWpq+uSTTxwcHHg8HkVR3Wzw5cuXsbGxNjY2zLSwAwcO5PF42dnZnWqkpKTk/v37hBBzc/P9+/ePGzfu/v37XWsKAAAAlANVYM/g8Xhbtmw5c+bM8ePHq6urc3Jy1q1bZ2VlFRAQwGzg5ORUUVFx9uzZpqamsrKyp0+fyr7d1NS0pKSksLCwpqZGfrFoa2tLCElPT5dIJPn5+bL3HaalpXU4UgxN07W1tVKplKbpsrKypKSkKVOm6Orqnj17lrkvkMfjrV69+sSJEwkJCdXV1S0tLcXFxS9evJB/+CUlJWvXrn3w4EFjY+OdO3eePn06efLkrjUFAAAASqLq6zaagShw5U4qlUZHRzs7O3O5XBMTE29v77y8PHZteXn5zJkzeTyevb39hg0bgoODCSFOTk5FRUU0TWdlZdnZ2fH5/KlTp758+VL+jkJCQkxNTY2NjZcsWfLVV18RQhwdHYuKii5duiQUCqOiotq+5dy5c6NHjxYIBHp6ejo6OuS/04dMmjQpMjKyvLxcduOGhoaQkBBbW1sOh2Nubu7j45Obm3vw4EFm6lhnZ+eCgoLDhw8zVaOdnd3Dhw8LCwtFIpGJiYmuru4777wTFhbW3Nz8tqY67G1mWMEON+sDFPlcQdegbwEAOkTRuIFGARRFJSUlyY75B70nOTl56dKl2vDJxOeq96BvAQA6hCvCAAAAANoIVaB6efDgAfV2fn5+qg4QAAAA+ghUgepl6NChcq7fnzx5UtUBgppKT08PDQ1NSUlxcHBgfjOsWrVKdgMPDw+hUKirqztixIisrCxVxUkIkUqlsbGxIpFIduG5c+c+++yzbo5/DgAAnYIqEEDj7dy5Mz4+fvv27T4+Po8fP3Z0dOzfv//x48cvXrzIbvP999+fOnXK09MzNzd33Lhxqgo1Pz9/2rRpmzdvrq+vl13u5eXF4/Hc3d2ZAdgBAEAJUAWCNhKLxa3ORalDU11z4MCBkydPJicnC4VCdmF8fLyOjk5AQEBVVZUKY2vl7t2727ZtW7dunYuLS9u1GzduHDNmzLx585qbm5UfGwCAFkIVCNro6NGjpaWl6tZUFzx69Cg8/P9r715DmvrDOID/ZrucbW65UKdoM3VWimaUUq6iIhBCSE2sgb4w32hUw7xQmonpsheChuAICfaiQsoLCqG9KFgQSRQpipGVqGE3p3mZd3Pn/+LQGF7m3KbL/76fd+f3O+fxOT/O5GFn5zk3b926RVGU+bhCocjKyvr27Vtubq6zclsuMjKyoaEhJSWFx+OtuENxcXFHRwdeqQcAsDlQBcJWRdN0RUVFaGgoj8eTSCQJCQkfP35kplQqFZfL9fHxYTYvXbokFApZLNbw8DAhJCsrKycnp7e3l8ViyeXyqqoqiqK8vb0zMzN9fX0pilIoFKZ23OsKRQh59uzZmr27Haiqqoqm6TNnziyfUqvVu3fvvn///vPnz1c81sICajQaoVAoEAiam5tPnz4tFov9/f1ra2tNxy4uLhYVFclkMj6fv2/fPqbFo/0kEsnx48fv3r3rCn2CAACcbxN6Ev4PEHSg3URWdo0uKiricrkPHjwYGxvr7Ow8cOCAp6enqed2SkqKVCo17VxeXk4I0ev1zGZSUlJwcLBpNiMjQygUfvjwYXZ2tru7Ozo6WiQSMQ291xvq6dOnIpGopKTEmjO1/7oKCgoKCwtbMhgcHNzX10fT9OvXr93c3Hbt2jU5OUnTdGtra3x8vGk3ywt448YNQsiLFy/Gx8eHhoaOHTsmFArn5+eZ2dzcXB6PV19fPzo6WlBQ4Obm9vbtW+vTPnToUGRk5IpT+fn5hJD29nbro60In1kAgDXhu0DYkmZmZioqKs6ePZuamrp9+/aIiIh79+4NDw/X1NTYFpDNZjPfioWFhWk0GoPBoNVqbYgTFxc3MTFx8+ZN29JYl6mpqb6+vuDg4NV2iImJuXr1an9///Xr15dMWbmACoVCLBZ7eXkplcqpqamvX78SQmZnZzUaTWJiYlJSkoeHR2FhIYfDsW25lgsJCSGEdHV1OSQaAABYgCoQtqTu7u7JycmoqCjTSHR0NJfLNX+xss2ioqIEAoHp9ug/a2hoiKZp5s1+q1Gr1Xv27Kmurn716pX5+HoXkMvlEkKYl1z39PRMT0+Hh4czU3w+38fHx1HLxZzOr1+/HBINAAAsQBUIWxLTT8Td3d180MPDw2AwOCQ+j8fT6/UOCbVxZmdnCSGrPWnBoChKq9WyWKz09PSZmRnTuD0LODU1RQgpLCw09TMfGBhY0vnFZnw+n/w9NQAA2FCoAmFL8vDwIIQsKVnGxsb8/f3tD76wsOCoUBuKKZjW7LQcExOTnZ39+fPn0tJS06A9C+jl5UUIqaysNP9xSVtbmw2nsNz8/Dz5e2oAALChUAXClhQeHu7u7v7u3TvTyJs3b+bn5w8ePMhsstls5valDXQ6HU3Thw8ftj/UhvL29maxWNZ0BCwtLd27d297e7tpZM0FtGDnzp0URXV0dNiWtmXM6Uil0o0IDgAA5lAFwpZEUVROTk5jY+PDhw8nJia6urouXrzo6+ubkZHB7CCXy3///t3U1LSwsKDX6wcGBswP37Fjx/fv3/v7+w0GA1PhGY3G0dHRP3/+dHZ2ZmVlyWSytLQ0G0K1trZuWqcYgUAQFBQ0ODi45p7MfeFt27aZj1heQMvRLly4UFtbq9FoJiYmFhcXBwcHf/z4QQhRKpVSqdSeN9QxpxMREWFzBAAAsJZzHk3eagi6TmwiKzvFGI3G8vLykJAQDocjkUgSExN7enpMsyMjIydPnqQoKjAw8MqVK3l5eYQQuVzO9H95//59QEAAn88/evToz58/MzIyOByOn58fm80Wi8UJCQm9vb22hWppaRGJRGq12poztf+6UqlUHA5nenqa2WxsbGQeGfb09Lx8+fKSnfPy8sw7xVhYwOrqauYpjZCQkN7e3pqaGrFYTAgJCAj49OkTTdNzc3PXrl2TyWRsNtvLyyspKam7u5um6cTEREJIUVHRitm2tbUdOXLE19eX+efj4+OjUChevnxpvk9cXJyfn5/RaLRnWWh8ZgEArMCi0Z3VCiwW6/Hjx+fOnXN2Ii7hyZMn58+f38wrMzMzs66ubmRkZNP+IsP+6+rLly+hoaFarTY1NdWBidnMaDSeOHEiLS0tPT3dhsNHRkb8/f3VanVOTo6dmeAzCwCwJtwRBiDEimcs/k1yubykpKSkpGRyctLZuZDFxcWmpiaDwaBUKm2LUFxcvH//fpVK5djEAABgRagCAba2/Pz85ORkpVJpzWMiG0qn0zU0NLS2tlpuYbiaioqKjo6OlpYWDofj8NwAAGA5VIHg6goKCrRa7fj4eGBgYH19vbPTscXt27dVKtWdO3ecm8apU6cePXpkeufyujQ3N8/Nzel0OolE4vDEAABgRWxnJwDgZGVlZWVlZc7Owl6xsbGxsbHOzsJ28fHx8fHxzs4CAMC14LtAAAAAAFeEKhAAAADAFaEKBAAAAHBFqAIBAAAAXBGeDrFWZWVlXV2ds7NwCcw7xJKTk52dyGbAdQUAAM6Cd4dYxUUqEoD/k+zs7JiYGGdnAQDw70IVCAAAAOCK8LtAAAAAAFeEKhAAAADAFaEKBAAAAHBFqAIBAAAAXNF/flG7h50+RVsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvp16EsGHulp",
        "outputId": "c32df922-e7a4-44e7-ebe7-37f436bba0ea"
      },
      "source": [
        "help(Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Model in module tensorflow.python.keras.engine.training:\n",
            "\n",
            "class Model(tensorflow.python.keras.engine.base_layer.Layer, tensorflow.python.keras.utils.version_utils.ModelVersionSelector)\n",
            " |  `Model` groups layers into an object with training and inference features.\n",
            " |  \n",
            " |  Arguments:\n",
            " |      inputs: The input(s) of the model: a `keras.Input` object or list of\n",
            " |          `keras.Input` objects.\n",
            " |      outputs: The output(s) of the model. See Functional API example below.\n",
            " |      name: String, the name of the model.\n",
            " |  \n",
            " |  There are two ways to instantiate a `Model`:\n",
            " |  \n",
            " |  1 - With the \"Functional API\", where you start from `Input`,\n",
            " |  you chain layer calls to specify the model's forward pass,\n",
            " |  and finally you create your model from inputs and outputs:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  inputs = tf.keras.Input(shape=(3,))\n",
            " |  x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n",
            " |  outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
            " |  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
            " |  ```\n",
            " |  \n",
            " |  2 - By subclassing the `Model` class: in that case, you should define your\n",
            " |  layers in `__init__` and you should implement the model's forward pass\n",
            " |  in `call`.\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super(MyModel, self).__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |  \n",
            " |    def call(self, inputs):\n",
            " |      x = self.dense1(inputs)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  If you subclass `Model`, you can optionally have\n",
            " |  a `training` argument (boolean) in `call`, which you can use to specify\n",
            " |  a different behavior in training and inference:\n",
            " |  \n",
            " |  ```python\n",
            " |  import tensorflow as tf\n",
            " |  \n",
            " |  class MyModel(tf.keras.Model):\n",
            " |  \n",
            " |    def __init__(self):\n",
            " |      super(MyModel, self).__init__()\n",
            " |      self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
            " |      self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
            " |      self.dropout = tf.keras.layers.Dropout(0.5)\n",
            " |  \n",
            " |    def call(self, inputs, training=False):\n",
            " |      x = self.dense1(inputs)\n",
            " |      if training:\n",
            " |        x = self.dropout(x, training=training)\n",
            " |      return self.dense2(x)\n",
            " |  \n",
            " |  model = MyModel()\n",
            " |  ```\n",
            " |  \n",
            " |  Once the model is created, you can config the model with losses and metrics\n",
            " |  with `model.compile()`, train the model with `model.fit()`, or use the model\n",
            " |  to do prediction with `model.predict()`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Model\n",
            " |      tensorflow.python.keras.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
            " |      tensorflow.python.keras.utils.version_utils.ModelVersionSelector\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *args, **kwargs)\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Builds the model based on input shapes received.\n",
            " |      \n",
            " |      This is to be used for subclassed models, which do not know at instantiation\n",
            " |      time what their inputs look like.\n",
            " |      \n",
            " |      This method only exists for users who want to call `model.build()` in a\n",
            " |      standalone way (as a substitute for calling the model on real data to\n",
            " |      build it). It will never be called by the framework (and thus it will\n",
            " |      never throw unexpected errors in an unrelated workflow).\n",
            " |      \n",
            " |      Args:\n",
            " |       input_shape: Single tuple, TensorShape, or list/dict of shapes, where\n",
            " |           shapes are tuples, integers, or TensorShapes.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError:\n",
            " |          1. In case of invalid user-provided data (not of type tuple,\n",
            " |             list, TensorShape, or dict).\n",
            " |          2. If the model requires call arguments that are agnostic\n",
            " |             to the input shapes (positional or kwarg in call signature).\n",
            " |          3. If not all layers were properly built.\n",
            " |          4. If float type inputs are not supported within the layers.\n",
            " |      \n",
            " |        In each of these cases, the user should build their model by calling it\n",
            " |        on real tensor data.\n",
            " |  \n",
            " |  call(self, inputs, training=None, mask=None)\n",
            " |      Calls the model on new inputs.\n",
            " |      \n",
            " |      In this case `call` just reapplies\n",
            " |      all ops in the graph to the new inputs\n",
            " |      (e.g. build a new computational graph from the provided inputs).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: A tensor or list of tensors.\n",
            " |          training: Boolean or boolean scalar tensor, indicating whether to run\n",
            " |            the `Network` in training mode or inference mode.\n",
            " |          mask: A mask or list of masks. A mask can be\n",
            " |              either a tensor or None (no mask).\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor if there is a single output, or\n",
            " |          a list of tensors if there are more than one outputs.\n",
            " |  \n",
            " |  compile(self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, **kwargs)\n",
            " |      Configures the model for training.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          optimizer: String (name of optimizer) or optimizer instance. See\n",
            " |            `tf.keras.optimizers`.\n",
            " |          loss: String (name of objective function), objective function or\n",
            " |            `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n",
            " |            function is any callable with the signature `loss = fn(y_true,\n",
            " |            y_pred)`, where y_true = ground truth values with shape =\n",
            " |            `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse\n",
            " |            categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`.\n",
            " |            y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It\n",
            " |            returns a weighted loss float tensor. If a custom `Loss` instance is\n",
            " |            used and reduction is set to NONE, return value has the shape\n",
            " |            [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values;\n",
            " |            otherwise, it is a scalar. If the model has multiple outputs, you can\n",
            " |            use a different loss on each output by passing a dictionary or a list\n",
            " |            of losses. The loss value that will be minimized by the model will\n",
            " |            then be the sum of all individual losses.\n",
            " |          metrics: List of metrics to be evaluated by the model during training\n",
            " |            and testing. Each of this can be a string (name of a built-in\n",
            " |            function), function or a `tf.keras.metrics.Metric` instance. See\n",
            " |            `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A\n",
            " |            function is any callable with the signature `result = fn(y_true,\n",
            " |            y_pred)`. To specify different metrics for different outputs of a\n",
            " |            multi-output model, you could also pass a dictionary, such as\n",
            " |              `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n",
            " |                You can also pass a list (len = len(outputs)) of lists of metrics\n",
            " |                such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n",
            " |                `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
            " |                strings 'accuracy' or 'acc', we convert this to one of\n",
            " |                `tf.keras.metrics.BinaryAccuracy`,\n",
            " |                `tf.keras.metrics.CategoricalAccuracy`,\n",
            " |                `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n",
            " |                function used and the model output shape. We do a similar\n",
            " |                conversion for the strings 'crossentropy' and 'ce' as well.\n",
            " |          loss_weights: Optional list or dictionary specifying scalar coefficients\n",
            " |            (Python floats) to weight the loss contributions of different model\n",
            " |            outputs. The loss value that will be minimized by the model will then\n",
            " |            be the *weighted sum* of all individual losses, weighted by the\n",
            " |            `loss_weights` coefficients.\n",
            " |              If a list, it is expected to have a 1:1 mapping to the model's\n",
            " |                outputs. If a dict, it is expected to map output names (strings)\n",
            " |                to scalar coefficients.\n",
            " |          weighted_metrics: List of metrics to be evaluated and weighted by\n",
            " |            sample_weight or class_weight during training and testing.\n",
            " |          run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
            " |            logic will not be wrapped in a `tf.function`. Recommended to leave\n",
            " |            this as `None` unless your `Model` cannot be run inside a\n",
            " |            `tf.function`.\n",
            " |          steps_per_execution: Int. Defaults to 1. The number of batches to\n",
            " |            run during each `tf.function` call. Running multiple batches\n",
            " |            inside a single `tf.function` call can greatly improve performance\n",
            " |            on TPUs or small models with a large Python overhead.\n",
            " |            At most, one full epoch will be run each\n",
            " |            execution. If a number larger than the size of the epoch is passed,\n",
            " |            the execution will be truncated to the size of the epoch.\n",
            " |            Note that if `steps_per_execution` is set to `N`,\n",
            " |            `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n",
            " |            will only be called every `N` batches\n",
            " |            (i.e. before/after each `tf.function` execution).\n",
            " |          **kwargs: Arguments supported for backwards compatibility only.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: In case of invalid arguments for\n",
            " |              `optimizer`, `loss` or `metrics`.\n",
            " |  \n",
            " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False)\n",
            " |      Returns the loss value & metrics values for the model in test mode.\n",
            " |      \n",
            " |      Computation is done in batches (see the `batch_size` arg.)\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely). If\n",
            " |            `x` is a dataset, generator or `keras.utils.Sequence` instance, `y`\n",
            " |            should not be specified (since targets will be obtained from the\n",
            " |            iterator/dataset).\n",
            " |          batch_size: Integer or `None`. Number of samples per batch of\n",
            " |            computation. If unspecified, `batch_size` will default to 32. Do not\n",
            " |            specify the `batch_size` if your data is in the form of a dataset,\n",
            " |            generators, or `keras.utils.Sequence` instances (since they generate\n",
            " |            batches).\n",
            " |          verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n",
            " |          sample_weight: Optional Numpy array of weights for the test samples,\n",
            " |            used for weighting the loss function. You can either pass a flat (1D)\n",
            " |            Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples), or in the case of\n",
            " |                temporal data, you can pass a 2D array with shape `(samples,\n",
            " |                sequence_length)`, to apply a different weight to every timestep\n",
            " |                of every sample. This argument is not supported when `x` is a\n",
            " |                dataset, instead pass sample weights as the third element of `x`.\n",
            " |          steps: Integer or `None`. Total number of steps (batches of samples)\n",
            " |            before declaring the evaluation round finished. Ignored with the\n",
            " |            default value of `None`. If x is a `tf.data` dataset and `steps` is\n",
            " |            None, 'evaluate' will run until the dataset is exhausted. This\n",
            " |            argument is not supported with array inputs.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances. List of\n",
            " |            callbacks to apply during evaluation. See\n",
            " |            [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |            input only. Maximum size for the generator queue. If unspecified,\n",
            " |            `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |            only. Maximum number of processes to spin up when using process-based\n",
            " |            threading. If unspecified, `workers` will default to 1. If 0, will\n",
            " |            execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |            threading. If unspecified, `use_multiprocessing` will default to\n",
            " |            `False`. Note that because this implementation relies on\n",
            " |            multiprocessing, you should not pass non-picklable arguments to the\n",
            " |            generator as they can't be passed easily to children processes.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.evaluate` is wrapped in `tf.function`.\n",
            " |          ValueError: in case of invalid arguments.\n",
            " |  \n",
            " |  evaluate_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Evaluates the model on a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.evaluate` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |              if the model has named inputs.\n",
            " |            - A `tf.data` dataset. Should return a tuple\n",
            " |              of either `(inputs, targets)` or\n",
            " |              `(inputs, targets, sample_weights)`.\n",
            " |            - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            " |              or `(inputs, targets, sample_weights)`.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given below.\n",
            " |          y: Target data. Like the input data `x`,\n",
            " |            it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            " |            It should be consistent with `x` (you cannot have Numpy inputs and\n",
            " |            tensor targets, or inversely). If `x` is a dataset, generator,\n",
            " |            or `keras.utils.Sequence` instance, `y` should\n",
            " |            not be specified (since targets will be obtained from `x`).\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per gradient update.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          epochs: Integer. Number of epochs to train the model.\n",
            " |              An epoch is an iteration over the entire `x` and `y`\n",
            " |              data provided.\n",
            " |              Note that in conjunction with `initial_epoch`,\n",
            " |              `epochs` is to be understood as \"final epoch\".\n",
            " |              The model is not trained for a number of iterations\n",
            " |              given by `epochs`, but merely until the epoch\n",
            " |              of index `epochs` is reached.\n",
            " |          verbose: 0, 1, or 2. Verbosity mode.\n",
            " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            " |              Note that the progress bar is not particularly useful when\n",
            " |              logged to a file, so verbose=2 is recommended when not running\n",
            " |              interactively (eg, in a production environment).\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during training.\n",
            " |              See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
            " |              and `tf.keras.callbacks.History` callbacks are created automatically\n",
            " |              and need not be passed into `model.fit`.\n",
            " |              `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
            " |              `verbose` argument to `model.fit`.\n",
            " |          validation_split: Float between 0 and 1.\n",
            " |              Fraction of the training data to be used as validation data.\n",
            " |              The model will set apart this fraction of the training data,\n",
            " |              will not train on it, and will evaluate\n",
            " |              the loss and any model metrics\n",
            " |              on this data at the end of each epoch.\n",
            " |              The validation data is selected from the last samples\n",
            " |              in the `x` and `y` data provided, before shuffling. This argument is\n",
            " |              not supported when `x` is a dataset, generator or\n",
            " |             `keras.utils.Sequence` instance.\n",
            " |          validation_data: Data on which to evaluate\n",
            " |              the loss and any model metrics at the end of each epoch.\n",
            " |              The model will not be trained on this data. Thus, note the fact\n",
            " |              that the validation loss of data provided using `validation_split`\n",
            " |              or `validation_data` is not affected by regularization layers like\n",
            " |              noise and dropout.\n",
            " |              `validation_data` will override `validation_split`.\n",
            " |              `validation_data` could be:\n",
            " |                - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
            " |                - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
            " |                - dataset\n",
            " |              For the first two cases, `batch_size` must be provided.\n",
            " |              For the last case, `validation_steps` could be provided.\n",
            " |              Note that `validation_data` does not support all the data types that\n",
            " |              are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
            " |          shuffle: Boolean (whether to shuffle the training data\n",
            " |              before each epoch) or str (for 'batch'). This argument is ignored\n",
            " |              when `x` is a generator. 'batch' is a special option for dealing\n",
            " |              with the limitations of HDF5 data; it shuffles in batch-sized\n",
            " |              chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers)\n",
            " |              to a weight (float) value, used for weighting the loss function\n",
            " |              (during training only).\n",
            " |              This can be useful to tell the model to\n",
            " |              \"pay more attention\" to samples from\n",
            " |              an under-represented class.\n",
            " |          sample_weight: Optional Numpy array of weights for\n",
            " |              the training samples, used for weighting the loss function\n",
            " |              (during training only). You can either pass a flat (1D)\n",
            " |              Numpy array with the same length as the input samples\n",
            " |              (1:1 mapping between weights and samples),\n",
            " |              or in the case of temporal data,\n",
            " |              you can pass a 2D array with shape\n",
            " |              `(samples, sequence_length)`,\n",
            " |              to apply a different weight to every timestep of every sample. This\n",
            " |              argument is not supported when `x` is a dataset, generator, or\n",
            " |             `keras.utils.Sequence` instance, instead provide the sample_weights\n",
            " |              as the third element of `x`.\n",
            " |          initial_epoch: Integer.\n",
            " |              Epoch at which to start training\n",
            " |              (useful for resuming a previous training run).\n",
            " |          steps_per_epoch: Integer or `None`.\n",
            " |              Total number of steps (batches of samples)\n",
            " |              before declaring one epoch finished and starting the\n",
            " |              next epoch. When training with input tensors such as\n",
            " |              TensorFlow data tensors, the default `None` is equal to\n",
            " |              the number of samples in your dataset divided by\n",
            " |              the batch size, or 1 if that cannot be determined. If x is a\n",
            " |              `tf.data` dataset, and 'steps_per_epoch'\n",
            " |              is None, the epoch will run until the input dataset is exhausted.\n",
            " |              When passing an infinitely repeating dataset, you must specify the\n",
            " |              `steps_per_epoch` argument. This argument is not supported with\n",
            " |              array inputs.\n",
            " |          validation_steps: Only relevant if `validation_data` is provided and\n",
            " |              is a `tf.data` dataset. Total number of steps (batches of\n",
            " |              samples) to draw before stopping when performing validation\n",
            " |              at the end of every epoch. If 'validation_steps' is None, validation\n",
            " |              will run until the `validation_data` dataset is exhausted. In the\n",
            " |              case of an infinitely repeated dataset, it will run into an\n",
            " |              infinite loop. If 'validation_steps' is specified and only part of\n",
            " |              the dataset will be consumed, the evaluation will start from the\n",
            " |              beginning of the dataset at each epoch. This ensures that the same\n",
            " |              validation samples are used every time.\n",
            " |          validation_batch_size: Integer or `None`.\n",
            " |              Number of samples per validation batch.\n",
            " |              If unspecified, will default to `batch_size`.\n",
            " |              Do not specify the `validation_batch_size` if your data is in the\n",
            " |              form of datasets, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          validation_freq: Only relevant if validation data is provided. Integer\n",
            " |              or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
            " |              If an integer, specifies how many training epochs to run before a\n",
            " |              new validation run is performed, e.g. `validation_freq=2` runs\n",
            " |              validation every 2 epochs. If a Container, specifies the epochs on\n",
            " |              which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            " |              validation at the end of the 1st, 2nd, and 10th epochs.\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up\n",
            " |              when using process-based threading. If unspecified, `workers`\n",
            " |              will default to 1. If 0, will execute the generator on the main\n",
            " |              thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      Unpacking behavior for iterator-like inputs:\n",
            " |          A common pattern is to pass a tf.data.Dataset, generator, or\n",
            " |        tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            " |        yield not only features (x) but optionally targets (y) and sample weights.\n",
            " |        Keras requires that the output of such iterator-likes be unambiguous. The\n",
            " |        iterator should return a tuple of length 1, 2, or 3, where the optional\n",
            " |        second and third elements will be used for y and sample_weight\n",
            " |        respectively. Any other type provided will be wrapped in a length one\n",
            " |        tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
            " |        should still adhere to the top-level tuple structure.\n",
            " |        e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            " |        features, targets, and weights from the keys of a single dict.\n",
            " |          A notable unsupported data type is the namedtuple. The reason is that\n",
            " |        it behaves like both an ordered datatype (tuple) and a mapping\n",
            " |        datatype (dict). So given a namedtuple of the form:\n",
            " |            `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            " |        it is ambiguous whether to reverse the order of the elements when\n",
            " |        interpreting the value. Even worse is a tuple of the form:\n",
            " |            `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            " |        where it is unclear if the tuple was intended to be unpacked into x, y,\n",
            " |        and sample_weight or passed through as a single element to `x`. As a\n",
            " |        result the data processing code will simply raise a ValueError if it\n",
            " |        encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          A `History` object. Its `History.history` attribute is\n",
            " |          a record of training loss values and metrics values\n",
            " |          at successive epochs, as well as validation loss values\n",
            " |          and validation metrics values (if applicable).\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: 1. If the model was never compiled or,\n",
            " |          2. If `model.fit` is  wrapped in `tf.function`.\n",
            " |      \n",
            " |          ValueError: In case of mismatch between the provided input data\n",
            " |              and what the model expects or when the input data is empty.\n",
            " |  \n",
            " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
            " |      Fits the model on data yielded batch-by-batch by a Python generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.fit` now supports generators, so there is no longer any need to use\n",
            " |        this endpoint.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  get_layer(self, name=None, index=None)\n",
            " |      Retrieves a layer based on either its name (unique) or index.\n",
            " |      \n",
            " |      If `name` and `index` are both provided, `index` will take precedence.\n",
            " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          name: String, name of layer.\n",
            " |          index: Integer, index of layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: In case of invalid layer name or index.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Retrieves the weights of the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A flat list of Numpy arrays.\n",
            " |  \n",
            " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None)\n",
            " |      Loads all layer weights, either from a TensorFlow or an HDF5 weight file.\n",
            " |      \n",
            " |      If `by_name` is False weights are loaded based on the network's\n",
            " |      topology. This means the architecture should be the same as when the weights\n",
            " |      were saved.  Note that layers that don't have weights are not taken into\n",
            " |      account in the topological ordering, so adding or removing layers is fine as\n",
            " |      long as they don't have weights.\n",
            " |      \n",
            " |      If `by_name` is True, weights are loaded into layers only if they share the\n",
            " |      same name. This is useful for fine-tuning or transfer-learning models where\n",
            " |      some of the layers have changed.\n",
            " |      \n",
            " |      Only topological loading (`by_name=False`) is supported when loading weights\n",
            " |      from the TensorFlow format. Note that topological loading differs slightly\n",
            " |      between TensorFlow and HDF5 formats for user-defined classes inheriting from\n",
            " |      `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the\n",
            " |      TensorFlow format loads based on the object-local names of attributes to\n",
            " |      which layers are assigned in the `Model`'s constructor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String, path to the weights file to load. For weight files in\n",
            " |              TensorFlow format, this is the file prefix (the same as was passed\n",
            " |              to `save_weights`).\n",
            " |          by_name: Boolean, whether to load weights by name or by topological\n",
            " |              order. Only topological loading is supported for weight files in\n",
            " |              TensorFlow format.\n",
            " |          skip_mismatch: Boolean, whether to skip loading of layers where there is\n",
            " |              a mismatch in the number of weights, or a mismatch in the shape of\n",
            " |              the weight (only valid when `by_name=True`).\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for loading weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          When loading a weight file in TensorFlow format, returns the same status\n",
            " |          object as `tf.train.Checkpoint.restore`. When graph building, restore\n",
            " |          ops are run automatically as soon as the network is built (on first call\n",
            " |          for user-defined classes inheriting from `Model`, immediately if it is\n",
            " |          already built).\n",
            " |      \n",
            " |          When loading weights in HDF5 format, returns `None`.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If h5py is not available and the weight file is in HDF5\n",
            " |              format.\n",
            " |          ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n",
            " |            `False`.\n",
            " |  \n",
            " |  make_predict_function(self)\n",
            " |      Creates a function that executes one step of inference.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.predict` and `Model.predict_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.predict_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.predict` or\n",
            " |      `Model.predict_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return the outputs of the `Model`.\n",
            " |  \n",
            " |  make_test_function(self)\n",
            " |      Creates a function that executes one step of evaluation.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.evaluate` and `Model.test_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual evaluation\n",
            " |      logic to `Model.test_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.evaluate` or\n",
            " |      `Model.test_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_test_batch_end`.\n",
            " |  \n",
            " |  make_train_function(self)\n",
            " |      Creates a function that executes one step of training.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.fit` and `Model.train_on_batch`.\n",
            " |      \n",
            " |      Typically, this method directly controls `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings, and delegates the actual training\n",
            " |      logic to `Model.train_step`.\n",
            " |      \n",
            " |      This function is cached the first time `Model.fit` or\n",
            " |      `Model.train_on_batch` is called. The cache is cleared whenever\n",
            " |      `Model.compile` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Function. The function created by this method should accept a\n",
            " |        `tf.data.Iterator`, and return a `dict` containing values that will\n",
            " |        be passed to `tf.keras.Callbacks.on_train_batch_end`, such as\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
            " |      Generates output predictions for the input samples.\n",
            " |      \n",
            " |      Computation is done in batches. This method is designed for performance in\n",
            " |      large scale inputs. For small amount of inputs that fit in one batch,\n",
            " |      directly using `__call__` is recommended for faster execution, e.g.,\n",
            " |      `model(x)`, or `model(x, training=False)` if you have layers such as\n",
            " |      `tf.keras.layers.BatchNormalization` that behaves differently during\n",
            " |      inference. Also, note the fact that test loss is not affected by\n",
            " |      regularization layers like noise and dropout.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input samples. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |              (in case the model has multiple inputs).\n",
            " |            - A `tf.data` dataset.\n",
            " |            - A generator or `keras.utils.Sequence` instance.\n",
            " |            A more detailed description of unpacking behavior for iterator types\n",
            " |            (Dataset, generator, Sequence) is given in the `Unpacking behavior\n",
            " |            for iterator-like inputs` section of `Model.fit`.\n",
            " |          batch_size: Integer or `None`.\n",
            " |              Number of samples per batch.\n",
            " |              If unspecified, `batch_size` will default to 32.\n",
            " |              Do not specify the `batch_size` if your data is in the\n",
            " |              form of dataset, generators, or `keras.utils.Sequence` instances\n",
            " |              (since they generate batches).\n",
            " |          verbose: Verbosity mode, 0 or 1.\n",
            " |          steps: Total number of steps (batches of samples)\n",
            " |              before declaring the prediction round finished.\n",
            " |              Ignored with the default value of `None`. If x is a `tf.data`\n",
            " |              dataset and `steps` is None, `predict` will\n",
            " |              run until the input dataset is exhausted.\n",
            " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
            " |              List of callbacks to apply during prediction.\n",
            " |              See [callbacks](/api_docs/python/tf/keras/callbacks).\n",
            " |          max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            " |              input only. Maximum size for the generator queue.\n",
            " |              If unspecified, `max_queue_size` will default to 10.\n",
            " |          workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            " |              only. Maximum number of processes to spin up when using\n",
            " |              process-based threading. If unspecified, `workers` will default\n",
            " |              to 1. If 0, will execute the generator on the main thread.\n",
            " |          use_multiprocessing: Boolean. Used for generator or\n",
            " |              `keras.utils.Sequence` input only. If `True`, use process-based\n",
            " |              threading. If unspecified, `use_multiprocessing` will default to\n",
            " |              `False`. Note that because this implementation relies on\n",
            " |              multiprocessing, you should not pass non-picklable arguments to\n",
            " |              the generator as they can't be passed easily to children processes.\n",
            " |      \n",
            " |      See the discussion of `Unpacking behavior for iterator-like inputs` for\n",
            " |      `Model.fit`. Note that Model.predict uses the same interpretation rules as\n",
            " |      `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all\n",
            " |      three methods.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of mismatch between the provided\n",
            " |              input data and the model's expectations,\n",
            " |              or in case a stateful model receives a number of samples\n",
            " |              that is not a multiple of the batch size.\n",
            " |  \n",
            " |  predict_generator(self, generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
            " |      Generates predictions for the input samples from a data generator.\n",
            " |      \n",
            " |      DEPRECATED:\n",
            " |        `Model.predict` now supports generators, so there is no longer any need\n",
            " |        to use this endpoint.\n",
            " |  \n",
            " |  predict_on_batch(self, x)\n",
            " |      Returns predictions for a single batch of samples.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
            " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
            " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
            " |      \n",
            " |      Returns:\n",
            " |          Numpy array(s) of predictions.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.predict_on_batch` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of mismatch between given number of inputs and\n",
            " |            expectations of the model.\n",
            " |  \n",
            " |  predict_step(self, data)\n",
            " |      The logic for one inference step.\n",
            " |      \n",
            " |      This method can be overridden to support custom inference logic.\n",
            " |      This method is called by `Model.make_predict_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of inference.\n",
            " |      This typically includes the forward pass.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_predict_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The result of one inference step, typically the output of calling the\n",
            " |        `Model` on data.\n",
            " |  \n",
            " |  reset_metrics(self)\n",
            " |      Resets the state of all the metrics in the model.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> _ = model.fit(x, y, verbose=0)\n",
            " |      >>> assert all(float(m.result()) for m in model.metrics)\n",
            " |      \n",
            " |      >>> model.reset_metrics()\n",
            " |      >>> assert all(float(m.result()) == 0 for m in model.metrics)\n",
            " |  \n",
            " |  reset_states(self)\n",
            " |  \n",
            " |  save(self, filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None, save_traces=True)\n",
            " |      Saves the model to Tensorflow SavedModel or a single HDF5 file.\n",
            " |      \n",
            " |      Please see `tf.keras.models.save_model` or the\n",
            " |      [Serialization and Saving guide](https://keras.io/guides/serialization_and_saving/)\n",
            " |      for details.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String, PathLike, path to SavedModel or H5 file to save the\n",
            " |              model.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          include_optimizer: If True, save optimizer's state together.\n",
            " |          save_format: Either `'tf'` or `'h5'`, indicating whether to save the\n",
            " |              model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X,\n",
            " |              and 'h5' in TF 1.X.\n",
            " |          signatures: Signatures to save with the SavedModel. Applicable to the\n",
            " |              'tf' format only. Please see the `signatures` argument in\n",
            " |              `tf.saved_model.save` for details.\n",
            " |          options: (only applies to SavedModel format)\n",
            " |              `tf.saved_model.SaveOptions` object that specifies options for\n",
            " |              saving to SavedModel.\n",
            " |          save_traces: (only applies to SavedModel format) When enabled, the\n",
            " |              SavedModel will store the function traces for each layer. This\n",
            " |              can be disabled, so that only the configs of each layer are stored.\n",
            " |              Defaults to `True`. Disabling this will decrease serialization time\n",
            " |              and reduce file size, but it requires that all custom layers/models\n",
            " |              implement a `get_config()` method.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      from keras.models import load_model\n",
            " |      \n",
            " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
            " |      del model  # deletes the existing model\n",
            " |      \n",
            " |      # returns a compiled model\n",
            " |      # identical to the previous one\n",
            " |      model = load_model('my_model.h5')\n",
            " |      ```\n",
            " |  \n",
            " |  save_weights(self, filepath, overwrite=True, save_format=None, options=None)\n",
            " |      Saves all layer weights.\n",
            " |      \n",
            " |      Either saves in HDF5 or in TensorFlow format based on the `save_format`\n",
            " |      argument.\n",
            " |      \n",
            " |      When saving in HDF5 format, the weight file has:\n",
            " |        - `layer_names` (attribute), a list of strings\n",
            " |            (ordered names of model layers).\n",
            " |        - For every layer, a `group` named `layer.name`\n",
            " |            - For every such layer group, a group attribute `weight_names`,\n",
            " |                a list of strings\n",
            " |                (ordered names of weights tensor of the layer).\n",
            " |            - For every weight in the layer, a dataset\n",
            " |                storing the weight value, named after the weight tensor.\n",
            " |      \n",
            " |      When saving in TensorFlow format, all objects referenced by the network are\n",
            " |      saved in the same format as `tf.train.Checkpoint`, including any `Layer`\n",
            " |      instances or `Optimizer` instances assigned to object attributes. For\n",
            " |      networks constructed from inputs and outputs using `tf.keras.Model(inputs,\n",
            " |      outputs)`, `Layer` instances used by the network are tracked/saved\n",
            " |      automatically. For user-defined classes which inherit from `tf.keras.Model`,\n",
            " |      `Layer` instances must be assigned to object attributes, typically in the\n",
            " |      constructor. See the documentation of `tf.train.Checkpoint` and\n",
            " |      `tf.keras.Model` for details.\n",
            " |      \n",
            " |      While the formats are the same, do not mix `save_weights` and\n",
            " |      `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be\n",
            " |      loaded using `Model.load_weights`. Checkpoints saved using\n",
            " |      `tf.train.Checkpoint.save` should be restored using the corresponding\n",
            " |      `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over\n",
            " |      `save_weights` for training checkpoints.\n",
            " |      \n",
            " |      The TensorFlow format matches objects and variables by starting at a root\n",
            " |      object, `self` for `save_weights`, and greedily matching attribute\n",
            " |      names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this\n",
            " |      is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n",
            " |      means saving a `tf.keras.Model` using `save_weights` and loading into a\n",
            " |      `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n",
            " |      the `Model`'s variables. See the [guide to training\n",
            " |      checkpoints](https://www.tensorflow.org/guide/checkpoint) for details\n",
            " |      on the TensorFlow format.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          filepath: String or PathLike, path to the file to save the weights to.\n",
            " |              When saving in TensorFlow format, this is the prefix used for\n",
            " |              checkpoint files (multiple files are generated). Note that the '.h5'\n",
            " |              suffix causes weights to be saved in HDF5 format.\n",
            " |          overwrite: Whether to silently overwrite any existing file at the\n",
            " |              target location, or provide the user with a manual prompt.\n",
            " |          save_format: Either 'tf' or 'h5'. A `filepath` ending in '.h5' or\n",
            " |              '.keras' will default to HDF5 if `save_format` is `None`. Otherwise\n",
            " |              `None` defaults to 'tf'.\n",
            " |          options: Optional `tf.train.CheckpointOptions` object that specifies\n",
            " |              options for saving weights.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: If h5py is not available when attempting to save in HDF5\n",
            " |              format.\n",
            " |          ValueError: For invalid/unknown format arguments.\n",
            " |  \n",
            " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
            " |      Prints a string summary of the network.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          line_length: Total length of printed lines\n",
            " |              (e.g. set this to adapt the display to different\n",
            " |              terminal window sizes).\n",
            " |          positions: Relative or absolute positions of log elements\n",
            " |              in each line. If not provided,\n",
            " |              defaults to `[.33, .55, .67, 1.]`.\n",
            " |          print_fn: Print function to use. Defaults to `print`.\n",
            " |              It will be called on each line of the summary.\n",
            " |              You can set it to a custom function\n",
            " |              in order to capture the string summary.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if `summary()` is called before the model is built.\n",
            " |  \n",
            " |  test_on_batch(self, x, y=None, sample_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Test the model on a single batch of samples.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be: - A Numpy array (or array-like), or a list\n",
            " |            of arrays (in case the model has multiple inputs). - A TensorFlow\n",
            " |            tensor, or a list of tensors (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors, if\n",
            " |            the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar test loss (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`.\n",
            " |          ValueError: In case of invalid user-provided arguments.\n",
            " |  \n",
            " |  test_step(self, data)\n",
            " |      The logic for one evaluation step.\n",
            " |      \n",
            " |      This method can be overridden to support custom evaluation logic.\n",
            " |      This method is called by `Model.make_test_function`.\n",
            " |      \n",
            " |      This function should contain the mathematical logic for one step of\n",
            " |      evaluation.\n",
            " |      This typically includes the forward pass, loss calculation, and metrics\n",
            " |      updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_test_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned.\n",
            " |  \n",
            " |  to_json(self, **kwargs)\n",
            " |      Returns a JSON string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a JSON save file, use\n",
            " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `json.dumps()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A JSON string.\n",
            " |  \n",
            " |  to_yaml(self, **kwargs)\n",
            " |      Returns a yaml string containing the network configuration.\n",
            " |      \n",
            " |      To load a network from a yaml save file, use\n",
            " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
            " |      \n",
            " |      `custom_objects` should be a dictionary mapping\n",
            " |      the names of custom losses / layers / etc to the corresponding\n",
            " |      functions / classes.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `yaml.dump()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A YAML string.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ImportError: if yaml module is not found.\n",
            " |  \n",
            " |  train_on_batch(self, x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False)\n",
            " |      Runs a single gradient update on a single batch of data.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          x: Input data. It could be:\n",
            " |            - A Numpy array (or array-like), or a list of arrays\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A TensorFlow tensor, or a list of tensors\n",
            " |                (in case the model has multiple inputs).\n",
            " |            - A dict mapping input names to the corresponding array/tensors,\n",
            " |                if the model has named inputs.\n",
            " |          y: Target data. Like the input data `x`, it could be either Numpy\n",
            " |            array(s) or TensorFlow tensor(s). It should be consistent with `x`\n",
            " |            (you cannot have Numpy inputs and tensor targets, or inversely).\n",
            " |          sample_weight: Optional array of the same length as x, containing\n",
            " |            weights to apply to the model's loss for each sample. In the case of\n",
            " |            temporal data, you can pass a 2D array with shape (samples,\n",
            " |            sequence_length), to apply a different weight to every timestep of\n",
            " |            every sample.\n",
            " |          class_weight: Optional dictionary mapping class indices (integers) to a\n",
            " |            weight (float) to apply to the model's loss for the samples from this\n",
            " |            class during training. This can be useful to tell the model to \"pay\n",
            " |            more attention\" to samples from an under-represented class.\n",
            " |          reset_metrics: If `True`, the metrics returned will be only for this\n",
            " |            batch. If `False`, the metrics will be statefully accumulated across\n",
            " |            batches.\n",
            " |          return_dict: If `True`, loss and metric results are returned as a dict,\n",
            " |            with each key being the name of the metric. If `False`, they are\n",
            " |            returned as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Scalar training loss\n",
            " |          (if the model has a single output and no metrics)\n",
            " |          or list of scalars (if the model has multiple outputs\n",
            " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
            " |          the display labels for the scalar outputs.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If `model.train_on_batch` is wrapped in `tf.function`.\n",
            " |        ValueError: In case of invalid user-provided arguments.\n",
            " |  \n",
            " |  train_step(self, data)\n",
            " |      The logic for one training step.\n",
            " |      \n",
            " |      This method can be overridden to support custom training logic.\n",
            " |      This method is called by `Model.make_train_function`.\n",
            " |      \n",
            " |      This method should contain the mathematical logic for one step of training.\n",
            " |      This typically includes the forward pass, loss calculation, backpropagation,\n",
            " |      and metric updates.\n",
            " |      \n",
            " |      Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
            " |      `tf.distribute.Strategy` settings), should be left to\n",
            " |      `Model.make_train_function`, which can also be overridden.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        data: A nested structure of `Tensor`s.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `dict` containing values that will be passed to\n",
            " |        `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
            " |        values of the `Model`'s metrics are returned. Example:\n",
            " |        `{'loss': 0.2, 'accuracy': 0.7}`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_config(config, custom_objects=None) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Arguments:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  distribute_strategy\n",
            " |      The `tf.distribute.Strategy` this model was created under.\n",
            " |  \n",
            " |  layers\n",
            " |  \n",
            " |  metrics\n",
            " |      Returns the model's metrics added using `compile`, `add_metric` APIs.\n",
            " |      \n",
            " |      Note: Metrics passed to `compile()` are available only after a `keras.Model`\n",
            " |      has been trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.add_metric(\n",
            " |      ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> [m.name for m in model.metrics]\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc', 'mean']\n",
            " |  \n",
            " |  metrics_names\n",
            " |      Returns the model's display labels for all outputs.\n",
            " |      \n",
            " |      Note: `metrics_names` are available only after a `keras.Model` has been\n",
            " |      trained/evaluated on actual data.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> outputs = tf.keras.layers.Dense(2)(inputs)\n",
            " |      >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
            " |      >>> model.metrics_names\n",
            " |      []\n",
            " |      \n",
            " |      >>> x = np.random.random((2, 3))\n",
            " |      >>> y = np.random.randint(0, 2, (2, 2))\n",
            " |      >>> model.fit(x, y)\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'mae']\n",
            " |      \n",
            " |      >>> inputs = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2, name='out')\n",
            " |      >>> output_1 = d(inputs)\n",
            " |      >>> output_2 = d(inputs)\n",
            " |      >>> model = tf.keras.models.Model(\n",
            " |      ...    inputs=inputs, outputs=[output_1, output_2])\n",
            " |      >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
            " |      >>> model.fit(x, (y, y))\n",
            " |      >>> model.metrics_names\n",
            " |      ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',\n",
            " |      'out_1_acc']\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Non-trainable weights are *not* updated during training. They are expected\n",
            " |      to be updated manually in `call()`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of non-trainable variables.\n",
            " |  \n",
            " |  run_eagerly\n",
            " |      Settable attribute indicating whether the model should run eagerly.\n",
            " |      \n",
            " |      Running eagerly means that your model will be run step by step,\n",
            " |      like Python code. Your model might run slower, but it should become easier\n",
            " |      for you to debug it by stepping into individual layer calls.\n",
            " |      \n",
            " |      By default, we will attempt to compile your model to a static graph to\n",
            " |      deliver the best execution performance.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Boolean, whether the model should run eagerly.\n",
            " |  \n",
            " |  state_updates\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Returns the `updates` from all layers that are stateful.\n",
            " |      \n",
            " |      This is useful for separating training updates and\n",
            " |      state updates, e.g. when we need to update a layer's internal state\n",
            " |      during prediction.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A list of update ops.\n",
            " |  \n",
            " |  trainable_weights\n",
            " |      List of all trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Trainable weights are updated via gradient descent during training.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of trainable variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __call__(self, *args, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        *args: Positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
            " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_loss(self, losses, **kwargs)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be dependent\n",
            " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
            " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
            " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(self, inputs):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any loss Tensors passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      losses become part of the model's topology and are tracked in `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Activity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss references\n",
            " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
            " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
            " |      topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      d = tf.keras.layers.Dense(10)\n",
            " |      x = d(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      Arguments:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
            " |          may also be zero-argument callables which create a loss tensor.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |            inputs - Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_metric(self, value, name=None, **kwargs)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      This method can be used inside the `call()` method of a subclassed layer\n",
            " |      or model.\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
            " |        def __init__(self):\n",
            " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
            " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
            " |      \n",
            " |        def call(self, inputs):\n",
            " |          self.add_metric(self.mean(x))\n",
            " |          self.add_metric(tf.reduce_sum(x), name='metric_2')\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any tensor passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      metrics become part of the model's topology and are tracked when you\n",
            " |      save the model via `save()`.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
            " |      Functional Model, as shown in the example below, is not supported. This is\n",
            " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        name: String metric name.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |          `aggregation` - When the `value` tensor provided is not the result of\n",
            " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
            " |          using a `keras.Metric.Mean`.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Add update op(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and variance\n",
            " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
            " |      when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case, variable\n",
            " |      updates are run on the fly and thus do not need to be tracked for later\n",
            " |      execution).\n",
            " |      \n",
            " |      Arguments:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |        inputs: Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        use_resource: Whether to use `ResourceVariable`.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            " |          `AUTO` and the current `DistributionStrategy` chooses\n",
            " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
            " |          `trainable` must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The variable created.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
            " |  \n",
            " |  apply(self, inputs, *args, **kwargs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      This is an alias of `self.__call__`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask=None)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      If the layer has not been built, this method will call `build` on the\n",
            " |      layer. This assumes that the layer will later be used with inputs that\n",
            " |      match the input shape provided here.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
            " |          how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves losses relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of loss tensors of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves updates relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of update ops of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from Numpy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      sets the weight values from numpy arrays. The weight values should be\n",
            " |      passed in the order they are created by the layer. Note that the layer's\n",
            " |      weights must be instantiated before calling this function by calling\n",
            " |      the layer.\n",
            " |      \n",
            " |      For example, a Dense layer returns a list of two values-- per-output\n",
            " |      weights and the bias value. These can be used to set the weights of another\n",
            " |      Dense layer:\n",
            " |      \n",
            " |      >>> a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> b.set_weights(a.get_weights())\n",
            " |      >>> b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Arguments:\n",
            " |          weights: a list of Numpy arrays. The number\n",
            " |              of arrays and their shape must match\n",
            " |              number of the dimensions of the weights\n",
            " |              of the layer (i.e. it should match the\n",
            " |              output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the provided weights list does not match the\n",
            " |              layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  compute_dtype\n",
            " |      The dtype of the layer's computations.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
            " |      the weights.\n",
            " |      \n",
            " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
            " |      computations and the output to be in the compute dtype as well. This is done\n",
            " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
            " |      these casts if implementing your own layer.\n",
            " |      \n",
            " |      Layers often perform certain internal computations in higher precision when\n",
            " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
            " |      will still typically be float16 or bfloat16 in such cases.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The layer's compute dtype.\n",
            " |  \n",
            " |  dtype\n",
            " |      The dtype of the layer weights.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
            " |      dtype of the layer's computations.\n",
            " |  \n",
            " |  dtype_policy\n",
            " |      The dtype policy associated with this layer.\n",
            " |      \n",
            " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
            " |  \n",
            " |  dynamic\n",
            " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  input_spec\n",
            " |      `InputSpec` instance(s) describing the input format for this layer.\n",
            " |      \n",
            " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
            " |      the layer to run input compatibility checks when it is called.\n",
            " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
            " |      of rank 4. As such, you can set, in `__init__()`:\n",
            " |      \n",
            " |      ```python\n",
            " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
            " |      ```\n",
            " |      \n",
            " |      Now, if you try to call the layer on an input that isn't rank 4\n",
            " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
            " |      error:\n",
            " |      \n",
            " |      ```\n",
            " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
            " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
            " |      ```\n",
            " |      \n",
            " |      Input checks that can be specified via `input_spec` include:\n",
            " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
            " |      - Shape\n",
            " |      - Rank (ndim)\n",
            " |      - Dtype\n",
            " |      \n",
            " |      For more information, see `tf.keras.layers.InputSpec`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
            " |  \n",
            " |  losses\n",
            " |      List of losses added using the `add_loss()` API.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is accessed,\n",
            " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
            " |      propagate gradients back to the corresponding variables.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
            " |      ...   def call(self, inputs):\n",
            " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |      ...     return inputs\n",
            " |      >>> l = MyLayer()\n",
            " |      >>> l(np.ones((10, 1)))\n",
            " |      >>> l.losses\n",
            " |      [1.0]\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Activity regularization.\n",
            " |      >>> len(model.losses)\n",
            " |      0\n",
            " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      >>> len(model.losses)\n",
            " |      1\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
            " |      >>> x = d(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Weight regularization.\n",
            " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      >>> model.losses\n",
            " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  name\n",
            " |      Name of the layer (string), set in the constructor.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  supports_masking\n",
            " |      Whether this layer supports computing a mask using `compute_mask`.\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variable_dtype\n",
            " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      >>> class MyModule(tf.Module):\n",
            " |      ...   @tf.Module.with_name_scope\n",
            " |      ...   def __call__(self, x):\n",
            " |      ...     if not hasattr(self, 'w'):\n",
            " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
            " |      ...     return tf.matmul(x, self.w)\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      >>> mod = MyModule()\n",
            " |      >>> mod(tf.ones([1, 2]))\n",
            " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
            " |      >>> mod.w\n",
            " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
            " |      numpy=..., dtype=float32)>\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      >>> a = tf.Module()\n",
            " |      >>> b = tf.Module()\n",
            " |      >>> c = tf.Module()\n",
            " |      >>> a.b = b\n",
            " |      >>> b.c = c\n",
            " |      >>> list(a.submodules) == [b, c]\n",
            " |      True\n",
            " |      >>> list(b.submodules) == [c]\n",
            " |      True\n",
            " |      >>> list(c.submodules) == []\n",
            " |      True\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVEiWb3bHy_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc78494-ebff-4395-e2e1-5171d37872e2"
      },
      "source": [
        "help(main_model.fit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method fit in module tensorflow.python.keras.engine.training:\n",
            "\n",
            "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) method of tensorflow.python.keras.engine.functional.Functional instance\n",
            "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
            "    \n",
            "    Arguments:\n",
            "        x: Input data. It could be:\n",
            "          - A Numpy array (or array-like), or a list of arrays\n",
            "            (in case the model has multiple inputs).\n",
            "          - A TensorFlow tensor, or a list of tensors\n",
            "            (in case the model has multiple inputs).\n",
            "          - A dict mapping input names to the corresponding array/tensors,\n",
            "            if the model has named inputs.\n",
            "          - A `tf.data` dataset. Should return a tuple\n",
            "            of either `(inputs, targets)` or\n",
            "            `(inputs, targets, sample_weights)`.\n",
            "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
            "            or `(inputs, targets, sample_weights)`.\n",
            "          A more detailed description of unpacking behavior for iterator types\n",
            "          (Dataset, generator, Sequence) is given below.\n",
            "        y: Target data. Like the input data `x`,\n",
            "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
            "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
            "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
            "          or `keras.utils.Sequence` instance, `y` should\n",
            "          not be specified (since targets will be obtained from `x`).\n",
            "        batch_size: Integer or `None`.\n",
            "            Number of samples per gradient update.\n",
            "            If unspecified, `batch_size` will default to 32.\n",
            "            Do not specify the `batch_size` if your data is in the\n",
            "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
            "            (since they generate batches).\n",
            "        epochs: Integer. Number of epochs to train the model.\n",
            "            An epoch is an iteration over the entire `x` and `y`\n",
            "            data provided.\n",
            "            Note that in conjunction with `initial_epoch`,\n",
            "            `epochs` is to be understood as \"final epoch\".\n",
            "            The model is not trained for a number of iterations\n",
            "            given by `epochs`, but merely until the epoch\n",
            "            of index `epochs` is reached.\n",
            "        verbose: 0, 1, or 2. Verbosity mode.\n",
            "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
            "            Note that the progress bar is not particularly useful when\n",
            "            logged to a file, so verbose=2 is recommended when not running\n",
            "            interactively (eg, in a production environment).\n",
            "        callbacks: List of `keras.callbacks.Callback` instances.\n",
            "            List of callbacks to apply during training.\n",
            "            See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
            "            and `tf.keras.callbacks.History` callbacks are created automatically\n",
            "            and need not be passed into `model.fit`.\n",
            "            `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
            "            `verbose` argument to `model.fit`.\n",
            "        validation_split: Float between 0 and 1.\n",
            "            Fraction of the training data to be used as validation data.\n",
            "            The model will set apart this fraction of the training data,\n",
            "            will not train on it, and will evaluate\n",
            "            the loss and any model metrics\n",
            "            on this data at the end of each epoch.\n",
            "            The validation data is selected from the last samples\n",
            "            in the `x` and `y` data provided, before shuffling. This argument is\n",
            "            not supported when `x` is a dataset, generator or\n",
            "           `keras.utils.Sequence` instance.\n",
            "        validation_data: Data on which to evaluate\n",
            "            the loss and any model metrics at the end of each epoch.\n",
            "            The model will not be trained on this data. Thus, note the fact\n",
            "            that the validation loss of data provided using `validation_split`\n",
            "            or `validation_data` is not affected by regularization layers like\n",
            "            noise and dropout.\n",
            "            `validation_data` will override `validation_split`.\n",
            "            `validation_data` could be:\n",
            "              - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
            "              - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
            "              - dataset\n",
            "            For the first two cases, `batch_size` must be provided.\n",
            "            For the last case, `validation_steps` could be provided.\n",
            "            Note that `validation_data` does not support all the data types that\n",
            "            are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
            "        shuffle: Boolean (whether to shuffle the training data\n",
            "            before each epoch) or str (for 'batch'). This argument is ignored\n",
            "            when `x` is a generator. 'batch' is a special option for dealing\n",
            "            with the limitations of HDF5 data; it shuffles in batch-sized\n",
            "            chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
            "        class_weight: Optional dictionary mapping class indices (integers)\n",
            "            to a weight (float) value, used for weighting the loss function\n",
            "            (during training only).\n",
            "            This can be useful to tell the model to\n",
            "            \"pay more attention\" to samples from\n",
            "            an under-represented class.\n",
            "        sample_weight: Optional Numpy array of weights for\n",
            "            the training samples, used for weighting the loss function\n",
            "            (during training only). You can either pass a flat (1D)\n",
            "            Numpy array with the same length as the input samples\n",
            "            (1:1 mapping between weights and samples),\n",
            "            or in the case of temporal data,\n",
            "            you can pass a 2D array with shape\n",
            "            `(samples, sequence_length)`,\n",
            "            to apply a different weight to every timestep of every sample. This\n",
            "            argument is not supported when `x` is a dataset, generator, or\n",
            "           `keras.utils.Sequence` instance, instead provide the sample_weights\n",
            "            as the third element of `x`.\n",
            "        initial_epoch: Integer.\n",
            "            Epoch at which to start training\n",
            "            (useful for resuming a previous training run).\n",
            "        steps_per_epoch: Integer or `None`.\n",
            "            Total number of steps (batches of samples)\n",
            "            before declaring one epoch finished and starting the\n",
            "            next epoch. When training with input tensors such as\n",
            "            TensorFlow data tensors, the default `None` is equal to\n",
            "            the number of samples in your dataset divided by\n",
            "            the batch size, or 1 if that cannot be determined. If x is a\n",
            "            `tf.data` dataset, and 'steps_per_epoch'\n",
            "            is None, the epoch will run until the input dataset is exhausted.\n",
            "            When passing an infinitely repeating dataset, you must specify the\n",
            "            `steps_per_epoch` argument. This argument is not supported with\n",
            "            array inputs.\n",
            "        validation_steps: Only relevant if `validation_data` is provided and\n",
            "            is a `tf.data` dataset. Total number of steps (batches of\n",
            "            samples) to draw before stopping when performing validation\n",
            "            at the end of every epoch. If 'validation_steps' is None, validation\n",
            "            will run until the `validation_data` dataset is exhausted. In the\n",
            "            case of an infinitely repeated dataset, it will run into an\n",
            "            infinite loop. If 'validation_steps' is specified and only part of\n",
            "            the dataset will be consumed, the evaluation will start from the\n",
            "            beginning of the dataset at each epoch. This ensures that the same\n",
            "            validation samples are used every time.\n",
            "        validation_batch_size: Integer or `None`.\n",
            "            Number of samples per validation batch.\n",
            "            If unspecified, will default to `batch_size`.\n",
            "            Do not specify the `validation_batch_size` if your data is in the\n",
            "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
            "            (since they generate batches).\n",
            "        validation_freq: Only relevant if validation data is provided. Integer\n",
            "            or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
            "            If an integer, specifies how many training epochs to run before a\n",
            "            new validation run is performed, e.g. `validation_freq=2` runs\n",
            "            validation every 2 epochs. If a Container, specifies the epochs on\n",
            "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
            "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
            "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
            "            input only. Maximum size for the generator queue.\n",
            "            If unspecified, `max_queue_size` will default to 10.\n",
            "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
            "            only. Maximum number of processes to spin up\n",
            "            when using process-based threading. If unspecified, `workers`\n",
            "            will default to 1. If 0, will execute the generator on the main\n",
            "            thread.\n",
            "        use_multiprocessing: Boolean. Used for generator or\n",
            "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
            "            threading. If unspecified, `use_multiprocessing` will default to\n",
            "            `False`. Note that because this implementation relies on\n",
            "            multiprocessing, you should not pass non-picklable arguments to\n",
            "            the generator as they can't be passed easily to children processes.\n",
            "    \n",
            "    Unpacking behavior for iterator-like inputs:\n",
            "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
            "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
            "      yield not only features (x) but optionally targets (y) and sample weights.\n",
            "      Keras requires that the output of such iterator-likes be unambiguous. The\n",
            "      iterator should return a tuple of length 1, 2, or 3, where the optional\n",
            "      second and third elements will be used for y and sample_weight\n",
            "      respectively. Any other type provided will be wrapped in a length one\n",
            "      tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
            "      should still adhere to the top-level tuple structure.\n",
            "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
            "      features, targets, and weights from the keys of a single dict.\n",
            "        A notable unsupported data type is the namedtuple. The reason is that\n",
            "      it behaves like both an ordered datatype (tuple) and a mapping\n",
            "      datatype (dict). So given a namedtuple of the form:\n",
            "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
            "      it is ambiguous whether to reverse the order of the elements when\n",
            "      interpreting the value. Even worse is a tuple of the form:\n",
            "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
            "      where it is unclear if the tuple was intended to be unpacked into x, y,\n",
            "      and sample_weight or passed through as a single element to `x`. As a\n",
            "      result the data processing code will simply raise a ValueError if it\n",
            "      encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
            "    \n",
            "    Returns:\n",
            "        A `History` object. Its `History.history` attribute is\n",
            "        a record of training loss values and metrics values\n",
            "        at successive epochs, as well as validation loss values\n",
            "        and validation metrics values (if applicable).\n",
            "    \n",
            "    Raises:\n",
            "        RuntimeError: 1. If the model was never compiled or,\n",
            "        2. If `model.fit` is  wrapped in `tf.function`.\n",
            "    \n",
            "        ValueError: In case of mismatch between the provided input data\n",
            "            and what the model expects or when the input data is empty.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qGf1FQf7AkD"
      },
      "source": [
        "# 6.0 Compile model now\r\n",
        "# Ref: Model.compile: \r\n",
        "#      https://wwwa.tensorflow.org/api_docs/python/tf/keras/Model\r\n",
        "\r\n",
        "main_model.compile(\r\n",
        "                     loss = ['mse', 'mse'],        # Could also be in dict() format\r\n",
        "                     metrics = \"mse\",\r\n",
        "                     loss_weights= {\"out_a\": 0.9,   # More weight to error here\r\n",
        "                                    \"out_b\" : 0.1   # Less weight to error here\r\n",
        "                                    }\r\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEeYQ5aD7oJV",
        "outputId": "4ebb46d8-fd39-4416-99ce-10077a9226f5"
      },
      "source": [
        "# 6.1\r\n",
        "main_model.fit(\r\n",
        "               {                            #[X_train[:,:4],X_train[:,1:8]]\r\n",
        "                   \"in_a\" : X_train[:,:4],  # One input\r\n",
        "                   \"in_b\" : X_train[:,1:8]  # IInd input     \r\n",
        "               },              \r\n",
        "               [y_train,y_train],\r\n",
        "               epochs = 100\r\n",
        "           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "516/516 [==============================] - 2s 1ms/step - loss: 2.5911 - out_a_loss: 2.5820 - out_b_loss: 2.6737 - out_a_mse: 2.5820 - out_b_mse: 2.6737\n",
            "Epoch 2/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4350 - out_a_loss: 2.4350 - out_b_loss: 2.4350 - out_a_mse: 2.4350 - out_b_mse: 2.4350\n",
            "Epoch 3/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4574 - out_a_loss: 2.4574 - out_b_loss: 2.4574 - out_a_mse: 2.4574 - out_b_mse: 2.4574\n",
            "Epoch 4/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4582 - out_a_loss: 2.4582 - out_b_loss: 2.4582 - out_a_mse: 2.4582 - out_b_mse: 2.4582\n",
            "Epoch 5/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4532 - out_a_loss: 2.4532 - out_b_loss: 2.4533 - out_a_mse: 2.4532 - out_b_mse: 2.4533\n",
            "Epoch 6/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4427 - out_a_loss: 2.4427 - out_b_loss: 2.4428 - out_a_mse: 2.4427 - out_b_mse: 2.4428\n",
            "Epoch 7/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4674 - out_a_loss: 2.4673 - out_b_loss: 2.4675 - out_a_mse: 2.4673 - out_b_mse: 2.4675\n",
            "Epoch 8/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4591 - out_a_loss: 2.4591 - out_b_loss: 2.4592 - out_a_mse: 2.4591 - out_b_mse: 2.4592\n",
            "Epoch 9/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4436 - out_a_loss: 2.4436 - out_b_loss: 2.4437 - out_a_mse: 2.4436 - out_b_mse: 2.4437\n",
            "Epoch 10/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4898 - out_a_loss: 2.4898 - out_b_loss: 2.4899 - out_a_mse: 2.4898 - out_b_mse: 2.4899\n",
            "Epoch 11/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5099 - out_a_loss: 2.5099 - out_b_loss: 2.5100 - out_a_mse: 2.5099 - out_b_mse: 2.5100\n",
            "Epoch 12/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4986 - out_a_loss: 2.4986 - out_b_loss: 2.4988 - out_a_mse: 2.4986 - out_b_mse: 2.4988\n",
            "Epoch 13/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4484 - out_a_loss: 2.4484 - out_b_loss: 2.4486 - out_a_mse: 2.4484 - out_b_mse: 2.4486\n",
            "Epoch 14/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4271 - out_a_loss: 2.4271 - out_b_loss: 2.4273 - out_a_mse: 2.4271 - out_b_mse: 2.4273\n",
            "Epoch 15/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4411 - out_a_loss: 2.4410 - out_b_loss: 2.4413 - out_a_mse: 2.4410 - out_b_mse: 2.4413\n",
            "Epoch 16/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4360 - out_a_loss: 2.4360 - out_b_loss: 2.4362 - out_a_mse: 2.4360 - out_b_mse: 2.4362\n",
            "Epoch 17/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4699 - out_a_loss: 2.4699 - out_b_loss: 2.4701 - out_a_mse: 2.4699 - out_b_mse: 2.4701\n",
            "Epoch 18/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5292 - out_a_loss: 2.5292 - out_b_loss: 2.5294 - out_a_mse: 2.5292 - out_b_mse: 2.5294\n",
            "Epoch 19/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4796 - out_a_loss: 2.4795 - out_b_loss: 2.4798 - out_a_mse: 2.4795 - out_b_mse: 2.4798\n",
            "Epoch 20/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4125 - out_a_loss: 2.4125 - out_b_loss: 2.4127 - out_a_mse: 2.4125 - out_b_mse: 2.4127\n",
            "Epoch 21/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4889 - out_a_loss: 2.4888 - out_b_loss: 2.4890 - out_a_mse: 2.4888 - out_b_mse: 2.4890\n",
            "Epoch 22/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4562 - out_a_loss: 2.4561 - out_b_loss: 2.4565 - out_a_mse: 2.4561 - out_b_mse: 2.4565\n",
            "Epoch 23/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4430 - out_a_loss: 2.4430 - out_b_loss: 2.4433 - out_a_mse: 2.4430 - out_b_mse: 2.4433\n",
            "Epoch 24/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4581 - out_a_loss: 2.4581 - out_b_loss: 2.4583 - out_a_mse: 2.4581 - out_b_mse: 2.4583\n",
            "Epoch 25/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4487 - out_a_loss: 2.4486 - out_b_loss: 2.4489 - out_a_mse: 2.4486 - out_b_mse: 2.4489\n",
            "Epoch 26/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4573 - out_a_loss: 2.4573 - out_b_loss: 2.4575 - out_a_mse: 2.4573 - out_b_mse: 2.4575\n",
            "Epoch 27/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4628 - out_a_loss: 2.4627 - out_b_loss: 2.4630 - out_a_mse: 2.4627 - out_b_mse: 2.4630\n",
            "Epoch 28/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5600 - out_a_loss: 2.5600 - out_b_loss: 2.5602 - out_a_mse: 2.5600 - out_b_mse: 2.5602\n",
            "Epoch 29/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4565 - out_a_loss: 2.4565 - out_b_loss: 2.4567 - out_a_mse: 2.4565 - out_b_mse: 2.4567\n",
            "Epoch 30/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4409 - out_a_loss: 2.4409 - out_b_loss: 2.4412 - out_a_mse: 2.4409 - out_b_mse: 2.4412\n",
            "Epoch 31/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4835 - out_a_loss: 2.4834 - out_b_loss: 2.4837 - out_a_mse: 2.4834 - out_b_mse: 2.4837\n",
            "Epoch 32/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4467 - out_a_loss: 2.4467 - out_b_loss: 2.4470 - out_a_mse: 2.4467 - out_b_mse: 2.4470\n",
            "Epoch 33/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4756 - out_a_loss: 2.4756 - out_b_loss: 2.4759 - out_a_mse: 2.4756 - out_b_mse: 2.4759\n",
            "Epoch 34/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5118 - out_a_loss: 2.5117 - out_b_loss: 2.5121 - out_a_mse: 2.5117 - out_b_mse: 2.5121\n",
            "Epoch 35/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4832 - out_a_loss: 2.4831 - out_b_loss: 2.4834 - out_a_mse: 2.4831 - out_b_mse: 2.4834\n",
            "Epoch 36/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5273 - out_a_loss: 2.5273 - out_b_loss: 2.5274 - out_a_mse: 2.5273 - out_b_mse: 2.5274\n",
            "Epoch 37/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4942 - out_a_loss: 2.4942 - out_b_loss: 2.4945 - out_a_mse: 2.4942 - out_b_mse: 2.4945\n",
            "Epoch 38/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5033 - out_a_loss: 2.5033 - out_b_loss: 2.5035 - out_a_mse: 2.5033 - out_b_mse: 2.5035\n",
            "Epoch 39/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4355 - out_a_loss: 2.4355 - out_b_loss: 2.4357 - out_a_mse: 2.4355 - out_b_mse: 2.4357\n",
            "Epoch 40/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4905 - out_a_loss: 2.4905 - out_b_loss: 2.4907 - out_a_mse: 2.4905 - out_b_mse: 2.4907\n",
            "Epoch 41/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4172 - out_a_loss: 2.4171 - out_b_loss: 2.4174 - out_a_mse: 2.4171 - out_b_mse: 2.4174\n",
            "Epoch 42/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4704 - out_a_loss: 2.4703 - out_b_loss: 2.4706 - out_a_mse: 2.4703 - out_b_mse: 2.4706\n",
            "Epoch 43/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4840 - out_a_loss: 2.4840 - out_b_loss: 2.4842 - out_a_mse: 2.4840 - out_b_mse: 2.4842\n",
            "Epoch 44/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4912 - out_a_loss: 2.4911 - out_b_loss: 2.4913 - out_a_mse: 2.4911 - out_b_mse: 2.4913\n",
            "Epoch 45/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4381 - out_a_loss: 2.4380 - out_b_loss: 2.4383 - out_a_mse: 2.4380 - out_b_mse: 2.4383\n",
            "Epoch 46/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4690 - out_a_loss: 2.4689 - out_b_loss: 2.4692 - out_a_mse: 2.4689 - out_b_mse: 2.4692\n",
            "Epoch 47/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4898 - out_a_loss: 2.4897 - out_b_loss: 2.4900 - out_a_mse: 2.4897 - out_b_mse: 2.4900\n",
            "Epoch 48/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4628 - out_a_loss: 2.4628 - out_b_loss: 2.4630 - out_a_mse: 2.4628 - out_b_mse: 2.4630\n",
            "Epoch 49/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4624 - out_a_loss: 2.4624 - out_b_loss: 2.4626 - out_a_mse: 2.4624 - out_b_mse: 2.4626\n",
            "Epoch 50/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5195 - out_a_loss: 2.5195 - out_b_loss: 2.5197 - out_a_mse: 2.5195 - out_b_mse: 2.5197\n",
            "Epoch 51/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4737 - out_a_loss: 2.4736 - out_b_loss: 2.4739 - out_a_mse: 2.4736 - out_b_mse: 2.4739\n",
            "Epoch 52/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4519 - out_a_loss: 2.4519 - out_b_loss: 2.4522 - out_a_mse: 2.4519 - out_b_mse: 2.4522\n",
            "Epoch 53/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4936 - out_a_loss: 2.4935 - out_b_loss: 2.4938 - out_a_mse: 2.4935 - out_b_mse: 2.4938\n",
            "Epoch 54/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4075 - out_a_loss: 2.4075 - out_b_loss: 2.4078 - out_a_mse: 2.4075 - out_b_mse: 2.4078\n",
            "Epoch 55/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5435 - out_a_loss: 2.5435 - out_b_loss: 2.5437 - out_a_mse: 2.5435 - out_b_mse: 2.5437\n",
            "Epoch 56/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4904 - out_a_loss: 2.4904 - out_b_loss: 2.4907 - out_a_mse: 2.4904 - out_b_mse: 2.4907\n",
            "Epoch 57/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5147 - out_a_loss: 2.5146 - out_b_loss: 2.5149 - out_a_mse: 2.5146 - out_b_mse: 2.5149\n",
            "Epoch 58/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4771 - out_a_loss: 2.4771 - out_b_loss: 2.4774 - out_a_mse: 2.4771 - out_b_mse: 2.4774\n",
            "Epoch 59/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4851 - out_a_loss: 2.4851 - out_b_loss: 2.4853 - out_a_mse: 2.4851 - out_b_mse: 2.4853\n",
            "Epoch 60/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4336 - out_a_loss: 2.4336 - out_b_loss: 2.4338 - out_a_mse: 2.4336 - out_b_mse: 2.4338\n",
            "Epoch 61/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4309 - out_a_loss: 2.4308 - out_b_loss: 2.4311 - out_a_mse: 2.4308 - out_b_mse: 2.4311\n",
            "Epoch 62/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4544 - out_a_loss: 2.4544 - out_b_loss: 2.4547 - out_a_mse: 2.4544 - out_b_mse: 2.4547\n",
            "Epoch 63/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5337 - out_a_loss: 2.5337 - out_b_loss: 2.5340 - out_a_mse: 2.5337 - out_b_mse: 2.5340\n",
            "Epoch 64/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4933 - out_a_loss: 2.4932 - out_b_loss: 2.4934 - out_a_mse: 2.4932 - out_b_mse: 2.4934\n",
            "Epoch 65/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4342 - out_a_loss: 2.4341 - out_b_loss: 2.4343 - out_a_mse: 2.4341 - out_b_mse: 2.4343\n",
            "Epoch 66/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4835 - out_a_loss: 2.4835 - out_b_loss: 2.4836 - out_a_mse: 2.4835 - out_b_mse: 2.4836\n",
            "Epoch 67/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4765 - out_a_loss: 2.4765 - out_b_loss: 2.4767 - out_a_mse: 2.4765 - out_b_mse: 2.4767\n",
            "Epoch 68/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4427 - out_a_loss: 2.4427 - out_b_loss: 2.4430 - out_a_mse: 2.4427 - out_b_mse: 2.4430\n",
            "Epoch 69/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4669 - out_a_loss: 2.4668 - out_b_loss: 2.4670 - out_a_mse: 2.4668 - out_b_mse: 2.4670\n",
            "Epoch 70/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4657 - out_a_loss: 2.4657 - out_b_loss: 2.4659 - out_a_mse: 2.4657 - out_b_mse: 2.4659\n",
            "Epoch 71/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4559 - out_a_loss: 2.4559 - out_b_loss: 2.4561 - out_a_mse: 2.4559 - out_b_mse: 2.4561\n",
            "Epoch 72/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4404 - out_a_loss: 2.4403 - out_b_loss: 2.4405 - out_a_mse: 2.4403 - out_b_mse: 2.4405\n",
            "Epoch 73/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5044 - out_a_loss: 2.5044 - out_b_loss: 2.5046 - out_a_mse: 2.5044 - out_b_mse: 2.5046\n",
            "Epoch 74/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4506 - out_a_loss: 2.4506 - out_b_loss: 2.4508 - out_a_mse: 2.4506 - out_b_mse: 2.4508\n",
            "Epoch 75/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5175 - out_a_loss: 2.5175 - out_b_loss: 2.5177 - out_a_mse: 2.5175 - out_b_mse: 2.5177\n",
            "Epoch 76/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4438 - out_a_loss: 2.4437 - out_b_loss: 2.4440 - out_a_mse: 2.4437 - out_b_mse: 2.4440\n",
            "Epoch 77/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4701 - out_a_loss: 2.4700 - out_b_loss: 2.4703 - out_a_mse: 2.4700 - out_b_mse: 2.4703\n",
            "Epoch 78/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5124 - out_a_loss: 2.5123 - out_b_loss: 2.5127 - out_a_mse: 2.5123 - out_b_mse: 2.5127\n",
            "Epoch 79/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4659 - out_a_loss: 2.4659 - out_b_loss: 2.4662 - out_a_mse: 2.4659 - out_b_mse: 2.4662\n",
            "Epoch 80/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4961 - out_a_loss: 2.4961 - out_b_loss: 2.4963 - out_a_mse: 2.4961 - out_b_mse: 2.4963\n",
            "Epoch 81/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4690 - out_a_loss: 2.4690 - out_b_loss: 2.4692 - out_a_mse: 2.4690 - out_b_mse: 2.4692\n",
            "Epoch 82/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.5120 - out_a_loss: 2.5120 - out_b_loss: 2.5122 - out_a_mse: 2.5120 - out_b_mse: 2.5122\n",
            "Epoch 83/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4664 - out_a_loss: 2.4664 - out_b_loss: 2.4667 - out_a_mse: 2.4664 - out_b_mse: 2.4667\n",
            "Epoch 84/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4494 - out_a_loss: 2.4494 - out_b_loss: 2.4497 - out_a_mse: 2.4494 - out_b_mse: 2.4497\n",
            "Epoch 85/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4348 - out_a_loss: 2.4348 - out_b_loss: 2.4350 - out_a_mse: 2.4348 - out_b_mse: 2.4350\n",
            "Epoch 86/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4874 - out_a_loss: 2.4874 - out_b_loss: 2.4877 - out_a_mse: 2.4874 - out_b_mse: 2.4877\n",
            "Epoch 87/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5013 - out_a_loss: 2.5013 - out_b_loss: 2.5015 - out_a_mse: 2.5013 - out_b_mse: 2.5015\n",
            "Epoch 88/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4829 - out_a_loss: 2.4829 - out_b_loss: 2.4831 - out_a_mse: 2.4829 - out_b_mse: 2.4831\n",
            "Epoch 89/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4747 - out_a_loss: 2.4746 - out_b_loss: 2.4748 - out_a_mse: 2.4746 - out_b_mse: 2.4748\n",
            "Epoch 90/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4713 - out_a_loss: 2.4713 - out_b_loss: 2.4715 - out_a_mse: 2.4713 - out_b_mse: 2.4715\n",
            "Epoch 91/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4154 - out_a_loss: 2.4154 - out_b_loss: 2.4157 - out_a_mse: 2.4154 - out_b_mse: 2.4157\n",
            "Epoch 92/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4763 - out_a_loss: 2.4763 - out_b_loss: 2.4765 - out_a_mse: 2.4763 - out_b_mse: 2.4765\n",
            "Epoch 93/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4171 - out_a_loss: 2.4171 - out_b_loss: 2.4173 - out_a_mse: 2.4171 - out_b_mse: 2.4173\n",
            "Epoch 94/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5176 - out_a_loss: 2.5176 - out_b_loss: 2.5178 - out_a_mse: 2.5176 - out_b_mse: 2.5178\n",
            "Epoch 95/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4747 - out_a_loss: 2.4747 - out_b_loss: 2.4749 - out_a_mse: 2.4747 - out_b_mse: 2.4749\n",
            "Epoch 96/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.5004 - out_a_loss: 2.5004 - out_b_loss: 2.5006 - out_a_mse: 2.5004 - out_b_mse: 2.5006\n",
            "Epoch 97/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4754 - out_a_loss: 2.4754 - out_b_loss: 2.4755 - out_a_mse: 2.4754 - out_b_mse: 2.4755\n",
            "Epoch 98/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4470 - out_a_loss: 2.4470 - out_b_loss: 2.4473 - out_a_mse: 2.4470 - out_b_mse: 2.4473\n",
            "Epoch 99/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4850 - out_a_loss: 2.4850 - out_b_loss: 2.4853 - out_a_mse: 2.4850 - out_b_mse: 2.4853\n",
            "Epoch 100/100\n",
            "516/516 [==============================] - 1s 1ms/step - loss: 2.4775 - out_a_loss: 2.4775 - out_b_loss: 2.4778 - out_a_mse: 2.4775 - out_b_mse: 2.4778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdb8ac1ee80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaDhO7KXFy2m",
        "outputId": "0043cf12-d858-4580-ca5b-0fb4983dd413"
      },
      "source": [
        "# 6.2 One can also write outputs in a dictionary form, as:\r\n",
        "\r\n",
        "main_model.fit(\r\n",
        "               {                            #[X_train[:,:4],X_train[:,1:8]]\r\n",
        "                   \"in_a\" : X_train[:,:4],  # One input\r\n",
        "                   \"in_b\" : X_train[:,1:8]  # IInd input     \r\n",
        "               },              \r\n",
        "               {\r\n",
        "                   \"out_a\" : y_train,\r\n",
        "                   \"out_b\" : y_train\r\n",
        "               },\r\n",
        "               epochs = 100\r\n",
        "           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4699 - out_a_loss: 2.4699 - out_b_loss: 2.4702 - out_a_mse: 2.4699 - out_b_mse: 2.4702\n",
            "Epoch 2/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4699 - out_a_loss: 2.4698 - out_b_loss: 2.4701 - out_a_mse: 2.4698 - out_b_mse: 2.4701\n",
            "Epoch 3/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4701 - out_a_mse: 2.4698 - out_b_mse: 2.4701\n",
            "Epoch 4/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4700 - out_a_mse: 2.4698 - out_b_mse: 2.4700\n",
            "Epoch 5/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4699 - out_a_loss: 2.4699 - out_b_loss: 2.4701 - out_a_mse: 2.4699 - out_b_mse: 2.4701\n",
            "Epoch 6/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4701 - out_a_mse: 2.4698 - out_b_mse: 2.4701\n",
            "Epoch 7/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4700 - out_a_mse: 2.4698 - out_b_mse: 2.4700\n",
            "Epoch 8/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4699 - out_a_loss: 2.4699 - out_b_loss: 2.4701 - out_a_mse: 2.4699 - out_b_mse: 2.4701\n",
            "Epoch 9/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4699 - out_a_loss: 2.4699 - out_b_loss: 2.4702 - out_a_mse: 2.4699 - out_b_mse: 2.4702\n",
            "Epoch 10/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4701 - out_a_mse: 2.4698 - out_b_mse: 2.4701\n",
            "Epoch 11/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 12/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 13/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4700 - out_a_mse: 2.4698 - out_b_mse: 2.4700\n",
            "Epoch 14/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 15/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4701 - out_a_mse: 2.4698 - out_b_mse: 2.4701\n",
            "Epoch 16/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 17/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4700 - out_a_mse: 2.4698 - out_b_mse: 2.4700\n",
            "Epoch 18/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 19/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 20/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4699 - out_a_mse: 2.4697 - out_b_mse: 2.4699\n",
            "Epoch 21/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4699 - out_a_mse: 2.4697 - out_b_mse: 2.4699\n",
            "Epoch 22/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 23/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4697 - out_b_loss: 2.4701 - out_a_mse: 2.4697 - out_b_mse: 2.4701\n",
            "Epoch 24/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4698 - out_a_loss: 2.4698 - out_b_loss: 2.4700 - out_a_mse: 2.4698 - out_b_mse: 2.4700\n",
            "Epoch 25/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4699 - out_a_mse: 2.4697 - out_b_mse: 2.4699\n",
            "Epoch 26/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 27/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 28/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 29/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 30/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 31/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 32/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 33/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 34/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 35/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4699 - out_a_mse: 2.4697 - out_b_mse: 2.4699\n",
            "Epoch 36/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 37/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4700 - out_a_mse: 2.4697 - out_b_mse: 2.4700\n",
            "Epoch 38/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 39/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 40/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 41/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 42/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 43/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4699 - out_a_mse: 2.4697 - out_b_mse: 2.4699\n",
            "Epoch 44/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 45/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 46/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 47/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 48/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 49/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 50/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4697 - out_b_loss: 2.4699 - out_a_mse: 2.4697 - out_b_mse: 2.4699\n",
            "Epoch 51/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 52/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 53/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 54/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 55/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4697 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 56/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 57/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4699 - out_a_mse: 2.4696 - out_b_mse: 2.4699\n",
            "Epoch 58/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 59/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 60/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 61/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 62/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 63/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 64/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 65/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 66/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 67/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 68/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 69/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 70/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 71/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 72/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 73/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 74/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 75/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 76/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 77/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 78/100\n",
            "516/516 [==============================] - 2s 4ms/step - loss: 2.4696 - out_a_loss: 2.4696 - out_b_loss: 2.4698 - out_a_mse: 2.4696 - out_b_mse: 2.4698\n",
            "Epoch 79/100\n",
            "516/516 [==============================] - 2s 4ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 80/100\n",
            "516/516 [==============================] - 2s 4ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 81/100\n",
            "516/516 [==============================] - 2s 4ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 82/100\n",
            "516/516 [==============================] - 2s 4ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 83/100\n",
            "516/516 [==============================] - 2s 4ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 84/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 85/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 86/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 87/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 88/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4696 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 89/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 90/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4698 - out_a_mse: 2.4695 - out_b_mse: 2.4698\n",
            "Epoch 91/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 92/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 93/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 94/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 95/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 96/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 97/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 98/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4695 - out_a_loss: 2.4695 - out_b_loss: 2.4697 - out_a_mse: 2.4695 - out_b_mse: 2.4697\n",
            "Epoch 99/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n",
            "Epoch 100/100\n",
            "516/516 [==============================] - 1s 2ms/step - loss: 2.4694 - out_a_loss: 2.4694 - out_b_loss: 2.4697 - out_a_mse: 2.4694 - out_b_mse: 2.4697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdb885219e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqDTxCZr8fxs",
        "outputId": "060319d0-bb72-4b0a-dc48-4a770770b3db"
      },
      "source": [
        "# 7.0 To evaluate, we must also supply two inputs\r\n",
        "main_model.evaluate(\r\n",
        "                     [X_test[:,:4],X_test[:,1:8]],\r\n",
        "                      y_test\r\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129/129 [==============================] - 0s 1ms/step - loss: 2.4382 - out_a_loss: 2.4382 - out_b_loss: 2.4385 - out_a_mse: 2.4382 - out_b_mse: 2.4385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.438199758529663,\n",
              " 2.4381649494171143,\n",
              " 2.43851637840271,\n",
              " 2.4381649494171143,\n",
              " 2.43851637840271]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MhQF056p09N"
      },
      "source": [
        "########### It is done ##############"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}